<!DOCTYPE html><html lang='en'><head>
<style>

/*! normalize.css v8.0.1 | MIT License | github.com/necolas/normalize.css */html{line-height:1.15;-webkit-text-size-adjust:100%}body{margin:0}pre{font-family:monospace,monospace;font-size:1em}strong{font-weight:bolder}img{border-style:none}::-webkit-file-upload-button{-webkit-appearance:button;font:inherit}p,pre{margin:0}ul{list-style:none;margin:0;padding:0}html{font-family:system-ui,-apple-system,BlinkMacSystemFont,"Segoe UI",Roboto,"Helvetica Neue",Arial,"Noto Sans",sans-serif,"Apple Color Emoji","Segoe UI Emoji","Segoe UI Symbol","Noto Color Emoji";line-height:1.5}*,::after,::before{box-sizing:border-box;border-width:0;border-style:solid;border-color:#e2e8f0}img{border-style:solid}table{border-collapse:collapse}pre{font-family:Menlo,Monaco,Consolas,"Liberation Mono","Courier New",monospace}img{display:block;vertical-align:middle}img{max-width:100%;height:auto}.bg-gray-200{--bg-opacity:1;background-color:#edf2f7;background-color:rgba(237,242,247,var(--bg-opacity))}.border-gray-500{--border-opacity:1;border-color:#a0aec0;border-color:rgba(160,174,192,var(--border-opacity))}.border-blue-300{--border-opacity:1;border-color:#90cdf4;border-color:rgba(144,205,244,var(--border-opacity))}.rounded{border-radius:.25rem}.border-solid{border-style:solid}.border-b{border-bottom-width:1px}.float-left{float:left}.font-sans{font-family:system-ui,-apple-system,BlinkMacSystemFont,"Segoe UI",Roboto,"Helvetica Neue",Arial,"Noto Sans",sans-serif,"Apple Color Emoji","Segoe UI Emoji","Segoe UI Symbol","Noto Color Emoji"}.font-serif{font-family:Georgia,Cambria,"Times New Roman",Times,serif}.text-sm{font-size:.875rem}.text-xl{font-size:1.25rem}.text-2xl{font-size:1.5rem}.leading-normal{line-height:1.5}.m-0{margin:0}.mt-0{margin-top:0}.mb-3{margin-bottom:.75rem}.overflow-auto{overflow:auto}.overflow-hidden{overflow:hidden}.p-1{padding:.25rem}.pb-2{padding-bottom:.5rem}.pr-3{padding-right:.75rem}.pl-3{padding-left:.75rem}.shadow-lg{box-shadow:0 10px 15px -3px rgba(0,0,0,.1),0 4px 6px -2px rgba(0,0,0,.05)}.w-16{width:4rem}.lesson-image{width:250px}.code{background:#f4f4f4;border:1px solid #ddd;border-left:3px solid #f36d33;color:#666;page-break-inside:avoid;font-family:monospace;font-size:14px;min-width:710px;max-width:710px;line-height:1.6;margin-bottom:1.6em;padding:1em 1.5em;display:block;-moz-tab-size:2;-o-tab-size:2;tab-size:2;overflow-x:auto;word-wrap:break-word;white-space:pre-wrap}
</style>
</head>
<body class="lesson"><div id="__next"><section class="main-content bg-gray-200"><section class="rounded overflow-hidden shadow-lg"><div class="p-1 font-serif"><img class="lesson-image float-left pr-3" src="https://raw.githubusercontent.com/NSF-EC/INFO490Assets/master/src/assets/images/dmap/topics/TextTripping.png"/><div class="w-100 ma0 meta"><div class="pb-2 m-0 "><p class="text-2xl m-0 font-sans">Lesson: <strong>TFIDF</strong></p><p class="text-xl mt-0 mb-3">Finding The Important Words</p><div class="overflow-auto"><table class="w-50 text-sm" cellSpacing="0"><tbody class="leading-normal"><tr><td class="w-16 border-b border-solid border-gray-500">Version</td><td class="border-b border-solid border-gray-500">08.20.2020 <!-- -->Fall 2020</td></tr><tr><td class="w-16 border-b border-solid border-gray-500">Prerequisites</td><td class="border-b border-solid border-blue-300"><ul><li>INFO 490</li><li>INFO 490</li></ul></td></tr></tbody></table></div></div></div><div class="pl-3 text-xl"><div><strong>TF-IDF</strong></div><p>Imagine having an algorithm that can automatically determine which words of a novel are unique to each chapter as well as filter out the words that are common across all chapters without bootstrapping the algorithm with a set of stop words.  That does sound good.</p><p>The idea of ranking each term (i.e. a single word or an n-gram) within the context of a set of documents (or chapters in a novel) in which it appears is what the algorithm TF-IDF attempts to do. The Term Frequency Inverse Document Frequency is an information retrieval technique that scores each term by using its frequency (the TF weight) and its inverse document frequency (the IDF weight).  These two weights are multiplied together to get the TF*IDF score.  Each term has a score and the score determines how valuable the term is in separating out a set of documents (called a corpus).  Variations of the tf-idf weighting scheme are often used by search engines as a central tool in scoring and ranking a document&#x27;s relevance given a user query.</p><div><strong>TF</strong></div><p>We have already been calculating TF (term frequency) since we starting working with Python dictionaries and counting word occurrences.  A document in this context is very generic.  It can refer to an article, a book, a journal, a web page, a chapter, and even a sentence.  Usually a document needs to be long enough to have some context and a set of documents makes up the corpus.  In practice, a document is usually no smaller than a sentence or a set of sentences.  So in this context, TF would describe the frequency of a word within a document; it measures how common a term is within a document.  A term can be an n-gram as well.</p>IDF The IDF (inverse document frequency) weight isn&#x27;t as intuitive.  If TF measures commonness, IDF measure rarity.  However, IDF is looking at a corpus (a set of documents unified by some attribute). We can illustrate this using the following table -- which lists the number of documents a term appears in.  Assume there are 100 million documents:<div class="code"><pre>1 term          # of documents
2              containing term
3 ----------------------------
4 a                100,000,000
5 boat               1,000,000
6 mobile               100,000
7 mobilegeddon           1,000
</pre></div>Clearly, the term &#x27;a&#x27; provides no value in telling the documents apart.  However, since &#x27;mobilegeddon&#x27; only appears in 1,000 documents, that word does help separate a small set of the documents. To calculate the IDF of each term we take the the total number of documents and divide it by the count of document occurrences for that word (ie. how many documents contain the term).  Then we take the Log of that number to scale the result so that the weight is &#x27;dampened&#x27;.  This prevents interpreting that &#x27;mobilegeddon&#x27; is 1000 times more important than &#x27;boat&#x27;. term                  n/df[j]           IDF[j] a              Log(100M/100,000,000) -&gt; 0 boat           Log(100M/1,000,000)   -&gt; 1 mobile         Log(100M/100,000)     -&gt; 2 mobilegeddon   Log(100M/1,000)       -&gt; 3 So the following formula is the standard way to calculate IDF for term j. So IDF helps measure uniqueness or how much information a term provides.  For search engines, it helps identify what terms within each document are special. Not so Fast At first glance, it might seem &#x27;easy&#x27; to calculate TF•IDF.  However, for both weights there are different ways to calculate them.   The TF we have been describing is called the raw count of a term in a document.  A common alternative  is to scale the TF by the document length so that long documents don&#x27;t skew the results (e.g. a long chapter will have more words in it, but may be no more important than a shorter chapter). TF = raw count of term in a document/len(document) For IDF, there are many variants as well.  The one issue for calculating IDF is that the denominator could be zero (if a term does not show up in a document).  In most situations, all the terms are coming from the documents themselves, so this can never happen.  In any case, some implementations will calculate the &#x27;inverse&#x27; going from this equation: • term_idf[term] = math.log10(N/term_count) to this equation: • term_idf[term] = -1.0 * math.log10(term_count/N) If you remember your &#x27;log&#x27; rules (a small proof): 1. log(a/b) = log(a) - log(b) by definition 2. log(b/a) = log(b) - log(a) by definition 3. log(a) = log(b) - log(b/a)   rearrange terms in #2 4. log(a) - log(b) = - log(b/a) rearrange terms in #3 SO 5. log(a/b) = -1.0 * log(b/a)   definition #1 You may also see adding 1 to the result or the following: term_idf[term] = math.log10(N/(1 + term_count))to set a lower bound on common words and avoid giving a weight of zero to common terms. For the most part, if your documents are long and the number of terms large, the calculation will not matter for comparison purposes.  However, for simple test data, it can matter.  In practice, everyone seems to have their favorite tweaks to the classic formula. TF•IDF So for each document in the corpus, the final calculation is TF * IDF for each term.  Values close to zero are your custom set of stop words -- words that did not help separate out the documents. One subtle reminder is that TF is calculated per document and IDF is calculated across a set of documents. Stop Words Reprise Sometimes it is still useful to remove stop words from the document for the TF•IDF calculations.  In this case, the common words that TF•IDF finds are common themes/terms/ngrams that occur thoughout the corpus.  In the case of using TF•IDF to analyze novels, it might be important to know that &#x27;Tom&#x27; is a term that doesn&#x27;t differentiate chapters, but is seen as a common theme.  We could use that in our essay that we are suppose to write for English, but only had time to let Python &#x27;read&#x27; it. How Fast? TF•IDF involves lots of loops and in our Python only version, it can take a long time.  However, once you have finished, each document can now be represented as a compact vector (set of values).  Algorithms that cluster (grouping documents based on some similarity measure), classify (assign a label) or topic modeling sometimes use the TF•IDF representation as the document. Topic Modeling You may be wondering how TF•IDF is related to topic modeling.  If you aren&#x27;t, then you may not know what topic modeling is.  Topic modeling attempts to find a set of themes (e.g. topics) that describe a set of documents.  In the context of topic modeling, a topic is a probability distribution over the vocabulary (i.e. words) and a document is a probability distribution over topics.  You can think of a topic as a set of words that co-occur together and ideally, the topics are as distinct as possible.  There are many topic modeling algorithms with LDA (latent dirichlet allocation) being one of the more popular ones.  This is a good topic for an advanced class.  You may think of the top N TF•IDF words for a document as a &#x27;topic&#x27;; however they are not the same.  As mentioned above, some topic modeling algorithms will use the TF•IDF scores as the input for algorithm. This is the sequel to INFO 490 (Introduction to Programming for Data Science). Our focus will be on text analysis, machine learning, and of course advancing your Python knowledge. We have a lot of information to share and we hope you will enjoy learning how to move data though machines built using Python. Each lesson requires careful reading and understanding. Use Piazza to ask for help. You should tag your questions with LessonID (given below) This is the first lesson and its focus is on making sure you know how to set up the programming and testing environment properly.</div></div></section></section></div></body>
</html>
