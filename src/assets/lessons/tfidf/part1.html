<!DOCTYPE html><html lang="en"><head><meta charset="utf-8"><style>/*! normalize.css v8.0.1 | MIT License | github.com/necolas/normalize.css */html{line-height:1.15;-webkit-text-size-adjust:100%}body{margin:0}pre{font-family:monospace,monospace;font-size:1em}strong{font-weight:bolder}code{font-family:monospace,monospace;font-size:1em}img{border-style:none}::-webkit-file-upload-button{-webkit-appearance:button;font:inherit}p,pre{margin:0}html{font-family:system-ui,-apple-system,BlinkMacSystemFont,"Segoe UI",Roboto,"Helvetica Neue",Arial,"Noto Sans",sans-serif,"Apple Color Emoji","Segoe UI Emoji","Segoe UI Symbol","Noto Color Emoji";line-height:1.5}*,::after,::before{box-sizing:border-box;border-width:0;border-style:solid;border-color:#e2e8f0}img{border-style:solid}code,pre{font-family:Menlo,Monaco,Consolas,"Liberation Mono","Courier New",monospace}img{display:block;vertical-align:middle}img{max-width:100%;height:auto}.bg-gray-200{--bg-opacity:1;background-color:#edf2f7;background-color:rgba(237,242,247,var(--bg-opacity))}.bg-gray-300{--bg-opacity:1;background-color:#e2e8f0;background-color:rgba(226,232,240,var(--bg-opacity))}.border-gray-500{--border-opacity:1;border-color:#a0aec0;border-color:rgba(160,174,192,var(--border-opacity))}.rounded{border-radius:.25rem}.rounded-full{border-radius:9999px}.border-solid{border-style:solid}.border-t{border-top-width:1px}.inline-block{display:inline-block}.flex{display:flex}.justify-center{justify-content:center}.justify-around{justify-content:space-around}.font-serif{font-family:Georgia,Cambria,"Times New Roman",Times,serif}.font-semibold{font-weight:600}.font-bold{font-weight:700}.h-64{height:16rem}.text-sm{font-size:.875rem}.text-base{font-size:1rem}.text-xl{font-size:1.25rem}.m-2{margin:.5rem}.mr-2{margin-right:.5rem}.mb-3{margin-bottom:.75rem}.max-w-sm{max-width:24rem}.object-contain{-o-object-fit:contain;object-fit:contain}.overflow-hidden{overflow:hidden}.p-1{padding:.25rem}.py-1{padding-top:.25rem;padding-bottom:.25rem}.px-3{padding-left:.75rem;padding-right:.75rem}.py-4{padding-top:1rem;padding-bottom:1rem}.px-4{padding-left:1rem;padding-right:1rem}.px-6{padding-left:1.5rem;padding-right:1.5rem}.pl-3{padding-left:.75rem}.shadow-lg{box-shadow:0 10px 15px -3px rgba(0,0,0,.1),0 4px 6px -2px rgba(0,0,0,.05)}.text-center{text-align:center}.text-gray-700{--text-opacity:1;color:#4a5568;color:rgba(74,85,104,var(--text-opacity))}.text-gray-800{--text-opacity:1;color:#2d3748;color:rgba(45,55,72,var(--text-opacity))}.w-full{width:100%}.text-tiny{font-size:.5rem!important}p.new{padding-top:.5em;padding-bottom:.5em}code{font-size:14px}.code-large{background:#f4f4f4;font-family:monospace;font-size:14px;margin:10px;font-size:16px}.code{background:#f4f4f4;border:1px solid #ddd;border-left:3px solid #f36d33;color:#666;page-break-inside:avoid;font-family:monospace;font-size:14px;min-width:710px;max-width:710px;line-height:1.6;margin-bottom:1.6em;padding:1em 1.5em;display:block;-moz-tab-size:2;-o-tab-size:2;tab-size:2;overflow-x:auto;word-wrap:break-word;white-space:pre-wrap}
section:nth-child(2){display:block;}
section:not(:nth-child(2)){display:none;}</style><script src="https://kit.fontawesome.com/7efc4bcee2.js" crossOrigin="anonymous"></script></head><body class="lesson"><div id="__next"><div class="main-content bg-gray-200"><div class="p-1 font-serif"><div class="pl-3 text-xl"><div><section><div class="bg-gray-200 flex justify-center"><div class="max-w-sm rounded overflow-hidden shadow-lg"><div>¬†</div><img class="object-contain h-64 w-full" src="https://raw.githubusercontent.com/NSF-EC/INFO490Assets/master/src/assets/images/dmap/topics/TextTripping.png" alt="Text"/><div class="px-6 py-4"><div class="text-center font-bold text-xl">TF‚Ä¢IDF</div><p class="text-center text-gray-800 text-xl">Finding the Important Words</p><div class="text-gray-700 text-base">¬†</div><div class="text-center mb-3"><span class="inline-block bg-gray-300 rounded-full px-3 py-1 text-sm font-semibold text-gray-700 mr-2">#info490</span><span class="inline-block bg-gray-300 rounded-full px-3 py-1 text-sm font-semibold text-gray-700 mr-2">#python</span><span class="inline-block bg-gray-300 rounded-full px-3 py-1 text-sm font-semibold text-gray-700 mr-2">#text</span></div><div class="flex border-t border-solid border-gray-500 justify-around bg-gray-200"><div class="text-gray-700 text-center px-4 m-2 text-sm">D.MÔπ†üêç</div><div class="text-gray-700 text-center px-4 m-2 text-sm">Version: <!-- -->Fall 2020<!-- --> <!-- -->Fall 2020</div></div><div class="text-gray-700 text-center text-tiny">All Rights Reserved</div></div></div></div></section><section><p class="new"><strong>TF-IDF</strong><br/>Imagine having an algorithm that can automatically determine which words of a novel are unique to each chapter as well as 
filter out the words that are common across all chapters without bootstrapping the algorithm with a set of stop words.<br/>That does sound good.</p><i class="far fa-folder"></i><p class="new">The idea of ranking each term (i.e. a single word or an n-gram) within the context of a set of documents (or 
chapters in a novel) in which it appears is what the algorithm TF-IDF attempts to do. The Term Frequency Inverse 
Document Frequency is an information retrieval technique that scores each term by using its frequency (the TF weight) 
and its inverse document frequency (the IDF weight).  These two weights are multiplied together to get the TF*IDF score.<br/>Each term has a score and the score determines how valuable the term is in separating out a set of documents 
(called a corpus).  Variations of the tf-idf weighting scheme are often used by search engines as a central tool in 
scoring and ranking a document&#x27;s relevance given a user query.</p><p class="new"><strong>TF</strong><br/>We have already been calculating TF (term frequency) since we starting working with Python dictionaries and counting 
word occurrences.  A document in this context is very generic.  It can refer to an article, a book, a journal, 
a web page, a chapter, and even a sentence.  Usually a document needs to be long enough to have some context and a 
set of documents makes up the corpus.  In practice, a document is usually no smaller than a sentence or a set of 
sentences.  So in this context, TF would describe the frequency of a word within a document; it measures how common a 
term is within a document.  A term can be an n-gram as well.</p><p class="new"><strong>IDF</strong><br/>The IDF (inverse document frequency) weight isn&#x27;t as intuitive.  If TF measures commonness, IDF measure rarity.<br/>However, IDF is looking at a corpus (a set of documents unified by some attribute). We can illustrate this using the 
following table -- which lists the number of documents a term appears in.  Assume there are 100 million documents:</p><div class="code"><pre>term          # of documents
             containing term
----------------------------
a                100,000,000
boat               1,000,000
mobile               100,000
mobilegeddon           1,000
</pre></div><p class="new">Clearly, the term &#x27;a&#x27; provides no value in telling the documents apart.  However, since <code>&#x27;mobilegeddon&#x27;</code> 
only appears in 1,000 documents, that word does help separate a small set of the documents. To calculate the IDF of 
each term we take the the total number of documents and divide it by the count of document occurrences for that 
word (ie. how many documents contain the term).  Then we take the Log of that number to scale the result so that the 
weight is &#x27;dampened&#x27;.  This prevents interpreting that <code>&#x27;mobilegeddon&#x27;</code> is 1000 times more important than 
<code>&#x27;boat&#x27;</code>.</p><div class="code"><pre>term           n/df[j]               IDF[j]
a              Log(100M/100,000,000) -&gt; 0
boat           Log(100M/1,000,000)   -&gt; 1
mobile         Log(100M/100,000)     -&gt; 2
mobilegeddon   Log(100M/1,000)       -&gt; 3
</pre></div><p class="new">Hence, the following formula is the standard way to calculate IDF for term j. </p><img src="https://render.githubusercontent.com/render/math?math=\Huge idf_j%20%5C%2C%5C%2C%20%3D%20%5C%2C%5C%2C%20log(%5Cdfrac%7Bn%7D%7Bdf_j%7D)"/><p class="new">So IDF helps measure uniqueness or how much information a term provides.  For search engines, 
it helps identify what terms within each document are special. </p><p class="new"><strong>Not so Fast</strong><br/>At first glance, it might seem &#x27;easy&#x27; to calculate TF‚Ä¢IDF.  However, for both weights there are 
different ways to calculate them.   The TF we have been describing is called the raw count of a term in a document.<br/>A common alternative  is to scale the TF by the document length so that long documents don&#x27;t skew the 
results (e.g. a long chapter will have more words in it, but may be no more important than a shorter 
chapter). </p><code class="code-large">TF = raw count of term in a document/len(document)</code><p class="new">For IDF, there are many variants as well.  The one issue for calculating IDF is that the denominator could be zero (if 
a term does not show up in a document).  In most situations, all the terms are coming from the documents themselves, 
so this can never happen.  In any case, some implementations will calculate the &#x27;inverse&#x27; going from this equation: </p><code class="code-large">term_idf[term] = math.log10(N/term_count)</code><br/><p class="new">to this equation:<br/><code class="code-large">term_idf[term] = -1.0 * math.log10(term_count/N)

</code><br/></p><p class="new">If you remember your &#x27;log&#x27; rules (a small proof for the right hand side):<br/><span><img src="https://render.githubusercontent.com/render/math?math=\large 1.%5Cquad%20log(%5Cdfrac%7Ba%7D%7Bb%7D)%20%3D%20log(a)%20-%20log(b)" style="display:inline-block"/> (by definition)

</span><br/><span><img src="https://render.githubusercontent.com/render/math?math=\large 2.%5Cquad%20log(%5Cdfrac%7Bb%7D%7Ba%7D)%20%3D%20log(b)%20-%20log(a)" style="display:inline-block"/> (by definition)

</span><br/><span><img src="https://render.githubusercontent.com/render/math?math=\large 3.%5Cquad%20log(a)%20%3D%20log(b)%20-%20log(%5Cdfrac%7Bb%7D%7Ba%7D)" style="display:inline-block"/> (rearrange terms in #2)

</span><br/><span><img src="https://render.githubusercontent.com/render/math?math=\large 4.%5Cquad%20log(a)%20-%20log(b)%20%3D%20-log(%5Cdfrac%7Bb%7D%7Ba%7D)" style="display:inline-block"/> (rearrange terms in #3)

</span><br/><strong>SO</strong><br/><span><img src="https://render.githubusercontent.com/render/math?math=\large 5.%5Cquad%20log(%5Cdfrac%7Ba%7D%7Bb%7D)%20%3D%20-1.0%20%5Ctimes%20log(%5Cdfrac%7Bb%7D%7Ba%7D)" style="display:inline-block"/></span><br/></p><p class="new">You may also see adding 1 to the result or the following:<br/><code class="code-large">term_idf[term] = math.log10(N/(1 + term_count))</code><br/>to set a lower bound on common words and avoid giving a weight of zero to common terms. 
For the most part, if your documents are long and the number of terms large, the calculation will not matter for 
comparison purposes.  However, for simple test data, it can matter.  In practice, everyone seems to have their favorite tweaks 
to the classic formula. </p><p class="new"><strong>TF‚Ä¢IDF</strong><br/>So for each document in the corpus, the final calculation is TF * IDF for each term.  Values close to zero are 
your custom set of stop words -- words that did not help separate out the documents. One subtle 
reminder is that TF is calculated per document and IDF is calculated across a set of documents. </p><p class="new"><strong>Stop Words Reprise</strong><br/>Sometimes it is still useful to remove stop words from the document for the TF‚Ä¢IDF calculations.  In this case, 
the common words that TF‚Ä¢IDF finds are common themes/terms/ngrams that occur throughout the corpus. 
In the case of using TF‚Ä¢IDF to analyze novels, it might be important to know that &#x27;Tom&#x27; is a term that doesn&#x27;t 
differentiate chapters, but is seen as a common theme.  We could use that in our essay that we are suppose to write 
for English, but only had time to let Python &#x27;read&#x27; it. </p><p class="new"><strong>How Fast?</strong><br/>TF‚Ä¢IDF involves lots of loops and in our Python only version, it can take a long time.  However, once you have 
finished, each document can now be represented as a compact vector (set of values).  Algorithms that 
cluster (grouping documents based on some similarity measure), classify (assign a label) or topic modeling 
sometimes use the TF‚Ä¢IDF representation as the document.</p><p class="new"><strong>Topic Modeling</strong><br/>You may be wondering how TF‚Ä¢IDF is related to topic modeling.  If you aren&#x27;t, then you may not know what 
topic modeling is.  Topic modeling attempts to find a set of themes (e.g. topics) that describe a set of documents.
In the context of topic modeling, a topic is a probability distribution over the vocabulary (i.e. words) and a
document is a probability distribution over topics.  You can think of a topic as a set of words that co-occur
together and ideally, the topics are as distinct as possible.  There are many topic modeling algorithms
with LDA (latent dirichlet allocation) being one of the more popular ones. You may think of the top N TF‚Ä¢IDF words for a document as a &#x27;topic&#x27;; however they are not the same.  As mentioned
above, some topic modeling algorithms will use the TF‚Ä¢IDF scores as the input for algorithm.</p></section><section>FOOTER</section></div></div></div></div></div></body></html>