<!DOCTYPE html><html lang='en'><head><meta charset="utf-8"><style>/*! normalize.css v8.0.1 | MIT License | github.com/necolas/normalize.css */html{line-height:1.15;-webkit-text-size-adjust:100%}body{margin:0}h1{font-size:2em;margin:.67em 0}pre{font-family:monospace,monospace;font-size:1em}strong{font-weight:bolder}code{font-family:monospace,monospace;font-size:1em}img{border-style:none}input{font-family:inherit;font-size:100%;line-height:1.15;margin:0}input{overflow:visible}[type=checkbox]{box-sizing:border-box;padding:0}::-webkit-file-upload-button{-webkit-appearance:button;font:inherit}h1,h2,p,pre{margin:0}html{font-family:system-ui,-apple-system,BlinkMacSystemFont,"Segoe UI",Roboto,"Helvetica Neue",Arial,"Noto Sans",sans-serif,"Apple Color Emoji","Segoe UI Emoji","Segoe UI Symbol","Noto Color Emoji";line-height:1.5}*,::after,::before{box-sizing:border-box;border-width:0;border-style:solid;border-color:#e2e8f0}img{border-style:solid}input:-ms-input-placeholder{color:#a0aec0}input::-ms-input-placeholder{color:#a0aec0}input::-webkit-input-placeholder{color:#a0aec0}input::-moz-placeholder{color:#a0aec0}h1,h2{font-size:inherit;font-weight:inherit}input{padding:0;line-height:inherit;color:inherit}code,pre{font-family:Menlo,Monaco,Consolas,"Liberation Mono","Courier New",monospace}img{display:block;vertical-align:middle}img{max-width:100%;height:auto}.container{width:100%}@media (min-width:640px){.container{max-width:640px}}@media (min-width:768px){.container{max-width:768px}}@media (min-width:1024px){.container{max-width:1024px}}@media (min-width:1280px){.container{max-width:1280px}}.bg-gray-200{--bg-opacity:1;background-color:#edf2f7;background-color:rgba(237,242,247,var(--bg-opacity))}.bg-gray-300{--bg-opacity:1;background-color:#e2e8f0;background-color:rgba(226,232,240,var(--bg-opacity))}.bg-orange-300{--bg-opacity:1;background-color:#fbd38d;background-color:rgba(251,211,141,var(--bg-opacity))}.bg-green-200{--bg-opacity:1;background-color:#c6f6d5;background-color:rgba(198,246,213,var(--bg-opacity))}.border-gray-500{--border-opacity:1;border-color:#a0aec0;border-color:rgba(160,174,192,var(--border-opacity))}.border-indigo-500{--border-opacity:1;border-color:#667eea;border-color:rgba(102,126,234,var(--border-opacity))}.rounded{border-radius:.25rem}.rounded-full{border-radius:9999px}.border-solid{border-style:solid}.border-l-2{border-left-width:2px}.border-t{border-top-width:1px}.cursor-pointer{cursor:pointer}.block{display:block}.inline-block{display:inline-block}.flex{display:flex}.justify-center{justify-content:center}.justify-around{justify-content:space-around}.font-sans{font-family:system-ui,-apple-system,BlinkMacSystemFont,"Segoe UI",Roboto,"Helvetica Neue",Arial,"Noto Sans",sans-serif,"Apple Color Emoji","Segoe UI Emoji","Segoe UI Symbol","Noto Color Emoji"}.font-serif{font-family:Georgia,Cambria,"Times New Roman",Times,serif}.font-semibold{font-weight:600}.font-bold{font-weight:700}.h-48{height:12rem}.h-64{height:16rem}.text-sm{font-size:.875rem}.text-base{font-size:1rem}.text-xl{font-size:1.25rem}.leading-normal{line-height:1.5}.m-2{margin:.5rem}.mr-2{margin-right:.5rem}.mb-3{margin-bottom:.75rem}.ml-3{margin-left:.75rem}.mt-4{margin-top:1rem}.max-w-sm{max-width:24rem}.object-contain{-o-object-fit:contain;object-fit:contain}.opacity-0{opacity:0}.overflow-hidden{overflow:hidden}.p-1{padding:.25rem}.p-3{padding:.75rem}.py-1{padding-top:.25rem;padding-bottom:.25rem}.px-3{padding-left:.75rem;padding-right:.75rem}.py-4{padding-top:1rem;padding-bottom:1rem}.px-4{padding-left:1rem;padding-right:1rem}.px-6{padding-left:1.5rem;padding-right:1.5rem}.pl-3{padding-left:.75rem}.absolute{position:absolute}.shadow-md{box-shadow:0 4px 6px -1px rgba(0,0,0,.1),0 2px 4px -1px rgba(0,0,0,.06)}.shadow-lg{box-shadow:0 10px 15px -3px rgba(0,0,0,.1),0 4px 6px -2px rgba(0,0,0,.05)}.text-center{text-align:center}.text-gray-700{--text-opacity:1;color:#4a5568;color:rgba(74,85,104,var(--text-opacity))}.text-gray-800{--text-opacity:1;color:#2d3748;color:rgba(45,55,72,var(--text-opacity))}.w-2\/3{width:66.666667%}.w-full{width:100%}@media (min-width:768px){.md\:w-2\/3{width:66.666667%}}.text-tiny{font-size:.5rem!important}.lesson{padding-left:10px;padding-right:10px;--bg-opacity:1;background-color:#edf2f7;background-color:rgba(237,242,247,var(--bg-opacity))}h2{font-family:Times}p.new{padding-top:.5em;padding-bottom:.5em}h1{margin-top:1.5em}h2{margin-top:1em}code{font-size:14px}pre code{background:#f4f4f4;font-family:monospace;font-size:14px}.code-large{background:#f4f4f4;font-family:monospace;font-size:14px;margin:10px;font-size:16px}.code,pre code{background:#f4f4f4;font-family:monospace;font-size:14px;border:1px solid #ddd;border-left:3px solid #f36d33;color:#666;page-break-inside:avoid;min-width:710px;max-width:710px;line-height:1.6;margin-bottom:1.6em;padding:1em 1.5em;display:block;-moz-tab-size:2;-o-tab-size:2;tab-size:2;overflow-x:auto;word-wrap:break-word;white-space:pre-wrap}.tab{font-size:1rem;border-color:#8c6728}.tab-content{max-height:0;transition:max-height .35s}.tab input:checked~.tab-content{max-height:100vh}.tab input:checked+label{padding:1rem;border-left-width:2px;border-color:#6574cd;background-color:#f8fafc;color:#6574cd}.tab label::after{float:right;right:0;top:0;display:block;width:1em;height:1.5em;line-height:1.5;font-size:1rem;text-align:center;transition:all .35s}.tab input[type=checkbox]+label::after{content:"+";font-weight:700;border-width:1px;border-radius:9999px;border-color:#8c6728}.tab input[type=checkbox]:checked+label::after{transform:rotate(315deg);background-color:#6574cd;color:#f8fafc}
div.code-starter {display:none;}</style><script src="https://kit.fontawesome.com/7efc4bcee2.js" crossOrigin="anonymous"></script></head><body class = "lesson apples "><div class="main-content bg-gray-200 p-1 pl-3 text-xl font-serif"><div><h1 class="section" id="section3"><strong>IDF</strong></h1><p class="new">The IDF (inverse document frequency) weight isn't as intuitive.  If TF measures commonness, IDF measure rarity. 
However, IDF is looking at a corpus (a set of documents unified by some attribute). We can illustrate this using the 
following table -- which lists the number of documents a term appears in.  Assume there are 100 million documents:</p><pre><code>term          # of documents
             containing term
----------------------------
a                100,000,000
boat               1,000,000
mobile               100,000
mobilegeddon           1,000</code></pre><p class="new">Clearly, the term 'a' provides no value in telling the documents apart.  However, since <code>'mobilegeddon'</code> 
only appears in 1,000 documents, that word does help separate a small set of the documents. To calculate the IDF of 
each term we take the the total number of documents and divide it by the count of document occurrences for that 
word (ie. how many documents contain the term).  Then we take the Log of that number to scale the result so that the 
weight is 'dampened'.  This prevents interpreting that <code>'mobilegeddon'</code> is 1000 times more important than 
<code>'boat'</code>.</p><pre><code>term           n/df[j]               IDF[j]
a              Log(100M/100,000,000) -&gt; 0
boat           Log(100M/1,000,000)   -&gt; 1
mobile         Log(100M/100,000)     -&gt; 2
mobilegeddon   Log(100M/1,000)       -&gt; 3</code></pre><p class="new">Hence, the following formula is the standard way to calculate IDF for term j. </p><img class="" src="https://render.githubusercontent.com/render/math?math=\Huge idf_j%20%5C%2C%5C%2C%20%3D%20%5C%2C%5C%2C%20log(%5Cdfrac%7Bn%7D%7Bdf_j%7D)"/><p class="new">So IDF helps measure uniqueness or how much information a term provides.  For search engines, 
it helps identify what terms within each document are special. </p><h2 id="not-so-fast"><strong>Not so Fast</strong></h2><p class="new">At first glance, it might seem 'easy' to calculate TFâ€¢IDF. However, for both weights there are 
different ways to calculate them.   The TF we have been describing is called the raw count of a term in a document.<br/>A common alternative  is to scale the TF by the document length so that long documents don't skew the 
results (e.g. a long chapter will have more words in it, but may be no more important than a shorter 
chapter). </p><code class="code-large">TF = (raw count of term in a document)/len(document)</code><p class="new">For IDF, there are many variants as well.  The one issue for calculating IDF is that the denominator could be zero (if 
a term does not show up in a document).  In most situations, all the terms are coming from the documents themselves, 
so this can never happen.  In any case, some implementations will calculate the 'inverse' going from this equation: </p><code class="code-large">term_idf[term] = math.log10(N/term_count)</code><br/><p class="new">term_idf[term] = math.log10(N/term_count)&lt;/code&gt;<br/></p><p class="new">to this equation:<br/><code class="code-large">term_idf[term] = -1.0 * math.log10(term_count/N)

</code><br/></p><p class="new">If you remember your 'log' rules (here's a small 'proof' for the right hand side):<br/><span><img class="" src="https://render.githubusercontent.com/render/math?math=\large 1.%5Cquad%20log(%5Cdfrac%7Ba%7D%7Bb%7D)%20%3D%20log(a)%20-%20log(b)" style="display:inline-block"/> (by definition)

</span><br/><span><img class="" src="https://render.githubusercontent.com/render/math?math=\large 2.%5Cquad%20log(%5Cdfrac%7Bb%7D%7Ba%7D)%20%3D%20log(b)%20-%20log(a)" style="display:inline-block"/> (by definition)

</span><br/><span><img class="" src="https://render.githubusercontent.com/render/math?math=\large 3.%5Cquad%20log(a)%20%3D%20log(b)%20-%20log(%5Cdfrac%7Bb%7D%7Ba%7D)" style="display:inline-block"/> (rearrange terms in #2)

</span><br/><span><img class="" src="https://render.githubusercontent.com/render/math?math=\large 4.%5Cquad%20log(a)%20-%20log(b)%20%3D%20-log(%5Cdfrac%7Bb%7D%7Ba%7D)" style="display:inline-block"/> (rearrange terms in #3)

</span><br/><strong>SO</strong><br/><span><img class="" src="https://render.githubusercontent.com/render/math?math=\large 5.%5Cquad%20log(%5Cdfrac%7Ba%7D%7Bb%7D)%20%3D%20-1.0%20%5Ctimes%20log(%5Cdfrac%7Bb%7D%7Ba%7D)" style="display:inline-block"/></span><br/></p><p class="new">You may also see adding 1 to the result or the following:<br/><code class="code-large">term_idf[term] = math.log10(N/(1 + term_count))</code><br/>to set a lower bound on common words and avoid giving a weight of zero to common terms. 
For the most part, if your documents are long and the number of terms large, the calculation will not matter for 
comparison purposes.  However, for simple test data, it can matter.  In practice, everyone seems to have their favorite tweaks 
to the classic formula. </p><h2 id="tfidf"><strong>TFâ€¢IDF</strong></h2><p class="new">So for each document in the corpus, the final calculation is TF * IDF for each term.  Values close to zero are 
your custom set of stop words -- words that did not help separate out the documents. One subtle 
reminder is that TF is calculated per document and IDF is calculated across a set of documents. </p><h2 id="stop-words-reprise"><strong>Stop Words Reprise</strong></h2><p class="new">Sometimes it is still useful to remove stop words from the document for the TFâ€¢IDF calculations.  In this case, 
the common words that TFâ€¢IDF finds are common themes/terms/ngrams that occur throughout the corpus. 
In the case of using TFâ€¢IDF to analyze novels, it might be important to know that 'Tom' is a term that doesn't 
differentiate chapters, but is seen as a common theme.  We could use that in our essay that we are suppose to write 
for English, but only had time to let Python 'read' it. </p><h2 id="how-fast"><strong>How Fast?</strong></h2><p class="new">TFâ€¢IDF involves lots of loops and in our Python only version, it can take a long time.  However, once you have 
finished, each document can now be represented as a compact vector (set of values).  Algorithms that 
cluster (grouping documents based on some similarity measure), classify (assign a label) or topic modeling 
sometimes use the TFâ€¢IDF representation as the document.</p><h2 id="topic-modeling"><strong>Topic Modeling</strong></h2><p class="new">You may be wondering how TFâ€¢IDF is related to topic modeling.  If you aren't, then you may not know what 
topic modeling is.  Topic modeling attempts to find a set of themes (e.g. topics) that describe a set of documents.
In the context of topic modeling, a topic is a probability distribution over the vocabulary (i.e. words) and a
document is a probability distribution over topics.  You can think of a topic as a set of words that co-occur
together and ideally, the topics are as distinct as possible.  There are many topic modeling algorithms
with LDA (latent dirichlet allocation) being one of the more popular ones. You may think of the top N TFâ€¢IDF words for a document as a 'topic'; however they are not the same.  As mentioned
above, some topic modeling algorithms will use the TFâ€¢IDF scores as the input for algorithm.</p></div></div></body></html>