<!DOCTYPE html><html lang='en'><head><title>TF‚Ä¢IDF</title><meta charset="utf-8"><style>/*! normalize.css v8.0.1 | MIT License | github.com/necolas/normalize.css */html{line-height:1.15;-webkit-text-size-adjust:100%}body{margin:0}h1{font-size:2em;margin:.67em 0}pre{font-family:monospace,monospace;font-size:1em}strong{font-weight:bolder}code{font-family:monospace,monospace;font-size:1em}img{border-style:none}input{font-family:inherit;font-size:100%;line-height:1.15;margin:0}input{overflow:visible}[type=checkbox]{box-sizing:border-box;padding:0}::-webkit-file-upload-button{-webkit-appearance:button;font:inherit}h1,h2,p,pre{margin:0}ol{list-style:none;margin:0;padding:0}html{font-family:system-ui,-apple-system,BlinkMacSystemFont,"Segoe UI",Roboto,"Helvetica Neue",Arial,"Noto Sans",sans-serif,"Apple Color Emoji","Segoe UI Emoji","Segoe UI Symbol","Noto Color Emoji";line-height:1.5}*,::after,::before{box-sizing:border-box;border-width:0;border-style:solid;border-color:#e2e8f0}img{border-style:solid}input:-ms-input-placeholder{color:#a0aec0}input::-ms-input-placeholder{color:#a0aec0}input::-webkit-input-placeholder{color:#a0aec0}input::-moz-placeholder{color:#a0aec0}h1,h2{font-size:inherit;font-weight:inherit}input{padding:0;line-height:inherit;color:inherit}code,pre{font-family:Menlo,Monaco,Consolas,"Liberation Mono","Courier New",monospace}img{display:block;vertical-align:middle}img{max-width:100%;height:auto}.container{width:100%}@media (min-width:640px){.container{max-width:640px}}@media (min-width:768px){.container{max-width:768px}}@media (min-width:1024px){.container{max-width:1024px}}@media (min-width:1280px){.container{max-width:1280px}}.bg-gray-200{--bg-opacity:1;background-color:#edf2f7;background-color:rgba(237,242,247,var(--bg-opacity))}.bg-gray-300{--bg-opacity:1;background-color:#e2e8f0;background-color:rgba(226,232,240,var(--bg-opacity))}.bg-orange-300{--bg-opacity:1;background-color:#fbd38d;background-color:rgba(251,211,141,var(--bg-opacity))}.bg-green-200{--bg-opacity:1;background-color:#c6f6d5;background-color:rgba(198,246,213,var(--bg-opacity))}.border-gray-400{--border-opacity:1;border-color:#cbd5e0;border-color:rgba(203,213,224,var(--border-opacity))}.border-gray-500{--border-opacity:1;border-color:#a0aec0;border-color:rgba(160,174,192,var(--border-opacity))}.border-indigo-500{--border-opacity:1;border-color:#667eea;border-color:rgba(102,126,234,var(--border-opacity))}.rounded{border-radius:.25rem}.rounded-full{border-radius:9999px}.border-solid{border-style:solid}.border-l-2{border-left-width:2px}.border-t{border-top-width:1px}.cursor-pointer{cursor:pointer}.block{display:block}.inline-block{display:inline-block}.flex{display:flex}.justify-center{justify-content:center}.justify-around{justify-content:space-around}.float-left{float:left}.font-sans{font-family:system-ui,-apple-system,BlinkMacSystemFont,"Segoe UI",Roboto,"Helvetica Neue",Arial,"Noto Sans",sans-serif,"Apple Color Emoji","Segoe UI Emoji","Segoe UI Symbol","Noto Color Emoji"}.font-serif{font-family:Georgia,Cambria,"Times New Roman",Times,serif}.font-semibold{font-weight:600}.font-bold{font-weight:700}.h-64{height:16rem}.text-sm{font-size:.875rem}.text-base{font-size:1rem}.text-xl{font-size:1.25rem}.leading-normal{line-height:1.5}.m-2{margin:.5rem}.mt-0{margin-top:0}.mt-1{margin-top:.25rem}.mt-2{margin-top:.5rem}.mr-2{margin-right:.5rem}.mt-3{margin-top:.75rem}.mr-3{margin-right:.75rem}.mb-3{margin-bottom:.75rem}.ml-3{margin-left:.75rem}.mt-4{margin-top:1rem}.mt-6{margin-top:1.5rem}.max-w-sm{max-width:24rem}.max-w-2xl{max-width:42rem}.max-w-4xl{max-width:56rem}.object-contain{-o-object-fit:contain;object-fit:contain}.opacity-0{opacity:0}.overflow-hidden{overflow:hidden}.p-1{padding:.25rem}.p-3{padding:.75rem}.py-1{padding-top:.25rem;padding-bottom:.25rem}.px-3{padding-left:.75rem;padding-right:.75rem}.py-4{padding-top:1rem;padding-bottom:1rem}.px-4{padding-left:1rem;padding-right:1rem}.px-6{padding-left:1.5rem;padding-right:1.5rem}.pl-3{padding-left:.75rem}.absolute{position:absolute}.shadow-md{box-shadow:0 4px 6px -1px rgba(0,0,0,.1),0 2px 4px -1px rgba(0,0,0,.06)}.shadow-lg{box-shadow:0 10px 15px -3px rgba(0,0,0,.1),0 4px 6px -2px rgba(0,0,0,.05)}.text-center{text-align:center}.text-gray-700{--text-opacity:1;color:#4a5568;color:rgba(74,85,104,var(--text-opacity))}.text-gray-800{--text-opacity:1;color:#2d3748;color:rgba(45,55,72,var(--text-opacity))}.whitespace-no-wrap{white-space:nowrap}.w-2\/3{width:66.666667%}.w-full{width:100%}@media (min-width:768px){.md\:w-2\/3{width:66.666667%}}.text-tiny{font-size:.5rem!important}.lesson{padding-left:10px;padding-right:10px;--bg-opacity:1;background-color:#edf2f7;background-color:rgba(237,242,247,var(--bg-opacity))}.lesson-footer{margin-top:50px;margin-top:20px}p.new{padding-top:.5em;padding-bottom:.5em}.h-18rem{height:17rem}h1,h2{font-weight:700;margin-bottom:.5em;font-family:Georgia,Cambria,"Times New Roman",Times,serif!important}h1{margin-top:1.5em;font-size:2em!important}h2{margin-top:1em;font-size:1.5em!important}pre code{font-size:15px}p code{font-size:smaller}.code{background:#f4f4f4;font-family:monospace;font-size:15px;-webkit-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none;cursor:default;touch-action:none;-webkit-touch-callout:none;-webkit-tap-highlight-color:transparent;border:1px solid #ddd;color:#666;page-break-inside:avoid;display:block;min-width:740px;max-width:740px;line-height:1.6;margin-bottom:1.6em;padding:1em 1.5em;-moz-tab-size:2;-o-tab-size:2;tab-size:2;overflow-x:auto;word-wrap:break-word;white-space:pre-wrap;border-left:3px solid #f36d33!important}pre code:not(.line-number){background:#f4f4f4;font-family:monospace;font-size:15px;-webkit-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none;cursor:default;touch-action:none;-webkit-touch-callout:none;-webkit-tap-highlight-color:transparent;border:1px solid #ddd;color:#666;page-break-inside:avoid;display:block;min-width:740px;max-width:740px;line-height:1.6;margin-bottom:1.6em;padding:1em 1.5em;-moz-tab-size:2;-o-tab-size:2;tab-size:2;overflow-x:auto;word-wrap:break-word;white-space:pre-wrap;border-left:3px solid #f36d33}div.code-starter>pre code{border-left:3px solid #fdff44!important;background-image:radial-gradient(rgba(0,150,0,.75),#000 120%);color:#fff;font:.9rem Inconsolata,monospace}div.code-starter>pre code::after{content:"\a$_"}pre{counter-reset:line}code.line-number{counter-increment:line}code.line-number::before{padding:0 .5em;margin-right:.5em;border-right:1px solid #ddd}code.line-number:before{content:"0" counter(line)}code.line-number:nth-child(n+10):before{content:counter(line)}.tab{font-size:1rem;border-color:#8c6728}.tab-content{max-height:0;transition:max-height .35s}.tab input:checked~.tab-content{max-height:100vh}.tab input:checked+label{padding:1rem;border-left-width:2px;border-color:#6574cd;background-color:#f8fafc;color:#6574cd}.tab label::after{float:right;right:0;top:0;display:block;width:1em;height:1.5em;line-height:1.5;font-size:1rem;text-align:center;transition:all .35s}.tab input[type=checkbox]+label::after{content:"+";font-weight:700;border-width:1px;border-radius:9999px;border-color:#8c6728}.tab input[type=checkbox]:checked+label::after{transform:rotate(315deg);background-color:#6574cd;color:#f8fafc}</style><script src="https://kit.fontawesome.com/7efc4bcee2.js" crossOrigin="anonymous"></script></head><body class="lesson"><div class="main-content bg-gray-200 p-1 pl-3 text-xl font-serif"><div class="md-inner"><h1 class="overview"></h1><div class="lesson-overview bg-gray-200 flex justify-center"><div class="displaycard max-w-sm rounded overflow-hidden shadow-lg"><div>¬†</div><img alt="Text" class="object-contain h-64 w-full" src="https://github.com/NSF-EC/INFO490Assets/raw/master/src/dmap/lessons/tfidf/html/TextTripping-sm.png"/><div class="px-6 py-4"><div class="text-center font-bold text-xl">TF‚Ä¢IDF</div><p class="text-center text-gray-800 text-xl">Finding the Important Words</p><div class="text-gray-700 text-base">¬†</div><div class="text-center mb-3"><span class="inline-block bg-gray-300 rounded-full px-3 py-1 text-sm font-semibold text-gray-700 mr-2">#info490</span><span class="inline-block bg-gray-300 rounded-full px-3 py-1 text-sm font-semibold text-gray-700 mr-2">#python</span><span class="inline-block bg-gray-300 rounded-full px-3 py-1 text-sm font-semibold text-gray-700 mr-2">#text</span></div><div class="flex border-t border-solid border-gray-500 justify-around bg-gray-200"><div class="text-gray-700 text-center px-4 m-2 text-sm"><span class="whitespace-no-wrap">D.MÔπ†üêç</span></div><div class="text-gray-700 text-center px-4 m-2 text-sm"><span class="whitespace-no-wrap"><strong>Version:</strong> <!-- -->Fall 2020<!-- --> <!-- -->08.20.20</span></div></div><div class="text-gray-700 text-center text-tiny">All Rights Reserved</div></div></div><div class="ml-3 mb-3 h-18rem rounded overflow-hidden shadow-lg"><div class="px-6 py-4"><div class="text-center font-bold text-xl">Colab Notes</div><p class="max-w-sm text-gray-800 text-sm">1. <strong>Copy</strong> this notebook into your google drive</p><p class="max-w-sm text-gray-800 text-sm">2. <strong>Share</strong> the notebook, and copy the share ID</p><p class="max-w-sm text-gray-800 text-sm">3. <strong>Set</strong> the NOTEBOOK_ID variable to the share ID. ¬†<strong>And SAVE the notebook</strong> (again)</p><p class="max-w-sm text-gray-800 text-sm">4. <strong>Run</strong> the code cell that installs the INFO 490 IDE</p><div class="text-center font-bold text-xl">¬†</div><div class="text-center font-bold text-xl">Jupyter Notes</div><p class="max-w-sm text-gray-800 text-sm">The testing framework does <strong>not work</strong> (at this time) for Jupyter  notebooks or local code development</p></div></div></div><h1 class="section" id="section1">TF-IDF</h1><p class="new">Imagine having an algorithm that can automatically determine which words of a novel are 
unique/important to each chapter as well as filter out the words that are common across all chapters 
without bootstrapping the algorithm with a set of stop words. If we needed an algorithm to 'read'
our novels for us, this sounds almost perfect.</p><p class="new">The idea of ranking each term (i.e. a single word or an n-gram) within the context of a 
set of documents (or chapters in a novel) in which it appears is what the algorithm TF-IDF attempts 
to do. The Term Frequency Inverse Document Frequency is an information retrieval technique that scores 
each term by using its frequency (the TF weight) and its inverse document frequency (the IDF weight) -- 
more on that soon.  These two weights are multiplied together to get the TF‚Ä¢IDF score.  </p><p class="new">Each term has a score and the score determines how valuable the term is in separating out a set of documents.<br/>Variations of the TF‚Ä¢IDF weighting scheme are often used by search engines as a central 
tool in scoring and ranking a document's relevance given a user query.</p><h2 id="documents">Documents</h2><img alt="green_eggs-300.png" class="mt-2 float-left mr-3 sm" src="https://github.com/NSF-EC/INFO490Assets/raw/master/src/dmap/lessons/tfidf/html/green_eggs-300.png"/><div class="pl-3 max-w-4xl mt-0">A document for our purposes is very generic.  It can refer to an article, a book, a journal, a web page, 
a chapter, and even a sentence.  Usually a document needs to be long enough to have some context and a set 
of documents makes up a <em>corpus</em>.  In practice, a document is usually no smaller than a sentence or a set 
of sentences.  We will use the following as a running example to help us understand how to build a TF‚Ä¢IDF model.
</div><p class="new">Assume you have the following corpus (a set of documents unified by some theme/attribute)</p><pre><code>c1 = "Do you like Green eggs and ham"
c2 = "I do not like them Sam I am  I do not like Green eggs and ham"
c3 = "Would you like them Here or there"
c4 = "I would not like them Here or there I would not like them Anywhere"

corpus = [c1, c2, c3, c4]</code></pre><p class="new">Let's write some code to turn this array of strings into a set of 'documents'. 
In the following cell, implement the function <code>split_into_tokens</code>.  It takes in a string and simply uses 
white-space to break the string into words (i.e. tokens).</p><p class="new">Once that is done, you should see 7 words (i.e. tokens).  If you process all the
documents, you can confirm (using <code>set</code>) that there are 44 words (17 unique).</p><div class="ide code-starter blink"><pre><code>import LessonUtil as Util

def get_corpus():

    c1 = "Do you like Green eggs and ham"
    c2 = "I do not like them Sam I am  I do not like Green eggs and ham"
    c3 = "Would you like them Here or there"
    c4 = "I would not like them Here or there I would not like them Anywhere"
    return [c1, c2, c3, c4]
    
def split_into_tokens(data, normalize=True, min_length=0):

   # returns an array of words
   # each word is simply a string
  
   # splits the incoming data (a string) based on white-space
   # if normalize is True, normalize the case of the words
   # only return those words/tokens longer than min_length
   return []
  
def test_split():
  corpus   = get_corpus()
  doc1     = corpus[0]
  print(split_into_tokens(doc1))
  
test_split()

# also, use the test framework to test
# ide.tester.test_function(split_into_tokens)</code></pre></div><h2 id="tf">TF</h2><p class="new">We have already been calculating TF (term frequency) since we started working with Python dictionaries and 
counting word occurrences.  So for us, TF would describe the frequency of a word within a document; it 
measures how common a term is within a document.  It is the ratio of number of times the word 
appears in a document compared to the total number of words in that document:</p><img alt="math?math=%5CLarge%20%5Ctext%7B%0Atf(t%2Cd)%20%3D%20(count%20of%20term%20t%20in%20document%20d)%20%2F%20(number%20of%20words%20in%20document%20d)%0A%7D%0A" class="jax" src="https://render.githubusercontent.com/render/math?math=%5CLarge%20%5Ctext%7B%0Atf(t%2Cd)%20%3D%20(count%20of%20term%20t%20in%20document%20d)%20%2F%20(number%20of%20words%20in%20document%20d)%0A%7D%0A"/><img alt="math?math=%5CHuge%20tf_%7Bt%2Cd%7D%20%3D%20%5Cdfrac%7Bn_%7Bt%2Cd%7D%7D%7B%5Csum_%7B%7D%5E%7B%7D%20n_%7Bt%2Cd%7D%7D" class="mt-3 mb-3" src="https://render.githubusercontent.com/render/math?math=%5CHuge%20tf_%7Bt%2Cd%7D%20%3D%20%5Cdfrac%7Bn_%7Bt%2Cd%7D%7D%7B%5Csum_%7B%7D%5E%7B%7D%20n_%7Bt%2Cd%7D%7D"/><ol start="1"><li>‚Ä¢ <img alt="math?math=%5CLarge%20n_%7Bt%2Cd%7D" class="jax" src="https://render.githubusercontent.com/render/math?math=%5CLarge%20n_%7Bt%2Cd%7D" style="display:inline-block"/> (the numerator) is the number of times term <strong>t</strong> appears 
in document <strong>d</strong>.</li><li>‚Ä¢ Each document has its own TF.</li><li>‚Ä¢ A term can be an n-gram as well.</li></ol><p class="new">For this lesson, we will implement the TF data structure as a list of dictionaries (actually <code>collections.Counter</code>).
Each counter (i.e document) associates the word (the key) to it count (the value).<br/>For this example, <code>tf[1]['like']</code> would be the TF for the first document
for the word 'like' (i.e. 2/len(document1)). </p><p class="new">In the following code cell, implement the function named <code>build_tf</code></p><pre><code>import collections

def build_tf(corpus, min_length=0):

   # corpus is an list of documents
   # a document is an unparsed string of words
   
   # returns a tuple with two items:
   # first item is a collections.Counter 
   # for all the words (&gt; min_length) in the corpus
   vocab = collections.Counter
   tfs = None 
  
   # the second item is the TF array, one for each document
   # each TF is a collection.Counter, 
   # each counter maps a word to the relative frequency 
   # of each word in that document 
   
   return  vocab, tfs</code></pre><p class="new">A few notes.</p><ol start="1"><li>‚Ä¢ Use <img alt="math?math=%5Clarge%20tf_%7Bt%2Cd%7D%20%3D%20%5Cdfrac%7Bn_%7Bt%2Cd%7D%7D%7B%5Csum_%7B%7D%5E%7B%7D%20n_%7Bt%2Cd%7D%7D" class="jax" src="https://render.githubusercontent.com/render/math?math=%5Clarge%20tf_%7Bt%2Cd%7D%20%3D%20%5Cdfrac%7Bn_%7Bt%2Cd%7D%7D%7B%5Csum_%7B%7D%5E%7B%7D%20n_%7Bt%2Cd%7D%7D" style="display:inline-block"/></li><li>‚Ä¢ Each TF is a collections.Counter, as is the corpus vocabulary</li><li>‚Ä¢ <code>vocab['eggs']/len(vocab)</code> is frequency of 'eggs' across all documents (chapters)</li><li>‚Ä¢ <code>tf[0]['eggs']</code> is the relative frequency of 'eggs' in the first document (chapter)</li></ol><div class="ide code-starter blink"><pre><code>import collections

def build_tf(corpus, min_length=0):

   # corpus is an list of documents
   # a document is an unparsed string of words
   return  None, None
   
def test_tf():
  corpus = get_corpus()
  vocab, tf = build_tf(corpus)

# test_tf()</code></pre></div><p class="new">After you are done, you can compare your results to the chart below.</p><img alt="tf-gham.png" class="w-full" src="https://github.com/NSF-EC/INFO490Assets/raw/master/src/dmap/lessons/tfidf/html/tf-gham.png"/><p class="new">Note that the word 'like' is the only word that appears in all the documents.
You can also test it against the grader:</p><pre><code>print(vocab.most_common(5)) # 5 most common words
print(tf[1].most_common(5)) # 5 most common words in second document
print(tf[1]['eggs']         # relative freq of 'eggs' in the second document

ide.tester.test_function(build_tf)</code></pre><h2 id="idf">IDF</h2><p class="new">The IDF (inverse document frequency) weight isn't as intuitive.  TF measures commonness; 
IDF measure rarity. However, IDF is looking at a corpus. We can illustrate this using the 
following table -- which lists the number of documents a term appears in.  Assume there 
are 100 million documents:</p><pre><code>term          # of documents
             containing term
----------------------------
a                100,000,000
boat               1,000,000
mobile               100,000
mobilegeddon           1,000</code></pre><p class="new">Clearly, the term 'a' provides no value in telling the documents apart.  However, since 'mobilegeddon' 
only appears in 1,000 documents, that word does help separate a small set of the documents. To calculate the 
IDF of each term we take the the total number of documents and divide it by the count of document 
occurrences for that word (ie. how many documents contain the term).  Then we take the Log of that 
number to scale the result so that the weight is 'dampened'.  This prevents interpreting that 'mobilegeddon'
is 1000 times more important than 'boat'.</p><pre><code>term               n/df[j]            IDF[j]
a              Log(100M/100,000,000) -&gt; 0
boat           Log(100M/1,000,000)   -&gt; 1
mobile         Log(100M/100,000)     -&gt; 2
mobilegeddon   Log(100M/1,000)       -&gt; 3</code></pre><p class="new">Hence, the following formula is the standard way to calculate IDF for term j:</p><img alt="math?math=%5CHuge%20idf_j%20%5C%2C%5C%2C%20%3D%20%5C%2C%5C%2C%20log(%5Cdfrac%7Bn%7D%7Bdf_j%7D)" class="jax" src="https://render.githubusercontent.com/render/math?math=%5CHuge%20idf_j%20%5C%2C%5C%2C%20%3D%20%5C%2C%5C%2C%20log(%5Cdfrac%7Bn%7D%7Bdf_j%7D)"/><ol start="1"><li>‚Ä¢ The value <img alt="math?math=%5CLarge%20n" class="jax" src="https://render.githubusercontent.com/render/math?math=%5CLarge%20n" style="display:inline-block"/> is the total number of documents</li><li>‚Ä¢ <img alt="math?math=%5CLarge%20df_j" class="jax" src="https://render.githubusercontent.com/render/math?math=%5CLarge%20df_j" style="display:inline-block"/> is the total number of documents that has term <code>j</code></li></ol><p class="new">So IDF helps measure uniqueness or how much information a term provides.  For search engines, 
it helps identify what terms within each document are special. </p><p class="new">Let's write the function <code>build_idf</code>:</p><p class="new">A few notes:</p><ol start="1"><li>‚Ä¢ Use math.log (natural logarithm)</li><li>‚Ä¢ IDF is computed once for all documents.</li></ol><div class="ide code-starter blink"><pre><code>import math
def build_idf(vocabulary, corpus_tf):

    # return a collection.Counter object
    # such that counter[term] is the idf for that term
    
    term_idf = collections.Counter()
    
    #
    # code in the magic
    #
    
    return term_idf</code></pre></div><p class="new">You can test your code using the following template:</p><pre><code>def test_idf():
    corpus    = get_corpus()
    vocab, tf = build_tf(corpus)
    idf = build_idf(vocab, tf)
    
    print(idf['eggs'])

test_idf()

# ide.tester.test_function(build_idf)</code></pre><p class="new">After you are done, you can compare your results to the chart below.
<img alt="idf-gham.png" class="w-full" src="https://github.com/NSF-EC/INFO490Assets/raw/master/src/dmap/lessons/tfidf/html/idf-gham.png"/></p><h2 id="tfidf">TF‚Ä¢IDF</h2><p class="new">As a reminder, the TF is calculated per document and IDF is calculated across a set of documents. The final 
calculation (the assignment of this lesson) is to create the final TF * IDF. So for each document in the corpus, 
the the TF‚Ä¢IDF value is TF * IDF for each term.  Values close to zero are your custom set of stop words -- 
words that did not help separate out the documents.  Note that you can get these stopwords by just looking at the IDF values.</p><h2 id="stop-words-reprise">Stop Words Reprise</h2><p class="new">Sometimes it is still useful to remove stop words from the document for the TF‚Ä¢IDF calculations.  In this case, 
the common words that TF‚Ä¢IDF finds are common themes/terms/ngrams that occur throughout the corpus. </p><p class="new">In the case of using TF‚Ä¢IDF to analyze novels, it might be important to which words don't differentiate chapters, but 
are still seen as a common thread.  We could use that in our essay that we are suppose to write for English, 
but only had time to let Python 'read' it. There is an extra credit portion in this lesson that demonstrates 
this with the text of Huckleberry Finn. </p><h2 id="how-fast">How Fast?</h2><p class="new">TF‚Ä¢IDF involves lots of loops and in our Python only version, it can take a long time.  However, once you have 
finished, each document can now be represented as a compact vector (set of values).  Algorithms that 
cluster (grouping documents based on some similarity measure), classify (assign a label) or topic modeling 
sometimes use the TF‚Ä¢IDF representation as the document.  </p><p class="new">Most implements will use Numpy to make the entire process as fast as possible. In the next lesson, we will 
use sklearn's implementation and understand how it differs from this 'classical' definition.</p><h2 id="topic-modeling">Topic Modeling</h2><p class="new">You may be wondering how TF‚Ä¢IDF is related to topic modeling.  If you aren't, then you may not know what 
topic modeling is.  Topic modeling attempts to find a set of themes (e.g. topics) that describe a set of documents.
In the context of topic modeling, a topic is a probability distribution over the vocabulary (i.e. words) and a
document is a probability distribution over topics.  </p><p class="new">You can think of a topic as a set of words that co-occur together and ideally, the topics are as distinct as 
possible.  There are many topic modeling algorithms with LDA (latent dirichlet allocation) being one of the 
more popular ones. You may think of the top N TF‚Ä¢IDF words for a document as a 'topic'; however they are 
not the same.  As mentioned above, some topic modeling algorithms will use the TF‚Ä¢IDF scores as the 
input for algorithm.</p><div class="font-sans container"><p>Before you go, you should <strong>know</strong>:</p><div class="w-2/3 md:w-2/3"><div class="shadow-md"><div class="tab overflow-hidden border-t bg-green-200"><input class="absolute opacity-0" id='"tab-multi-tab0' name="tabs" type="checkbox"/><label class="block p-3 leading-normal cursor-pointer" for='"tab-multi-tab0'>1: What does TF‚Ä¢IDF measure</label><div class="tab-content overflow-hidden border-l-2 bg-orange-300 border-indigo-500 leading-normal"><p class="p-3">No Answer Given</p></div></div><div class="tab overflow-hidden border-t bg-green-200"><input class="absolute opacity-0" id='"tab-multi-tab1' name="tabs" type="checkbox"/><label class="block p-3 leading-normal cursor-pointer" for='"tab-multi-tab1'>2: The Meaning of TF</label><div class="tab-content overflow-hidden border-l-2 bg-orange-300 border-indigo-500 leading-normal"><p class="p-3">No Answer Given</p></div></div><div class="tab overflow-hidden border-t bg-green-200"><input class="absolute opacity-0" id='"tab-multi-tab2' name="tabs" type="checkbox"/><label class="block p-3 leading-normal cursor-pointer" for='"tab-multi-tab2'>3: The Meaning of IDF</label><div class="tab-content overflow-hidden border-l-2 bg-orange-300 border-indigo-500 leading-normal"><p class="p-3">No Answer Given</p></div></div></div></div></div><h1 class="section" id="section2">Lesson Assignment</h1><p class="new">There's only more more part to write (the final calculation).  Finish the function <code>compute_TFIDF</code> function. 
You can see how it will be used as well:</p><pre><code>def compute_TFIDF(doc_tf, idf):
    # returns a collection counter for the document_tf and the corpus idf
    return None

def build_tf_idf(tfs, idf):
    tfidf = [collections.Counter() for x in tfs]
    for idx, doc_tf in enumerate(tfs):
        tfidf[idx] = compute_TFIDF(doc_tf, idf)
    return tfidf

def test_tfidf():

    corpus    = get_corpus()
    vocab, tf = build_tf(corpus)
    idf       = build_idf(vocab, tf)
    
    tfidf = build_tf_idf(tf, idf)
    
    print(idf['eggs'])
    print(tf[0]['eggs'])</code></pre><div class="ide code-starter blink"><pre><code></code></pre></div><p class="new">After you are done, you can compare your results to the chart below.
<img alt="tfidf-gham.png" class="w-full" src="https://github.com/NSF-EC/INFO490Assets/raw/master/src/dmap/lessons/tfidf/html/tfidf-gham.png"/></p><p class="new">A few Observations:</p><ol start="1"><li>‚Ä¢ The word <strong>I</strong> helps differentiate one of the documents</li><li>‚Ä¢ TF‚Ä¢IDF could be used to help reduce the essence of each document</li></ol><h2 id="more-fun">More Fun.</h2><img alt="cith-150.png" class="mt-1 mb-3 float-left mr-3 sm" src="https://github.com/NSF-EC/INFO490Assets/raw/master/src/dmap/lessons/tfidf/html/cith-150.png"/><p class="new">The lesson framework (installed early on), has many of the components already coded
up for you. The module  <code>LessonUtil.py</code> has most of the parsing and cleaning routines.
You can try your tf‚Ä¢idf on different text.  </p><p class="new">You can look at the code modules to refresh your Python memory, but the following 
should work once everything is done:</p><pre><code>import LessonUtil as Util

def process_cith():
    data     = Util.read_data_file('cith.txt') 
    chapters = split_into_chapters(data)
    corpus   = Util.clean_chapters(chapters)
    
    # first 3 chapters, tokenize words &gt; 2 characters long
    vocab, tf = build_tf(corpus[0:3], 2)
    idf = build_idf(vocab, tf)
    tfidf = build_tf_idf(tf, idf)
    
    Util.print_tfidf(tfidf, top_n=5)</code></pre><p class="new">Output if the corpus is the first 3 chapters and you ignore words &lt; 2 characters long</p><pre><code>Document: 1
 Word: sat            TF-IDF:    0.07324
 Word: too            TF-IDF:    0.05493
 Word: cold           TF-IDF:    0.05493
 Word: sit            TF-IDF:    0.02703
 Word: did            TF-IDF:    0.02027
Document: 2
 Word: bump           TF-IDF:    0.04120
 Word: looked         TF-IDF:    0.02747
 Word: saw            TF-IDF:    0.02747
 Word: him            TF-IDF:    0.02747
 Word: cat            TF-IDF:    0.01520
Document: 3
 Word: hold           TF-IDF:    0.03315
 Word: look           TF-IDF:    0.03315
 Word: fish           TF-IDF:    0.02841
 Word: now            TF-IDF:    0.01894
 Word: can            TF-IDF:    0.01748</code></pre><p class="new">Looks like you can boil the first 3 chapters of the Cat in the Hat down to sitting in the cold, bumping with a cat, 
and looking/holding fish.</p><h1 class="section" id="section3">Extra Credit</h1><p class="new">Modify both <code>build_tf</code> and <code>split_into_tokens</code>, such that a <strong>stoplist</strong> 
could be passed in (see lines 6 and 7).<br/>This is a named parameter whose default value is the empty list.</p><div class="code-block code"><pre><code class="line-number">def process_huck():
</code><code class="line-number">    data     = Util.read_data_file('huck.txt') 
</code><code class="line-number">    chapters = split_into_chapters(data)
</code><code class="line-number">    corpus   = Util.clean_chapters(chapters)
</code><code class="line-number">
</code><code class="line-number">    stopwords = Util.load_stopwords()
</code><code class="line-number">    vocab, tf = build_tf(corpus, min_length=2, stopwords)
</code><code class="line-number">    idf = build_idf(vocab, tf)
</code><code class="line-number">    tfidf = build_tf_idf(tf, idf)
</code><code class="line-number">    
</code><code class="line-number">    Util.print_tfidf(tfidf, top_n=5)
</code></pre></div><h1>Test and Submit</h1><p>Once done you can download your code (via <code>ide.tester</code>) and upload that file to Gradescope:</p><div class=""><pre><code><strong>to test your code (either works)</strong><br/>ide.tester.test_notebook()<br/>ide.tester.test_notebook(verbose=True)<br/><strong>to download your code</strong><br/>ide.tester.download_solution()</code></pre></div><div class="lesson-footer flex bg-gray-200 justify-center"><div class="displaycard border-t border-gray-400 max-w-2xl rounded overflow-hidden shadow-lg"><div class="px-6 py-4"><div class="text-center font-bold text-xl">TF‚Ä¢IDF</div><p class="text-center text-gray-800 text-xl">Finding the Important Words</p><div class="text-center mt-6 text-xl"><i aria-hidden="true" class="fas fa-tags"></i> any questions on Piazza with <span class="font-bold">tfidf</span></div><div class="text-gray-700 text-base">¬†</div><div></div><div class="flex mt-4 border-t border-solid border-gray-500 justify-around bg-gray-200"><div class="text-gray-700 text-center px-4 m-2 text-sm">D.MÔπ†üêç</div><div class="text-gray-700 text-center px-4 m-2 text-sm"><strong>Version:</strong> <!-- -->Fall 2020<!-- --> <!-- -->08.20.20</div></div><div class="text-gray-700 text-center text-tiny">All Rights Reserved</div></div></div></div><div>¬†</div></div></div></body></html>