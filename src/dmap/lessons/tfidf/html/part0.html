<!DOCTYPE html><html lang="en"><head><meta charset="utf-8"><style>/*! normalize.css v8.0.1 | MIT License | github.com/necolas/normalize.css */html{line-height:1.15;-webkit-text-size-adjust:100%}body{margin:0}pre{font-family:monospace,monospace;font-size:1em}strong{font-weight:bolder}code{font-family:monospace,monospace;font-size:1em}img{border-style:none}input{font-family:inherit;font-size:100%;line-height:1.15;margin:0}input{overflow:visible}[type=checkbox]{box-sizing:border-box;padding:0}::-webkit-file-upload-button{-webkit-appearance:button;font:inherit}p,pre{margin:0}html{font-family:system-ui,-apple-system,BlinkMacSystemFont,"Segoe UI",Roboto,"Helvetica Neue",Arial,"Noto Sans",sans-serif,"Apple Color Emoji","Segoe UI Emoji","Segoe UI Symbol","Noto Color Emoji";line-height:1.5}*,::after,::before{box-sizing:border-box;border-width:0;border-style:solid;border-color:#e2e8f0}img{border-style:solid}input:-ms-input-placeholder{color:#a0aec0}input::-ms-input-placeholder{color:#a0aec0}input::-webkit-input-placeholder{color:#a0aec0}input::-moz-placeholder{color:#a0aec0}input{padding:0;line-height:inherit;color:inherit}code,pre{font-family:Menlo,Monaco,Consolas,"Liberation Mono","Courier New",monospace}img{display:block;vertical-align:middle}img{max-width:100%;height:auto}.container{width:100%}@media (min-width:640px){.container{max-width:640px}}@media (min-width:768px){.container{max-width:768px}}@media (min-width:1024px){.container{max-width:1024px}}@media (min-width:1280px){.container{max-width:1280px}}.bg-gray-200{--bg-opacity:1;background-color:#edf2f7;background-color:rgba(237,242,247,var(--bg-opacity))}.bg-gray-300{--bg-opacity:1;background-color:#e2e8f0;background-color:rgba(226,232,240,var(--bg-opacity))}.bg-orange-300{--bg-opacity:1;background-color:#fbd38d;background-color:rgba(251,211,141,var(--bg-opacity))}.bg-green-200{--bg-opacity:1;background-color:#c6f6d5;background-color:rgba(198,246,213,var(--bg-opacity))}.border-gray-500{--border-opacity:1;border-color:#a0aec0;border-color:rgba(160,174,192,var(--border-opacity))}.border-indigo-500{--border-opacity:1;border-color:#667eea;border-color:rgba(102,126,234,var(--border-opacity))}.rounded{border-radius:.25rem}.rounded-full{border-radius:9999px}.border-solid{border-style:solid}.border-l-2{border-left-width:2px}.border-t{border-top-width:1px}.cursor-pointer{cursor:pointer}.block{display:block}.inline-block{display:inline-block}.flex{display:flex}.justify-center{justify-content:center}.justify-around{justify-content:space-around}.font-sans{font-family:system-ui,-apple-system,BlinkMacSystemFont,"Segoe UI",Roboto,"Helvetica Neue",Arial,"Noto Sans",sans-serif,"Apple Color Emoji","Segoe UI Emoji","Segoe UI Symbol","Noto Color Emoji"}.font-serif{font-family:Georgia,Cambria,"Times New Roman",Times,serif}.font-semibold{font-weight:600}.font-bold{font-weight:700}.h-48{height:12rem}.h-64{height:16rem}.text-sm{font-size:.875rem}.text-base{font-size:1rem}.text-xl{font-size:1.25rem}.leading-normal{line-height:1.5}.m-2{margin:.5rem}.mr-2{margin-right:.5rem}.mb-3{margin-bottom:.75rem}.ml-3{margin-left:.75rem}.mt-4{margin-top:1rem}.max-w-sm{max-width:24rem}.object-contain{-o-object-fit:contain;object-fit:contain}.opacity-0{opacity:0}.overflow-hidden{overflow:hidden}.p-1{padding:.25rem}.p-3{padding:.75rem}.py-1{padding-top:.25rem;padding-bottom:.25rem}.px-3{padding-left:.75rem;padding-right:.75rem}.py-4{padding-top:1rem;padding-bottom:1rem}.px-4{padding-left:1rem;padding-right:1rem}.px-6{padding-left:1.5rem;padding-right:1.5rem}.pl-3{padding-left:.75rem}.absolute{position:absolute}.shadow-md{box-shadow:0 4px 6px -1px rgba(0,0,0,.1),0 2px 4px -1px rgba(0,0,0,.06)}.shadow-lg{box-shadow:0 10px 15px -3px rgba(0,0,0,.1),0 4px 6px -2px rgba(0,0,0,.05)}.text-center{text-align:center}.text-gray-700{--text-opacity:1;color:#4a5568;color:rgba(74,85,104,var(--text-opacity))}.text-gray-800{--text-opacity:1;color:#2d3748;color:rgba(45,55,72,var(--text-opacity))}.w-2\/3{width:66.666667%}.w-full{width:100%}@media (min-width:768px){.md\:w-2\/3{width:66.666667%}}.text-tiny{font-size:.5rem!important}.lesson{padding-left:10px;padding-right:10px;--bg-opacity:1;background-color:#edf2f7;background-color:rgba(237,242,247,var(--bg-opacity))}.lesson-content{font-family:Times}p.new{padding-top:.5em;padding-bottom:.5em}code{font-size:14px}.code-large{background:#f4f4f4;font-family:monospace;font-size:14px;margin:10px;font-size:16px}.code{background:#f4f4f4;border:1px solid #ddd;border-left:3px solid #f36d33;color:#666;page-break-inside:avoid;font-family:monospace;font-size:14px;min-width:710px;max-width:710px;line-height:1.6;margin-bottom:1.6em;padding:1em 1.5em;display:block;-moz-tab-size:2;-o-tab-size:2;tab-size:2;overflow-x:auto;word-wrap:break-word;white-space:pre-wrap}.tab{font-size:1rem;border-color:#8c6728}.tab-content{max-height:0;transition:max-height .35s}.tab input:checked~.tab-content{max-height:100vh}.tab input:checked+label{padding:1rem;border-left-width:2px;border-color:#6574cd;background-color:#f8fafc;color:#6574cd}.tab label::after{float:right;right:0;top:0;display:block;width:1em;height:1.5em;line-height:1.5;font-size:1rem;text-align:center;transition:all .35s}.tab input[type=checkbox]+label::after{content:"+";font-weight:700;border-width:1px;border-radius:9999px;border-color:#8c6728}.tab input[type=checkbox]:checked+label::after{transform:rotate(315deg);background-color:#6574cd;color:#f8fafc}
section:nth-child(1){display:block;}
section:not(:nth-child(1)){display:none;}</style><script src="https://kit.fontawesome.com/7efc4bcee2.js" crossOrigin="anonymous"></script></head><body class="lesson"><div id="__next"><div class="main-content bg-gray-200"><div class="p-1 font-serif"><div class="pl-3 text-xl"><div><section class="overview-content"><div class="lesson-overview bg-gray-200 flex justify-center"><div class="displaycard max-w-sm rounded overflow-hidden shadow-lg"><div>¬†</div><img alt="Text" class="object-contain h-64 w-full" src="https://github.com/NSF-EC/INFO490Assets/raw/master/src/dmap/lessons/tfidf/html/TextTripping-sm.png"/><div class="px-6 py-4"><div class="text-center font-bold text-xl">TF‚Ä¢IDF</div><p class="text-center text-gray-800 text-xl">Finding the Important Words</p><div class="text-gray-700 text-base">¬†</div><div class="text-center mb-3"><span class="inline-block bg-gray-300 rounded-full px-3 py-1 text-sm font-semibold text-gray-700 mr-2">#info490</span><span class="inline-block bg-gray-300 rounded-full px-3 py-1 text-sm font-semibold text-gray-700 mr-2">#python</span><span class="inline-block bg-gray-300 rounded-full px-3 py-1 text-sm font-semibold text-gray-700 mr-2">#text</span></div><div class="flex border-t border-solid border-gray-500 justify-around bg-gray-200"><div class="text-gray-700 text-center px-4 m-2 text-sm">D.MÔπ†üêç</div><div class="text-gray-700 text-center px-4 m-2 text-sm">Version: <!-- -->Fall 2020<!-- --> <!-- -->08.20.20</div></div><div class="text-gray-700 text-center text-tiny">All Rights Reserved</div></div></div><div class="ml-3 mb-3 h-48 rounded overflow-hidden shadow-lg"><div class="px-6 py-4"><div class="text-center font-bold text-xl">Colab Notes</div><p class="max-w-sm text-gray-800 text-sm">1. Copy this notebook into your google drive</p><p class="max-w-sm text-gray-800 text-sm">2. Share the notebook, and copy the share ID</p><p class="max-w-sm text-gray-800 text-sm">3. Set the <code>NOTEBOOK_ID</code> variable in the next cell to the share ID.</p><p class="max-w-sm text-gray-800 text-sm">4. Run the next code cells to install the INFO 490 IDE</p></div></div></div></section><section class="lesson-content part1"><p class="new"><strong>TF-IDF</strong><br/>Imagine having an algorithm that can automatically determine which words of a novel are unique to each chapter as well as 
filter out the words that are common across all chapters without bootstrapping the algorithm with a set of stop words.
That does sound good.</p><p class="new">The idea of ranking each term (i.e. a single word or an n-gram) within the context of a set of documents (or 
chapters in a novel) in which it appears is what the algorithm TF-IDF attempts to do. The Term Frequency Inverse 
Document Frequency is an information retrieval technique that scores each term by using its frequency (the TF weight) 
and its inverse document frequency (the IDF weight) -- more on that soon.  These two weights are multiplied together to get the TF‚Ä¢IDF score.  </p><p class="new">Each term has a score and the score determines how valuable the term is in separating out a set of documents 
(called a corpus).  Variations of the tf-idf weighting scheme are often used by search engines as a central tool in 
scoring and ranking a document's relevance given a user query.</p><p class="new"><strong>Documents</strong><br/>A document for our purposes is very generic.  It can refer to an article, a book, a journal, a web page, a chapter, and even a sentence.  Usually a document needs to be long enough to have some context and a 
set of documents makes up the corpus.  In practice, a document is usually no smaller than a sentence or a set of 
sentences.  </p><p class="new"><strong>TF</strong><br/>We have already been calculating TF (term frequency) since we starting working with Python dictionaries and 
counting word occurrences.  So for us, TF would describe the frequency of a word within a document; it measures how common a 
term is within a document.  A term can be an n-gram as well. It is the ratio of number of times the word 
appears in a document compared to the total number of words in that document:</p><img class="" src="https://render.githubusercontent.com/render/math?math=\Large %5Ctext%7B%0Atf(t%2Cd)%20%3D%20(count%20of%20term%20t%20in%20document%20d)%20%2F%20(number%20of%20words%20in%20document%20d)%0A%7D%0A"/><img class="mt-4" src="https://render.githubusercontent.com/render/math?math=\Huge tf_%7Bt%2Cd%7D%20%3D%20%5Cdfrac%7Bn_%7Bt%2Cd%7D%7D%7B%5Csum_%7B%7D%5E%7B%7D%20n_%7Bt%2Cd%7D%7D"/><p class="new">Here <img class="" src="https://render.githubusercontent.com/render/math?math=\Large n_%7Bt%2Cd%7D" style="display:inline-block"/> is the number of times term t appears in document d</p><p class="new">Each document has its own TF. In Python, <code>tf[0]['cat]</code> would be the TF 
for the first document, for the word 'cat'</p><p class="new">In the following code cell, create a function named <code>build_tf</code></p><pre class="code"><p class="new">def build_tf(corpus):</p><p class="new">   # corpus is an list of documents
   # a document is a list of words</p><p class="new">   # <strong>returns</strong> a tuple with two items:
   # first item is the a collections.Counter 
   # for <strong>all</strong> the words in all the documents </p><p class="new">   # the second item is the TF array, one for each document
   # each TF is a collection.Counter, 
   # each counter maps a word to the number of times that word appears</p><p class="new">   pass</p></pre></section><section><p class="new">You can test it like the following</p><pre class="code"><p class="new">import util
data     = util.read_data('book.txt')
chapters = util.split_into_chapters(data)
corpus   = util.clean_chapters(chapters)</p><p class="new"># now test it
vocab, tf = build_tf(corpus)
print(vocab.most_common(10)) # 10 most common words
print(tf[0].most_common(10)) # 10 most common words in first document</p></pre></section><section class="lesson-content part2"><p class="new"><strong>IDF</strong><br/>The IDF (inverse document frequency) weight isn't as intuitive.  If TF measures commonness, IDF measure rarity. 
However, IDF is looking at a corpus (a set of documents unified by some attribute). We can illustrate this using the 
following table -- which lists the number of documents a term appears in.  Assume there are 100 million documents:</p><pre class="code">term          # of documents
             containing term
----------------------------
a                100,000,000
boat               1,000,000
mobile               100,000
mobilegeddon           1,000
</pre><p class="new">Clearly, the term 'a' provides no value in telling the documents apart.  However, since <code>'mobilegeddon'</code> 
only appears in 1,000 documents, that word does help separate a small set of the documents. To calculate the IDF of 
each term we take the the total number of documents and divide it by the count of document occurrences for that 
word (ie. how many documents contain the term).  Then we take the Log of that number to scale the result so that the 
weight is 'dampened'.  This prevents interpreting that <code>'mobilegeddon'</code> is 1000 times more important than 
<code>'boat'</code>.</p><pre class="code">term           n/df[j]               IDF[j]
a              Log(100M/100,000,000) -&gt; 0
boat           Log(100M/1,000,000)   -&gt; 1
mobile         Log(100M/100,000)     -&gt; 2
mobilegeddon   Log(100M/1,000)       -&gt; 3
</pre><p class="new">Hence, the following formula is the standard way to calculate IDF for term j. </p><img class="" src="https://render.githubusercontent.com/render/math?math=\Huge idf_j%20%5C%2C%5C%2C%20%3D%20%5C%2C%5C%2C%20log(%5Cdfrac%7Bn%7D%7Bdf_j%7D)"/><p class="new">So IDF helps measure uniqueness or how much information a term provides.  For search engines, 
it helps identify what terms within each document are special. </p><p class="new"><strong>Not so Fast</strong><br/>At first glance, it might seem 'easy' to calculate TF‚Ä¢IDF. However, for both weights there are 
different ways to calculate them.   The TF we have been describing is called the raw count of a term in a document.<br/>A common alternative  is to scale the TF by the document length so that long documents don't skew the 
results (e.g. a long chapter will have more words in it, but may be no more important than a shorter 
chapter). </p><code class="code-large">TF = (raw count of term in a document)/len(document)</code><p class="new">For IDF, there are many variants as well.  The one issue for calculating IDF is that the denominator could be zero (if 
a term does not show up in a document).  In most situations, all the terms are coming from the documents themselves, 
so this can never happen.  In any case, some implementations will calculate the 'inverse' going from this equation: </p><code class="code-large">term_idf[term] = math.log10(N/term_count)</code><br/><p class="new">to this equation:<br/><code class="code-large">term_idf[term] = -1.0 * math.log10(term_count/N)

</code><br/></p><p class="new">If you remember your 'log' rules (here's a small 'proof' for the right hand side):<br/><span><img class="" src="https://render.githubusercontent.com/render/math?math=\large 1.%5Cquad%20log(%5Cdfrac%7Ba%7D%7Bb%7D)%20%3D%20log(a)%20-%20log(b)" style="display:inline-block"/> (by definition)

</span><br/><span><img class="" src="https://render.githubusercontent.com/render/math?math=\large 2.%5Cquad%20log(%5Cdfrac%7Bb%7D%7Ba%7D)%20%3D%20log(b)%20-%20log(a)" style="display:inline-block"/> (by definition)

</span><br/><span><img class="" src="https://render.githubusercontent.com/render/math?math=\large 3.%5Cquad%20log(a)%20%3D%20log(b)%20-%20log(%5Cdfrac%7Bb%7D%7Ba%7D)" style="display:inline-block"/> (rearrange terms in #2)

</span><br/><span><img class="" src="https://render.githubusercontent.com/render/math?math=\large 4.%5Cquad%20log(a)%20-%20log(b)%20%3D%20-log(%5Cdfrac%7Bb%7D%7Ba%7D)" style="display:inline-block"/> (rearrange terms in #3)

</span><br/><strong>SO</strong><br/><span><img class="" src="https://render.githubusercontent.com/render/math?math=\large 5.%5Cquad%20log(%5Cdfrac%7Ba%7D%7Bb%7D)%20%3D%20-1.0%20%5Ctimes%20log(%5Cdfrac%7Bb%7D%7Ba%7D)" style="display:inline-block"/></span><br/></p><p class="new">You may also see adding 1 to the result or the following:<br/><code class="code-large">term_idf[term] = math.log10(N/(1 + term_count))</code><br/>to set a lower bound on common words and avoid giving a weight of zero to common terms. 
For the most part, if your documents are long and the number of terms large, the calculation will not matter for 
comparison purposes.  However, for simple test data, it can matter.  In practice, everyone seems to have their favorite tweaks 
to the classic formula. </p><p class="new"><strong>TF‚Ä¢IDF</strong><br/>So for each document in the corpus, the final calculation is TF * IDF for each term.  Values close to zero are 
your custom set of stop words -- words that did not help separate out the documents. One subtle 
reminder is that TF is calculated per document and IDF is calculated across a set of documents. </p><p class="new"><strong>Stop Words Reprise</strong><br/>Sometimes it is still useful to remove stop words from the document for the TF‚Ä¢IDF calculations.  In this case, 
the common words that TF‚Ä¢IDF finds are common themes/terms/ngrams that occur throughout the corpus. 
In the case of using TF‚Ä¢IDF to analyze novels, it might be important to know that 'Tom' is a term that doesn't 
differentiate chapters, but is seen as a common theme.  We could use that in our essay that we are suppose to write 
for English, but only had time to let Python 'read' it. </p><p class="new"><strong>How Fast?</strong><br/>TF‚Ä¢IDF involves lots of loops and in our Python only version, it can take a long time.  However, once you have 
finished, each document can now be represented as a compact vector (set of values).  Algorithms that 
cluster (grouping documents based on some similarity measure), classify (assign a label) or topic modeling 
sometimes use the TF‚Ä¢IDF representation as the document.</p><p class="new"><strong>Topic Modeling</strong><br/>You may be wondering how TF‚Ä¢IDF is related to topic modeling.  If you aren't, then you may not know what 
topic modeling is.  Topic modeling attempts to find a set of themes (e.g. topics) that describe a set of documents.
In the context of topic modeling, a topic is a probability distribution over the vocabulary (i.e. words) and a
document is a probability distribution over topics.  You can think of a topic as a set of words that co-occur
together and ideally, the topics are as distinct as possible.  There are many topic modeling algorithms
with LDA (latent dirichlet allocation) being one of the more popular ones. You may think of the top N TF‚Ä¢IDF words for a document as a 'topic'; however they are not the same.  As mentioned
above, some topic modeling algorithms will use the TF‚Ä¢IDF scores as the input for algorithm.</p><div class="font-sans container"><p>Before you go, you should <strong>know</strong>:</p><div class="w-2/3 md:w-2/3"><div class="shadow-md"><div class="tab overflow-hidden border-t bg-green-200"><input class="absolute opacity-0" id='"tab-multi-tab0' name="tabs" type="checkbox"/><label class="block p-3 leading-normal cursor-pointer" for='"tab-multi-tab0'>1: What TF‚Ä¢IDF measures that is super long and complicated</label><div class="tab-content overflow-hidden border-l-2 bg-orange-300 border-indigo-500 leading-normal"><p class="p-3">No answer given</p></div></div><div class="tab overflow-hidden border-t bg-green-200"><input class="absolute opacity-0" id='"tab-multi-tab1' name="tabs" type="checkbox"/><label class="block p-3 leading-normal cursor-pointer" for='"tab-multi-tab1'>2: The Meaning of TF</label><div class="tab-content overflow-hidden border-l-2 bg-orange-300 border-indigo-500 leading-normal"><p class="p-3">No answer given</p></div></div><div class="tab overflow-hidden border-t bg-green-200"><input class="absolute opacity-0" id='"tab-multi-tab2' name="tabs" type="checkbox"/><label class="block p-3 leading-normal cursor-pointer" for='"tab-multi-tab2'>3: The Meaning of IDF</label><div class="tab-content overflow-hidden border-l-2 bg-orange-300 border-indigo-500 leading-normal"><p class="p-3">No answer given</p></div></div></div></div></div></section><br/><section class="assignment"><p class="new"><strong>Lesson Assignment</strong><br/>Everything you need is in this lesson -- there is no need to search.<br/>The lesson framework (installed early on), has many of the components already coded
up for you.  You can look at the code modules to refresh your Python memory, but the
following should work:</p><div class="code"><pre>1 import util
2 data     = util.read_data('book.txt')
3 chapters = util.split
</pre></div><p class="new">The module  <code>util.py</code> has all the parsing and cleaning routines already coded for you.
A few notes:
<code>tokenize_prep</code> uses the same rules as in ngrams lesson</p><p class="new"><code>split_into_tokens</code> normalizes the tokens and skips tokens less than 3 characters long</p><p class="new">You will create a function named <code>tfidf</code> which will take an array of chapters and a <code>top_n</code> number:</p><p class="new"><code>tfidf(corpus, 10)</code></p></section></div></div></div></div></div></body></html>