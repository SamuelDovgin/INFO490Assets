<!DOCTYPE html><html lang='en'><head><meta charset="utf-8"><style>/*! normalize.css v8.0.1 | MIT License | github.com/necolas/normalize.css */html{line-height:1.15;-webkit-text-size-adjust:100%}body{margin:0}h1{font-size:2em;margin:.67em 0}pre{font-family:monospace,monospace;font-size:1em}strong{font-weight:bolder}code{font-family:monospace,monospace;font-size:1em}img{border-style:none}input{font-family:inherit;font-size:100%;line-height:1.15;margin:0}input{overflow:visible}[type=checkbox]{box-sizing:border-box;padding:0}::-webkit-file-upload-button{-webkit-appearance:button;font:inherit}h1,h2,p,pre{margin:0}html{font-family:system-ui,-apple-system,BlinkMacSystemFont,"Segoe UI",Roboto,"Helvetica Neue",Arial,"Noto Sans",sans-serif,"Apple Color Emoji","Segoe UI Emoji","Segoe UI Symbol","Noto Color Emoji";line-height:1.5}*,::after,::before{box-sizing:border-box;border-width:0;border-style:solid;border-color:#e2e8f0}img{border-style:solid}input:-ms-input-placeholder{color:#a0aec0}input::-ms-input-placeholder{color:#a0aec0}input::-webkit-input-placeholder{color:#a0aec0}input::-moz-placeholder{color:#a0aec0}h1,h2{font-size:inherit;font-weight:inherit}input{padding:0;line-height:inherit;color:inherit}code,pre{font-family:Menlo,Monaco,Consolas,"Liberation Mono","Courier New",monospace}img{display:block;vertical-align:middle}img{max-width:100%;height:auto}.container{width:100%}@media (min-width:640px){.container{max-width:640px}}@media (min-width:768px){.container{max-width:768px}}@media (min-width:1024px){.container{max-width:1024px}}@media (min-width:1280px){.container{max-width:1280px}}.bg-gray-200{--bg-opacity:1;background-color:#edf2f7;background-color:rgba(237,242,247,var(--bg-opacity))}.bg-gray-300{--bg-opacity:1;background-color:#e2e8f0;background-color:rgba(226,232,240,var(--bg-opacity))}.bg-orange-300{--bg-opacity:1;background-color:#fbd38d;background-color:rgba(251,211,141,var(--bg-opacity))}.bg-green-200{--bg-opacity:1;background-color:#c6f6d5;background-color:rgba(198,246,213,var(--bg-opacity))}.border-gray-500{--border-opacity:1;border-color:#a0aec0;border-color:rgba(160,174,192,var(--border-opacity))}.border-indigo-500{--border-opacity:1;border-color:#667eea;border-color:rgba(102,126,234,var(--border-opacity))}.rounded{border-radius:.25rem}.rounded-full{border-radius:9999px}.border-solid{border-style:solid}.border-l-2{border-left-width:2px}.border-t{border-top-width:1px}.cursor-pointer{cursor:pointer}.block{display:block}.inline-block{display:inline-block}.flex{display:flex}.justify-center{justify-content:center}.justify-around{justify-content:space-around}.font-sans{font-family:system-ui,-apple-system,BlinkMacSystemFont,"Segoe UI",Roboto,"Helvetica Neue",Arial,"Noto Sans",sans-serif,"Apple Color Emoji","Segoe UI Emoji","Segoe UI Symbol","Noto Color Emoji"}.font-serif{font-family:Georgia,Cambria,"Times New Roman",Times,serif}.font-semibold{font-weight:600}.font-bold{font-weight:700}.h-48{height:12rem}.h-64{height:16rem}.text-sm{font-size:.875rem}.text-base{font-size:1rem}.text-xl{font-size:1.25rem}.leading-normal{line-height:1.5}.m-2{margin:.5rem}.mr-2{margin-right:.5rem}.mb-3{margin-bottom:.75rem}.ml-3{margin-left:.75rem}.mt-4{margin-top:1rem}.max-w-sm{max-width:24rem}.object-contain{-o-object-fit:contain;object-fit:contain}.opacity-0{opacity:0}.overflow-hidden{overflow:hidden}.p-1{padding:.25rem}.p-3{padding:.75rem}.py-1{padding-top:.25rem;padding-bottom:.25rem}.px-3{padding-left:.75rem;padding-right:.75rem}.py-4{padding-top:1rem;padding-bottom:1rem}.px-4{padding-left:1rem;padding-right:1rem}.px-6{padding-left:1.5rem;padding-right:1.5rem}.pl-3{padding-left:.75rem}.absolute{position:absolute}.shadow-md{box-shadow:0 4px 6px -1px rgba(0,0,0,.1),0 2px 4px -1px rgba(0,0,0,.06)}.shadow-lg{box-shadow:0 10px 15px -3px rgba(0,0,0,.1),0 4px 6px -2px rgba(0,0,0,.05)}.text-center{text-align:center}.text-gray-700{--text-opacity:1;color:#4a5568;color:rgba(74,85,104,var(--text-opacity))}.text-gray-800{--text-opacity:1;color:#2d3748;color:rgba(45,55,72,var(--text-opacity))}.w-2\/3{width:66.666667%}.w-full{width:100%}@media (min-width:768px){.md\:w-2\/3{width:66.666667%}}.text-tiny{font-size:.5rem!important}.lesson{padding-left:10px;padding-right:10px;--bg-opacity:1;background-color:#edf2f7;background-color:rgba(237,242,247,var(--bg-opacity))}h2{font-family:Times}p.new{padding-top:.5em;padding-bottom:.5em}h1{margin-top:1.5em}h2{margin-top:1em}code{font-size:14px}pre code{background:#f4f4f4;font-family:monospace;font-size:14px}.code-large{background:#f4f4f4;font-family:monospace;font-size:14px;margin:10px;font-size:16px}.code,pre code{background:#f4f4f4;font-family:monospace;font-size:14px;border:1px solid #ddd;border-left:3px solid #f36d33;color:#666;page-break-inside:avoid;min-width:710px;max-width:710px;line-height:1.6;margin-bottom:1.6em;padding:1em 1.5em;display:block;-moz-tab-size:2;-o-tab-size:2;tab-size:2;overflow-x:auto;word-wrap:break-word;white-space:pre-wrap}.tab{font-size:1rem;border-color:#8c6728}.tab-content{max-height:0;transition:max-height .35s}.tab input:checked~.tab-content{max-height:100vh}.tab input:checked+label{padding:1rem;border-left-width:2px;border-color:#6574cd;background-color:#f8fafc;color:#6574cd}.tab label::after{float:right;right:0;top:0;display:block;width:1em;height:1.5em;line-height:1.5;font-size:1rem;text-align:center;transition:all .35s}.tab input[type=checkbox]+label::after{content:"+";font-weight:700;border-width:1px;border-radius:9999px;border-color:#8c6728}.tab input[type=checkbox]:checked+label::after{transform:rotate(315deg);background-color:#6574cd;color:#f8fafc}</style><script src="https://kit.fontawesome.com/7efc4bcee2.js" crossOrigin="anonymous"></script></head><body class = "lesson apples "><div class="main-content bg-gray-200 p-1 pl-3 text-xl font-serif"><div><h1 class="section" id="section2"><strong>TF-IDF</strong></h1><p class="new">Imagine having an algorithm that can automatically determine which words of a novel are unique to each chapter as well as 
filter out the words that are common across all chapters without bootstrapping the algorithm with a set of stop words.
That does sound good.</p><p class="new">The idea of ranking each term (i.e. a single word or an n-gram) within the context of a set of documents (or 
chapters in a novel) in which it appears is what the algorithm TF-IDF attempts to do. The Term Frequency Inverse 
Document Frequency is an information retrieval technique that scores each term by using its frequency (the TF weight) 
and its inverse document frequency (the IDF weight) -- more on that soon.  These two weights are multiplied together to get the TF•IDF score.  </p><p class="new">Each term has a score and the score determines how valuable the term is in separating out a set of documents 
(called a corpus).  Variations of the tf-idf weighting scheme are often used by search engines as a central tool in 
scoring and ranking a document's relevance given a user query.</p><h2 id="documents"><strong>Documents</strong></h2><p class="new">A document for our purposes is very generic.  It can refer to an article, a book, a journal, a web page, a chapter, and even a sentence.  Usually a document needs to be long enough to have some context and a 
set of documents makes up the corpus.  In practice, a document is usually no smaller than a sentence or a set of 
sentences.  </p><h2 id="tf"><strong>TF</strong></h2><p class="new">We have already been calculating TF (term frequency) since we starting working with Python dictionaries and 
counting word occurrences.  So for us, TF would describe the frequency of a word within a document; it measures how common a 
term is within a document.  A term can be an n-gram as well. It is the ratio of number of times the word 
appears in a document compared to the total number of words in that document:</p><img class="" src="https://render.githubusercontent.com/render/math?math=\Large %5Ctext%7B%0Atf(t%2Cd)%20%3D%20(count%20of%20term%20t%20in%20document%20d)%20%2F%20(number%20of%20words%20in%20document%20d)%0A%7D%0A"/><img class="mt-4" src="https://render.githubusercontent.com/render/math?math=\Huge tf_%7Bt%2Cd%7D%20%3D%20%5Cdfrac%7Bn_%7Bt%2Cd%7D%7D%7B%5Csum_%7B%7D%5E%7B%7D%20n_%7Bt%2Cd%7D%7D"/><p class="new">Here <img class="" src="https://render.githubusercontent.com/render/math?math=\Large n_%7Bt%2Cd%7D" style="display:inline-block"/> is the number of times term t appears in document d</p><p class="new">Each document has its own TF. In Python, <code>tf[0]['cat']</code> would be the TF 
for the first document, for the word 'cat'</p><p class="new">In the following code cell, create a function named <code>build_tf</code></p><pre><code>def split_into_tokens(data, normalize=True, min_length=2):

   # returns an array of words
   # each word is simply a string
  
   # • splits the incoming data (a string) based on whitespace
   # • if normalize is True, normalize the case of the words
   # • only return those words longer than min_length
   
def build_tf(corpus):

   # corpus is an list of documents
   # a document is an unparsed string of words
   
   # returns a tuple with two items:
   # first item is a collections.Counter 
   # for all the words in all the documents 
  
   # the second item is the TF array, one for each document
   # each TF is a collection.Counter, 
   # each counter maps a word to the number of times that word appears
   # feel free to use some of the helper functions in Util.py
   
   pass</code></pre><div class="code-starter"><pre><code># Starter Code
import Util
import collections

def split_into_tokens(data, normalize=True, min_length=2):
  pass

def build_tf(corpus):
  pass

def test_tf():
  text = ide.reader.read_data_file('cith.txt');
  corpus = Util.split_into_chapters(text)
  cleaned = Util.clean_chapters(corpus)
  build_tf(cleaned)

test_tf()

ide.tester.test_function(build_tf)</code></pre></div><p class="new">You can test it like the following</p><pre><code>import Util
data     = ide.reader.read_data_file('cith.txt');
chapters = util.split_into_chapters(data)
corpus   = util.clean_chapters(chapters)

# now test it
vocab, tf = build_tf(corpus)
print(vocab.most_common(10)) # 10 most common words
print(tf[0].most_common(10)) # 10 most common words in first document</code></pre></div></div></body></html>