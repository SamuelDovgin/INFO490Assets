<!DOCTYPE html><html lang='en'><head><title>Word Embeddings</title><meta charset="utf-8"><style>/*! normalize.css v8.0.1 | MIT License | github.com/necolas/normalize.css */html{line-height:1.15;-webkit-text-size-adjust:100%}body{margin:0}h1{font-size:2em;margin:.67em 0}pre{font-family:monospace,monospace;font-size:1em}a{background-color:transparent}strong{font-weight:bolder}code{font-family:monospace,monospace;font-size:1em}img{border-style:none}::-webkit-file-upload-button{-webkit-appearance:button;font:inherit}h1,h2,h3,p,pre{margin:0}ul{list-style:none;margin:0;padding:0}html{font-family:system-ui,-apple-system,BlinkMacSystemFont,"Segoe UI",Roboto,"Helvetica Neue",Arial,"Noto Sans",sans-serif,"Apple Color Emoji","Segoe UI Emoji","Segoe UI Symbol","Noto Color Emoji";line-height:1.5}*,::after,::before{box-sizing:border-box;border-width:0;border-style:solid;border-color:#e2e8f0}img{border-style:solid}h1,h2,h3{font-size:inherit;font-weight:inherit}a{color:inherit;text-decoration:inherit}code,pre{font-family:Menlo,Monaco,Consolas,"Liberation Mono","Courier New",monospace}img{display:block;vertical-align:middle}img{max-width:100%;height:auto}.bg-gray-200{--bg-opacity:1;background-color:#edf2f7;background-color:rgba(237,242,247,var(--bg-opacity))}.bg-gray-300{--bg-opacity:1;background-color:#e2e8f0;background-color:rgba(226,232,240,var(--bg-opacity))}.bg-blue-200{--bg-opacity:1;background-color:#bee3f8;background-color:rgba(190,227,248,var(--bg-opacity))}.bg-blue-300{--bg-opacity:1;background-color:#90cdf4;background-color:rgba(144,205,244,var(--bg-opacity))}.border-gray-400{--border-opacity:1;border-color:#cbd5e0;border-color:rgba(203,213,224,var(--border-opacity))}.border-gray-500{--border-opacity:1;border-color:#a0aec0;border-color:rgba(160,174,192,var(--border-opacity))}.rounded{border-radius:.25rem}.rounded-full{border-radius:9999px}.border-solid{border-style:solid}.border{border-width:1px}.border-t{border-top-width:1px}.inline-block{display:inline-block}.flex{display:flex}.justify-center{justify-content:center}.justify-around{justify-content:space-around}.float-left{float:left}.clearfix:after{content:"";display:table;clear:both}.font-serif{font-family:Georgia,Cambria,"Times New Roman",Times,serif}.font-semibold{font-weight:600}.font-bold{font-weight:700}.h-64{height:16rem}.text-xs{font-size:.75rem}.text-sm{font-size:.875rem}.text-base{font-size:1rem}.text-xl{font-size:1.25rem}.m-2{margin:.5rem}.mt-1{margin-top:.25rem}.mt-2{margin-top:.5rem}.mr-2{margin-right:.5rem}.mb-2{margin-bottom:.5rem}.mr-3{margin-right:.75rem}.mb-3{margin-bottom:.75rem}.mt-4{margin-top:1rem}.mt-6{margin-top:1.5rem}.max-w-sm{max-width:24rem}.max-w-2xl{max-width:42rem}.object-contain{-o-object-fit:contain;object-fit:contain}.overflow-hidden{overflow:hidden}.p-1{padding:.25rem}.py-1{padding-top:.25rem;padding-bottom:.25rem}.py-2{padding-top:.5rem;padding-bottom:.5rem}.px-3{padding-left:.75rem;padding-right:.75rem}.py-4{padding-top:1rem;padding-bottom:1rem}.px-4{padding-left:1rem;padding-right:1rem}.px-6{padding-left:1.5rem;padding-right:1.5rem}.pl-3{padding-left:.75rem}.shadow-lg{box-shadow:0 10px 15px -3px rgba(0,0,0,.1),0 4px 6px -2px rgba(0,0,0,.05)}.shadow-inner{box-shadow:inset 0 2px 4px 0 rgba(0,0,0,.06)}.text-left{text-align:left}.text-center{text-align:center}.text-black{--text-opacity:1;color:#000;color:rgba(0,0,0,var(--text-opacity))}.text-gray-700{--text-opacity:1;color:#4a5568;color:rgba(74,85,104,var(--text-opacity))}.text-gray-800{--text-opacity:1;color:#2d3748;color:rgba(45,55,72,var(--text-opacity))}.whitespace-no-wrap{white-space:nowrap}.w-1\/2{width:50%}.w-full{width:100%}.text-tiny{font-size:.5rem!important}body{color:#000!important;font-size:1.25rem!important}.lesson{padding-left:10px!important;padding-right:10px!important;--bg-opacity:1;background-color:#edf2f7;background-color:rgba(237,242,247,var(--bg-opacity))}.lesson-footer{margin-top:50px;margin-top:20px}.lesson ul{list-style:none!important;list-style-position:inside;margin-left:1em}.lesson ul li{padding-left:1em;text-indent:-1em}.lesson ul li::before{content:"‚óè";padding-right:5px}span{white-space:nowrap}p.new{text-indent:1.5em;padding-top:0;padding-bottom:.5em}h1,h2,h3{font-weight:700;margin-bottom:.25em;font-family:Georgia,Cambria,"Times New Roman",Times,serif!important}h1{margin-top:.5em;font-size:2em!important;clear:both;color:#000!important}h2{margin-top:1em;font-size:1.5em!important;clear:both;color:#8b0000!important}h3{margin-top:.5em;font-size:1.25em!important;clear:both;color:#006400!important}ul{margin-bottom:30px}p.new a{text-decoration:underline}.lesson a{text-decoration:underline}.shadow-lg{box-shadow:0 10px 15px -3px rgba(0,0,0,.1),0 4px 6px -2px rgba(0,0,0,.5)}img.small{width:33.333333%;margin-left:1.25rem;border:1px solid #021a40}img.border{border:1px solid #021a40}code{font-size:smaller}pre code{font-size:15px}pre code:not(.line-number){background:#f4f4f4;font-family:monospace;font-size:15px;-webkit-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none;cursor:default;touch-action:none;-webkit-touch-callout:none;-webkit-tap-highlight-color:transparent;clear:both;border:1px solid #ddd;color:#666;page-break-inside:avoid;display:block;min-width:740px;max-width:740px;line-height:1.6;margin-bottom:1.6em;padding:1em 1.5em;-moz-tab-size:2;-o-tab-size:2;tab-size:2;overflow-x:auto;word-wrap:break-word;white-space:pre-wrap;border-left:3px solid #f36d33}div.code-starter>pre code{border-left:3px solid #fdff44!important;background-image:radial-gradient(rgba(0,150,0,.75),#000 120%);color:#fff;font:.9rem Inconsolata,monospace}div.code-starter>pre code::after{content:"\a$_"}pre{counter-reset:line}</style><script src="https://kit.fontawesome.com/7efc4bcee2.js" crossOrigin="anonymous"></script></head><body class="lesson"><div class="main-content bg-gray-200 text-black p-1 pl-3 font-serif"><div class="md-inner">
<h1 class="overview"></h1><div class="lesson-overview bg-gray-200 flex justify-center"><div class="text-center px-4 py-2 m-2"><div class="displaycard bg-blue-200 max-w-sm rounded overflow-hidden shadow-lg"><div>¬†</div><img alt="Text" class="object-contain h-64 w-full" src="https://github.com/NSF-EC/INFO490Assets/raw/master/src/dmap/lessons/word-vectors/html/TextTripping-sm.png"/><div class="px-6 py-4"><div class="text-center font-bold text-xl">Word Embeddings</div><p class="text-center text-gray-800 text-xl">text to vectors</p><div class="text-gray-700 text-base">¬†</div><div class="text-center mb-3"><span class="inline-block bg-gray-300 rounded-full px-3 py-1 text-sm font-semibold text-gray-700 mr-2">#info490</span><span class="inline-block bg-gray-300 rounded-full px-3 py-1 text-sm font-semibold text-gray-700 mr-2">#spacy</span></div><div class="flex border-t border-solid border-gray-500 shadow-inner justify-around bg-blue-300"><div class="text-gray-700 text-center px-4 m-2 text-sm"><span class="whitespace-no-wrap">D.MÔπ†üêç</span></div><div class="text-gray-700 text-center px-4 m-2 text-sm"><span class="whitespace-no-wrap"><strong>Version:</strong> <!-- -->8/18/2020</span></div></div><div class="text-gray-700 mt-1 text-center text-tiny">All Rights Reserved</div></div></div></div><div class="text-center px-4 py-2 m-2 w-1/2"><div class="displaycard bg-gray-200 max-w-sm rounded overflow-hidden shadow-lg"><div class="px-6 py-4 text-left"><div class="text-center font-bold text-xl">Word Embeddings<br/>prerequisites</div><div class="text-center text-xs mb-2">(start only after finishing)</div><p class="max-w-sm text-gray-800 text-sm">‚¶ø <strong>info490 NLP</strong></p><p class="max-w-sm text-gray-800 text-sm">‚¶ø <strong>tfidf</strong></p></div><div class="px-6 py-4 text-left text-gray-800"><div class="text-center font-bold text-xl">Colab Notes</div><p class="max-w-sm text-sm">1. <strong>Copy</strong> this notebook into your google drive</p><p class="max-w-sm text-sm">2. <strong>Share</strong> the notebook, and copy the share ID</p><p class="max-w-sm text-sm">3. <strong>Set</strong> the NOTEBOOK_ID variable to the share ID</p><p class="max-w-sm text-gray-800 text-sm">4. <strong>SAVE</strong> the notebook (‚åò/ctrl+ s) again</p><p class="max-w-sm text-gray-800 text-sm">5. <strong>Run</strong> the code cell that installs the INFO 490 IDE</p><div class="text-center font-bold text-xl">¬†</div><div class="text-center font-bold text-xl">Jupyter/PyCharm Notes</div><p class="max-w-sm text-gray-800 text-sm text-left">The testing framework does <strong>not work</strong> (at this time) for Jupyter  notebooks or local code development.</p></div></div></div></div><h1 class="section" id="section1">Word Embeddings</h1><p class="new">As we first saw in the tf‚Ä¢idf lesson, in order for a computer to work with
text, we needed to first convert the text/words into a numerical representation.
However, coming up with a meaningful 'number' that captures the difference
between words is difficult.  </p><p class="new">Spellcheckers for example, can use
<img alt="editDistance.png" class="border small float-left mr-3" src="https://github.com/NSF-EC/INFO490Assets/raw/master/src/dmap/lessons/word-vectors/html/editDistance.png"/> 'edit' distance between two words to determine how similar they are.  The edit 
 distance is how many modifications (letter updates, additions, subtractions) are needed 
 to get from one word to the other.<br/> This allows one to offer possible corrections for a misspelled word.<br/> For example, each of the following pairs have a 1 edit distance: </p><ul><li>capital, capitol (change 'a' to 'o')</li><li>war, warm (add an 'm')</li><li>brand, band (drop the 'r')</li></ul><p class="new">Of course, edit distance is incapable of capturing meaning between words (the
 edit distance (i.e 4) between <em>warm</em> and <em>mild</em> is meaningless
 in trying to evaluate the distance in the semantics).</p><p class="new">A majority of NLP (natural language processing) techniques attempt to make
sense of (understand and derive meaning) text.  In order to do this, an
 important processing step is to convert each word into a representation that
  computers and algorithms can use.</p><p class="new">The result of converting a word/token into a number or list of numbers is a 
<strong>word embedding</strong>.  Each word is represented by a vector of real numbers
 (e.g. floats) -- the intrepretation of those numbers depends on the model or
  algorithm used.  </p><p class="new">  As an example the following vector may represent the word 'dog' with 4 attributes.</p><img alt="wordVector.png" class="border" src="https://github.com/NSF-EC/INFO490Assets/raw/master/src/dmap/lessons/word-vectors/html/wordVector.png"/><p class="new">Word vectors are synonymous with word embeddings. You may also see the word 'row vector' 
to describe a word embedding.
Additional these vectors can be called a <em>k-dimensional</em> vector or a vector
 with k columns, components, attributes, features, traits, variables, or
  dimensions.</p><p class="new">For most algorithms that produce word embeddings, the actual meaning or
 interpretation of the columns (i.e. attributes) is unknown.  This is
  especially true of many machine learning (ML) algorithms that produce 
  a model or representation that best 'fits' its data.  The numbers/vectors, etc. that 
  are generated cannot be interpreted in any meaningful way.  It's one of the 
  reasons why ML in many situations is described as a black box. 
For example, below are two word vectors for dog and car:</p><img alt="dogCarVector.png" class="border" src="https://github.com/NSF-EC/INFO490Assets/raw/master/src/dmap/lessons/word-vectors/html/dogCarVector.png"/><p class="new">In this case the columns could be related to the concepts of 'fast', 'pet', 'animal', 
'transportation', but we really have no way of knowing. </p><h1 class="section" id="section2">Words as Vectors</h1><p class="new">Representing words as vectors of numbers opens up the world of linear algebra
 (working with vectors and matrices) to working with text.<br/> We won't necessary require you to know linear algebra (we'll try to introduce the 
 concepts as needed), but if you want to know why the underlying math works, 
 taking a course in linear algebra will add to your data science expertise.</p><p class="new">Some free references: </p><pre><code> ‚Ä¢ http://joshua.smcvt.edu/linearalgebra/book.pdf)
 ‚Ä¢ https://www.deeplearningbook.org/contents/linear_algebra.html</code></pre><h2 id="spacy-and-word-vectors">Spacy and word vectors</h2><p class="new">The NLP package <a href="https://spacy.io/" target="_blank">spacy</a> includes the ability to work with
 word vectors.  In order to use spacy, you will need to download and install
  a language model -- a model trained using a certain corpus (like a news
   feed, Wikipedia, etc).  The following will take a few seconds to run.  You
    will need to do so each time you start a session.</p><div class="ide code-starter clearfix"><pre><code>import spacy
try:
  print('installing model, please wait')
  import en_core_web_md  # don't use lg (too big)
except ImportError:
  print('need to download .. please wait')
  !python -m spacy download en_core_web_md
  import en_core_web_md 

nlp = en_core_web_md.load()</code></pre></div><p class="new">Once that cell has run we can do some simple things with word vectors:</p><div class="ide code-starter clearfix"><pre><code># retrieve words from the English model vocabulary
cat = nlp.vocab['cat']
dog = nlp.vocab['dog']
car = nlp.vocab['car']

# print the dimension of word vectors
print('vector length:', len(cat.vector))

# print the word vector
print('cat:', cat.vector)</code></pre></div><h2 id="distance-between-words">Distance between words</h2><p class="new">Now that we have vectors for words, we can use linear algebra and vector space models to analyze the relationship between words.
In many data science and machine learning tasks, distance between two
 items can used to indicate similarity or to evaluate the 'cost' or 'fit' of
 a model.  </p><p class="new">Although we will have a separate lesson on distance metrics, 
one of the main metrics for working with vectors is cosine similarity. This 
metric relies on normalizing the two input vectors so that 'longer' or 'bigger' don't over influence the
 calculation (e.g. just because a document contains more words doesn't make it
 more important).</p><p class="new">We can visualize cosine similarity (the value of ùõ≥ in the image below) to show
the distance between to words (the value d, represents euclidean distance).
<img alt="eucos.png" class="ma-4 mt-6 border small" src="https://github.com/NSF-EC/INFO490Assets/raw/master/src/dmap/lessons/word-vectors/html/eucos.png"/></p><p class="new">Although we want the value of ùõ≥, we take the cosine of that angle to map the
 values to the [-1, 1] range.  Take a look at the cosine graph to re-familiarize yourself with the range of the cosine function:
<img alt="cosine.png" class="ma-4 mt-6 border small" src="https://github.com/NSF-EC/INFO490Assets/raw/master/src/dmap/lessons/word-vectors/html/cosine.png"/></p><p class="new">A value of 1, means the vectors are completely
 aligned (same word).  Each 'word' in spacy, has a similarity method that can
 be used to compare it to another 'word'.</p><p class="new">Using the cosine distance formula: <br/><img alt="math?math=%5Clarge%20cosine%5C_similarity(A%2C%20B)%20%3D%20%5Cfrac%7BA%20%5Ccdot%20B%7D%7B%5Cleft%20%5C%7C%20A%20%5Cright%20%5C%7C%5Cleft%20%5C%7C%20B%20%5Cright%20%5C%7C%7D" class="jax" src="https://render.githubusercontent.com/render/math?math=%5Clarge%20cosine%5C_similarity(A%2C%20B)%20%3D%20%5Cfrac%7BA%20%5Ccdot%20B%7D%7B%5Cleft%20%5C%7C%20A%20%5Cright%20%5C%7C%5Cleft%20%5C%7C%20B%20%5Cright%20%5C%7C%7D"/><br/>We can also calculate it using numpy:</p><pre><code>import numpy as np
cosine_A_B = np.dot(A, B)/(np.linalg.norm(A) * np.linalg.norm(B))</code></pre><p class="new">As we saw in the tf‚Ä¢idf lesson, the L2 norm (<code>np.linalg.norm</code>) is just the square
 root of the sum of the squared components:</p><pre><code>def simple_l2(vector):
  ssq = 0
  for v in vector:
    ssq += v * v 
  return np.sqrt(ssq)

print(np.linalg.norm(dog.vector))
print(dog.vector_norm)
print(simple_l2(dog.vector))</code></pre><p class="new">However, the spacy <code>similarity</code> method does this calculation:</p><div class="ide code-starter clearfix"><pre><code>print('The similarity between dog and dog:', dog.similarity(dog))
print('The similarity between dog and car:', dog.similarity(car))
print('The similarity between dog and cat:', dog.similarity(cat))</code></pre></div><h3 id="exercise-">Exercise </h3><p class="new">Implement the function sim which uses numpy to calculate the
cosine similarity between the two spacy tokens. You can use either 
<code>np.linalg.norm</code> or <code>&lt;spacy_word&gt;.vector_norm</code></p><div class="ide code-starter clearfix"><pre><code>def sim(a, b):
    return 0
    
print(sim(dog,car))
# or use the tester: print(ide.tester.test_fn(sim))</code></pre></div><h2 id="the-meaning-of-similar">The Meaning of Similar</h2><p class="new">As noted before, the range of sim will be from -1 to +1.  A value of 1 is a
 perfect match (same word), but a -1 does not mean 'opposite'. 
Word embeddings do not capture antonyms. 
The vectors captures the word-context from the trained corpus. 
So it highly depends on what corpus was used to build the word vectors. 
The cosine similarly of antonyms are generally high. 
The cosine similarity of (fast,slow) (high, low) should be quite high 
because they (most likely) occur in the same context.</p><p class="new">So similarity is co-occurrence and dissimilarity means the words do not appear 
within the same context (usually).</p><pre><code>cold = nlp.vocab['cold']
hot = nlp.vocab['hot']
print(sim(hot, cold))</code></pre><p class="new">What do you notice ?</p><h2 id="word-math">Word Math</h2><p class="new">With word vectors you can do simple math with them to determine if the
 'difference' between pairs of words have similar meaning. 
In the next example, we show that the difference between <strong>father</strong> and <strong>mother</strong> 
is very close to the difference between <strong>uncle</strong> and <strong>aunt</strong>:</p><div class="ide code-starter clearfix"><pre><code>def sim2(a,b):
  return np.dot(a, b)/(np.linalg.norm(a)*np.linalg.norm(b))
  
man = nlp.vocab['father'].vector
woman = nlp.vocab['mother'].vector
d1 = man - woman

uncle = nlp.vocab['uncle'].vector
aunt = nlp.vocab['aunt'].vector
d2 = uncle - aunt

print(sim2(d1, d2))</code></pre></div><h2 id="finding-similar-words">Finding Similar Words</h2><p class="new">Using the same math, we can build upon that to find all the words that are
 closet to a specific word.</p><div class="ide code-starter clearfix"><pre><code>def most_similar(word, topn=10):

    # get all words in the vocabulary
    allwords = [w for w in nlp.vocab if w.has_vector and w.is_lower and w.lower_ != word.lower_]  
    
    # sort words by similarity in descending order
    out = sorted(allwords, key=lambda w: word.similarity(w), reverse=True)  
    return out[:topn]

neighbors = most_similar(car)
print([w.text for w in neighbors])</code></pre></div><p class="new">What's so interesting/exciting is that the algorithm used to create these
 vectors had only the text as its input.  No dictionary, thesaurus, or
  experts were used.  In the next lesson, we will discuss some of these
 algorithms.</p><h2 id="word-play">Word Play</h2><p class="new">Perhaps 'coolest' feature of using linear algebra on word vectors is that
 we can do 'math' on words to find word analogies.  </p><p class="new">For example, let's fill in the blank:</p><p class="new"><strong>Math</strong> + <strong>Symbols</strong> is ‚ùì‚ùì‚ùì‚ùì</p><p class="new">We can use the similarity functionality we just build to find the word.</p><div class="ide code-starter clearfix"><pre><code>def find_closest(v, exclude):

  cos = lambda v1, v2: np.dot(v1, v2)/(np.linalg.norm(v1) * np.linalg.norm(v2))
  # valid ensures we don't include any of the words 
  # that are part of the equation (input)
  def valid(w, exclude):
    if w.has_vector and w.is_lower:
      for t in exclude:
        if w.lower_.find(t)&gt;=0:
          return False
      return True
    return False

  w_set = [w for w in nlp.vocab if valid(w, exclude)]
  candidates = sorted(w_set, key=lambda w: cos(result, w.vector), reverse=True)
  return candidates[:10]</code></pre></div><p class="new">Be sure to take the time to understand what is happening when you use
 <code>find_closest</code>:</p><pre><code>result = nlp.vocab['math'].vector + nlp.vocab['symbol'].vector
ex = ['math', 'symbol']
answer = find_closest(result, ex)
print(answer[0].text)</code></pre><p class="new">What did you get ?</p><p class="new">Depending on the data used to build the model (tweeter, news, Wikipedia, etc
) you may have to adjust your results. For example, using the <code>en_core_web_md</code> model gives different results.</p><img alt="kmw.png" class="border small float-left mb-3 mr-3" src="https://github.com/NSF-EC/INFO490Assets/raw/master/src/dmap/lessons/word-vectors/html/kmw.png"/><div><p class="new">One of the popular examples of math with word vectors is to solve this equation:</p><p class="new"><strong>Man</strong> is to <strong>Woman</strong> as <strong>King</strong> is to ‚ùì‚ùì‚ùì‚ùì</p><p class="new">This can be solved:
result = king - man + woman</p></div><br/><pre><code>result = nlp.vocab['king'].vector - nlp.vocab['man'].vector + nlp.vocab['woman'].vector
ex = ['king', 'man', 'woman']
answer = find_closest(result, ex)
print(answer[0].text)</code></pre><p class="new">Be sure to add this code to the previous cell.</p><p class="new">To find the equivalent of US fries in Canada (poutine), we can set up the following equation:</p><pre><code>result = nlp.vocab['canada'].vector - nlp.vocab['usa'].vector + nlp.vocab['fries'].vector
ex = ['canad', 'us', 'frie'] 
answer = find_closest(result, ex)
print(answer[0].text)</code></pre><p class="new">In order to narrow down the results, you need to remove synonyms for fries
 (potatoes) and remove any reference to hamburgers (probably due to the training data):</p><pre><code>ex = ['canad', 'us', 'frie', 'potato', 'burger']</code></pre><p class="new">If you inspect the top 10, you will find poutine, cheese, and gravy!!! Although the need to tweak the results, shows the importance of data
 used to build the word vectors.</p><h2 id="visualizing-word-vectors">Visualizing Word Vectors</h2><img alt="king.png" class="border small float-left mr-3" src="https://github.com/NSF-EC/INFO490Assets/raw/master/src/dmap/lessons/word-vectors/html/king.png"/><p class="new">Since each word vector contains hundreds of dimensions (300 for spacy), it's
impossible to visualize without projecting the high dimensions into lower
 dimensional space (i.e. 2 or 3).  </p><p class="new">Although we will have a separate lesson on how to transform a data-set with many
 dimensions to one with only a few of its 'most important' dimensions, we will 
use something called t-Distributed Stochastic Neighbor Embedding (t-SNE for
 short).  The t-SNE algorithm is particularly well suited for the visualization of 
high-dimensional datasets.  Don't worry too much about the particulars, other 
than making sure you can read the code and understand the resulting visualization.</p><div class="ide code-starter clearfix"><pre><code>def reduce_dimensions(labels):
  from sklearn.manifold import TSNE
  import numpy as np
  
  data = np.array([nlp.vocab[w].vector for w in labels])
  # reduce to two
  tsne_model = TSNE(n_components=2)
  data_2d = tsne_model.fit_transform(data)
  
  return data_2d

def plot_results(data_2d, labels):
  import matplotlib
  import matplotlib.pyplot as plt
  
  # plot the 2d vectors and show their labels
  fig, axes = plt.subplots()
  axes.scatter(data_2d[:, 0], data_2d[:, 1], s=100)
  for i, txt in enumerate(labels):
    axes.annotate(txt, (data_2d[i,0], data_2d[i,1]), xytext=(2, 3), textcoords='offset points')
  axes.grid()
  
  return fig

labels = ['king', 'man', 'queen', 'woman']
data = reduce_dimensions(labels)
fig = plot_results(data, labels)</code></pre></div><p class="new">If the following line is a bit confusing: <code>data_2d[:, 0], data_2d[:, 1]</code>, now's a good time to review array slicing from INFO 490.</p><p class="new">You can see that there is indeed similar 'distances' between the points. Another fun 
<a href="https://projector.tensorflow.org/" target="_blank">visualization tool</a> for word vectors is with using TensorFlow. 
TensorFlow is a Google product to create, use (and learn about) machine learning algorithms easy. 
You can even use TensorFlow to create word embeddings --assuming you have 
access to a large corpus to train on.  Our next lesson is on word2vec, one of
 the most popular implementations for building and using word vectors.</p><h1 class="section" id="section3">Lesson Assignment </h1><p class="new">Using what you learned in this lesson, you are going to create a function named
'find_analogy`.  This function takes 2 arguments:</p><ul><li>an array of 3 strings (used as words)</li><li>the nlp object to use to get the spacy word vectors</li></ul><pre><code>print(find_analogy(['france', 'paris', 'rome'], nlp))</code></pre><p class="new">This should report <code>Italy</code></p><p class="new">A few notes:</p><ul><li>don't worry about case (you should get the same answer regardless of case)</li><li>write at least one helper function for <code>find_analogy</code> to use.  This will 
make your code more readable, manageable, and testable).</li></ul><div class="ide code-starter clearfix"><pre><code>def find_analogy(three_words, nlp):
   return None</code></pre></div><p class="new">A few test cases:</p><pre><code>print(find_analogy(['king', 'man', 'woman'], nlp))      
# --&gt; queen
print(find_analogy(['Illinois', 'Springfield', 'Lansing'], nlp))  
# --&gt; michigan
print(find_analogy(['walked', 'walk', 'go'], nlp)) 
# --&gt; want past tense of go (went)</code></pre><p class="new">Note that order will matter:</p><pre><code>print(find_analogy(['pizza', 'Italy', 'Mexico'], nlp)) # YES tacos
print(find_analogy(['Italy', 'pizza', 'Mexico'], nlp)) # NO SPAIN</code></pre><p class="new">If you start playing with these word vectors, remember, not all word
 analogies will work:</p><pre><code># fastest, (furthest in top 10)
print(find_analogy(['quickest', 'quick', 'far'], nlp)) 

# raptor, (bird not in top 10) 
print(find_analogy(['mammal', 'dog', 'eagle'], nlp))</code></pre><h1 class="section" id="section4">Bonus Vectors</h1><p class="new">As with a lot of machine learning models, many will be too big to load into
 notebooks.  However, if you want to experiment with other word vectors (on
 your own computer), the following models have been trained using Wikipedia.</p><p class="new">‚Ä¢ <a href="https://fasttext.cc/docs/en/english-vectors.html" target="_blank">https://fasttext.cc/docs/en/english-vectors.html</a></p><h1>Test and Submit</h1><p>Once done you can download your code (via <code>ide.tester</code>) and upload that file to Gradescope:</p><div class=""><pre><code><strong># to test your code (either works)</strong><br/>print(ide.tester.test_notebook())<br/>print(ide.tester.test_notebook(verbose=True))<br/><strong># to prepare and download your code</strong><br/>ide.tester.download_solution()</code></pre></div><div class="lesson-footer flex bg-gray-200 justify-center"><div class="displaycard bg-blue-200 border-t border-gray-400 max-w-2xl rounded overflow-hidden shadow-lg"><div class="px-6 py-4"><div class="text-center font-bold text-xl">Word Embeddings</div><p class="text-center text-gray-800 text-xl">text to vectors</p><div class="text-center mt-6 text-xl"><i aria-hidden="true" class="fas fa-tags"></i> any questions on Piazza with <span class="font-bold">word-vectors</span></div><div class="text-gray-700 text-base">¬†</div><div></div><div class="flex mt-4 border-t border-solid border-gray-500 justify-around bg-gray-200"><div class="text-gray-700 text-center px-4 m-2 text-sm">D.MÔπ†üêç</div><div class="text-gray-700 text-center px-4 m-2 text-sm"><strong>Version:</strong> <!-- -->8/18/2020, 11:04:48 AM</div></div><div class="text-gray-700 mt-2 text-center text-tiny">All Rights Reserved</div><div class="text-gray-700 text-center text-tiny">Do not distribute this notebook outside of the class</div></div></div></div><div>¬†</div><div class="ide code-starter clearfix"><pre><code># print(ide.tester.test_notebook()) 
# print(ide.tester.test_notebook(verbose=True)) 

ide.tester.download_solution()</code></pre></div></div></div></body></html>