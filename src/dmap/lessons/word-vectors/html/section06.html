<!DOCTYPE html><html lang='en'><head><title>Word Embeddings</title><meta charset="utf-8"><style>/*! normalize.css v8.0.1 | MIT License | github.com/necolas/normalize.css */html{line-height:1.15;-webkit-text-size-adjust:100%}body{margin:0}h1{font-size:2em;margin:.67em 0}pre{font-family:monospace,monospace;font-size:1em}a{background-color:transparent}strong{font-weight:bolder}code{font-family:monospace,monospace;font-size:1em}img{border-style:none}::-webkit-file-upload-button{-webkit-appearance:button;font:inherit}h1,h2,h3,p,pre{margin:0}ul{list-style:none;margin:0;padding:0}html{font-family:system-ui,-apple-system,BlinkMacSystemFont,"Segoe UI",Roboto,"Helvetica Neue",Arial,"Noto Sans",sans-serif,"Apple Color Emoji","Segoe UI Emoji","Segoe UI Symbol","Noto Color Emoji";line-height:1.5}*,::after,::before{box-sizing:border-box;border-width:0;border-style:solid;border-color:#e2e8f0}img{border-style:solid}h1,h2,h3{font-size:inherit;font-weight:inherit}a{color:inherit;text-decoration:inherit}code,pre{font-family:Menlo,Monaco,Consolas,"Liberation Mono","Courier New",monospace}img{display:block;vertical-align:middle}img{max-width:100%;height:auto}.bg-gray-200{--bg-opacity:1;background-color:#edf2f7;background-color:rgba(237,242,247,var(--bg-opacity))}.bg-gray-300{--bg-opacity:1;background-color:#e2e8f0;background-color:rgba(226,232,240,var(--bg-opacity))}.bg-blue-200{--bg-opacity:1;background-color:#bee3f8;background-color:rgba(190,227,248,var(--bg-opacity))}.bg-blue-300{--bg-opacity:1;background-color:#90cdf4;background-color:rgba(144,205,244,var(--bg-opacity))}.border-gray-400{--border-opacity:1;border-color:#cbd5e0;border-color:rgba(203,213,224,var(--border-opacity))}.border-gray-500{--border-opacity:1;border-color:#a0aec0;border-color:rgba(160,174,192,var(--border-opacity))}.rounded{border-radius:.25rem}.rounded-full{border-radius:9999px}.border-solid{border-style:solid}.border{border-width:1px}.border-t{border-top-width:1px}.inline-block{display:inline-block}.flex{display:flex}.justify-center{justify-content:center}.justify-around{justify-content:space-around}.float-left{float:left}.clearfix:after{content:"";display:table;clear:both}.font-serif{font-family:Georgia,Cambria,"Times New Roman",Times,serif}.font-semibold{font-weight:600}.font-bold{font-weight:700}.h-64{height:16rem}.text-xs{font-size:.75rem}.text-sm{font-size:.875rem}.text-base{font-size:1rem}.text-xl{font-size:1.25rem}.m-2{margin:.5rem}.mt-1{margin-top:.25rem}.mt-2{margin-top:.5rem}.mr-2{margin-right:.5rem}.mb-2{margin-bottom:.5rem}.mr-3{margin-right:.75rem}.mb-3{margin-bottom:.75rem}.mt-4{margin-top:1rem}.mt-6{margin-top:1.5rem}.max-w-sm{max-width:24rem}.max-w-2xl{max-width:42rem}.object-contain{-o-object-fit:contain;object-fit:contain}.overflow-hidden{overflow:hidden}.p-1{padding:.25rem}.py-1{padding-top:.25rem;padding-bottom:.25rem}.py-2{padding-top:.5rem;padding-bottom:.5rem}.px-3{padding-left:.75rem;padding-right:.75rem}.py-4{padding-top:1rem;padding-bottom:1rem}.px-4{padding-left:1rem;padding-right:1rem}.px-6{padding-left:1.5rem;padding-right:1.5rem}.pl-3{padding-left:.75rem}.shadow-lg{box-shadow:0 10px 15px -3px rgba(0,0,0,.1),0 4px 6px -2px rgba(0,0,0,.05)}.shadow-inner{box-shadow:inset 0 2px 4px 0 rgba(0,0,0,.06)}.text-left{text-align:left}.text-center{text-align:center}.text-black{--text-opacity:1;color:#000;color:rgba(0,0,0,var(--text-opacity))}.text-gray-700{--text-opacity:1;color:#4a5568;color:rgba(74,85,104,var(--text-opacity))}.text-gray-800{--text-opacity:1;color:#2d3748;color:rgba(45,55,72,var(--text-opacity))}.whitespace-no-wrap{white-space:nowrap}.w-1\/2{width:50%}.w-full{width:100%}.text-tiny{font-size:.5rem!important}body{color:#000!important;font-size:1.25rem!important}.lesson{padding-left:10px!important;padding-right:10px!important;--bg-opacity:1;background-color:#edf2f7;background-color:rgba(237,242,247,var(--bg-opacity))}.lesson-footer{margin-top:50px;margin-top:20px}.lesson ul{list-style:none!important;list-style-position:inside;margin-left:1em}.lesson ul li{padding-left:1em;text-indent:-1em}.lesson ul li::before{content:"‚óè";padding-right:5px}span{white-space:nowrap}p.new{text-indent:1.5em;padding-top:0;padding-bottom:.5em}h1,h2,h3{font-weight:700;margin-bottom:.25em;font-family:Georgia,Cambria,"Times New Roman",Times,serif!important}h1{margin-top:.5em;font-size:2em!important;clear:both;color:#000!important}h2{margin-top:1em;font-size:1.5em!important;clear:both;color:#8b0000!important}h3{margin-top:.5em;font-size:1.25em!important;clear:both;color:#006400!important}ul{margin-bottom:30px}p.new a{text-decoration:underline}.lesson a{text-decoration:underline}.shadow-lg{box-shadow:0 10px 15px -3px rgba(0,0,0,.1),0 4px 6px -2px rgba(0,0,0,.5)}img.small{width:33.333333%;margin-left:1.25rem;border:1px solid #021a40}img.border{border:1px solid #021a40}code{font-size:smaller}pre code{font-size:15px}pre code:not(.line-number){background:#f4f4f4;font-family:monospace;font-size:15px;-webkit-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none;cursor:default;touch-action:none;-webkit-touch-callout:none;-webkit-tap-highlight-color:transparent;clear:both;border:1px solid #ddd;color:#666;page-break-inside:avoid;display:block;min-width:740px;max-width:740px;line-height:1.6;margin-bottom:1.6em;padding:1em 1.5em;-moz-tab-size:2;-o-tab-size:2;tab-size:2;overflow-x:auto;word-wrap:break-word;white-space:pre-wrap;border-left:3px solid #f36d33}div.code-starter>pre code{border-left:3px solid #fdff44!important;background-image:radial-gradient(rgba(0,150,0,.75),#000 120%);color:#fff;font:.9rem Inconsolata,monospace}div.code-starter>pre code::after{content:"\a$_"}pre{counter-reset:line}</style><script src="https://kit.fontawesome.com/7efc4bcee2.js" crossOrigin="anonymous"></script></head><body class="lesson"><div class="main-content bg-gray-200 text-black p-1 pl-3 font-serif"><div class="md-inner">
<h2 id="distance-between-words">Distance between words</h2><p class="new">Now that we have vectors for words, we can use linear algebra and vector space models to analyze the relationship between words.
In many data science and machine learning tasks, distance between two
 items can used to indicate similarity or to evaluate the 'cost' or 'fit' of
 a model.  </p><p class="new">Although we will have a separate lesson on distance metrics, 
one of the main metrics for working with vectors is cosine similarity. This 
metric relies on normalizing the two input vectors so that 'longer' or 'bigger' don't over influence the
 calculation (e.g. just because a document contains more words doesn't make it
 more important).</p><p class="new">We can visualize cosine similarity (the value of ùõ≥ in the image below) to show
the distance between to words (the value d, represents euclidean distance).
<img alt="eucos.png" class="ma-4 mt-6 border small" src="https://github.com/NSF-EC/INFO490Assets/raw/master/src/dmap/lessons/word-vectors/html/eucos.png"/></p><p class="new">Although we want the value of ùõ≥, we take the cosine of that angle to map the
 values to the [-1, 1] range.  Take a look at the cosine graph to re-familiarize yourself with the range of the cosine function:
<img alt="cosine.png" class="ma-4 mt-6 border small" src="https://github.com/NSF-EC/INFO490Assets/raw/master/src/dmap/lessons/word-vectors/html/cosine.png"/></p><p class="new">A value of 1, means the vectors are completely
 aligned (same word).  Each 'word' in spacy, has a similarity method that can
 be used to compare it to another 'word'.</p><p class="new">Using the cosine distance formula: <br/><img alt="math?math=%5Clarge%20cosine%5C_similarity(A%2C%20B)%20%3D%20%5Cfrac%7BA%20%5Ccdot%20B%7D%7B%5Cleft%20%5C%7C%20A%20%5Cright%20%5C%7C%5Cleft%20%5C%7C%20B%20%5Cright%20%5C%7C%7D" class="jax" src="https://render.githubusercontent.com/render/math?math=%5Clarge%20cosine%5C_similarity(A%2C%20B)%20%3D%20%5Cfrac%7BA%20%5Ccdot%20B%7D%7B%5Cleft%20%5C%7C%20A%20%5Cright%20%5C%7C%5Cleft%20%5C%7C%20B%20%5Cright%20%5C%7C%7D"/><br/>We can also calculate it using numpy:</p><pre><code>import numpy as np
cosine_A_B = np.dot(A, B)/(np.linalg.norm(A) * np.linalg.norm(B))</code></pre><p class="new">As we saw in the tf‚Ä¢idf lesson, the L2 norm (<code>np.linalg.norm</code>) is just the square
 root of the sum of the squared components:</p><pre><code>def simple_l2(vector):
  ssq = 0
  for v in vector:
    ssq += v * v 
  return np.sqrt(ssq)

print(np.linalg.norm(dog.vector))
print(dog.vector_norm)
print(simple_l2(dog.vector))</code></pre><p class="new">However, the spacy <code>similarity</code> method does this calculation:</p></div></div></body></html>