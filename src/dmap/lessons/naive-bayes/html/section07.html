<!DOCTYPE html><html lang='en'><head><title>Naive Bayes</title><meta charset="utf-8"><style>/*! normalize.css v8.0.1 | MIT License | github.com/necolas/normalize.css */html{line-height:1.15;-webkit-text-size-adjust:100%}body{margin:0}h1{font-size:2em;margin:.67em 0}hr{box-sizing:content-box;height:0;overflow:visible}pre{font-family:monospace,monospace;font-size:1em}a{background-color:transparent}strong{font-weight:bolder}code{font-family:monospace,monospace;font-size:1em}img{border-style:none}::-webkit-file-upload-button{-webkit-appearance:button;font:inherit}blockquote,h1,h3,h4,hr,p,pre{margin:0}ol,ul{list-style:none;margin:0;padding:0}html{font-family:system-ui,-apple-system,BlinkMacSystemFont,"Segoe UI",Roboto,"Helvetica Neue",Arial,"Noto Sans",sans-serif,"Apple Color Emoji","Segoe UI Emoji","Segoe UI Symbol","Noto Color Emoji";line-height:1.5}*,::after,::before{box-sizing:border-box;border-width:0;border-style:solid;border-color:#e2e8f0}hr{border-top-width:1px}img{border-style:solid}table{border-collapse:collapse}h1,h3,h4{font-size:inherit;font-weight:inherit}a{color:inherit;text-decoration:inherit}code,pre{font-family:Menlo,Monaco,Consolas,"Liberation Mono","Courier New",monospace}img{display:block;vertical-align:middle}img{max-width:100%;height:auto}.bg-gray-200{--bg-opacity:1;background-color:#edf2f7;background-color:rgba(237,242,247,var(--bg-opacity))}.bg-gray-300{--bg-opacity:1;background-color:#e2e8f0;background-color:rgba(226,232,240,var(--bg-opacity))}.border-gray-400{--border-opacity:1;border-color:#cbd5e0;border-color:rgba(203,213,224,var(--border-opacity))}.border-gray-500{--border-opacity:1;border-color:#a0aec0;border-color:rgba(160,174,192,var(--border-opacity))}.rounded{border-radius:.25rem}.rounded-full{border-radius:9999px}.border-solid{border-style:solid}.border-t{border-top-width:1px}.inline-block{display:inline-block}.flex{display:flex}.justify-center{justify-content:center}.justify-around{justify-content:space-around}.font-serif{font-family:Georgia,Cambria,"Times New Roman",Times,serif}.font-semibold{font-weight:600}.font-bold{font-weight:700}.h-64{height:16rem}.text-sm{font-size:.875rem}.text-base{font-size:1rem}.text-xl{font-size:1.25rem}.m-2{margin:.5rem}.mx-auto{margin-left:auto;margin-right:auto}.mr-2{margin-right:.5rem}.mb-3{margin-bottom:.75rem}.ml-3{margin-left:.75rem}.mt-4{margin-top:1rem}.mt-6{margin-top:1.5rem}.max-w-sm{max-width:24rem}.max-w-2xl{max-width:42rem}.object-contain{-o-object-fit:contain;object-fit:contain}.object-center{-o-object-position:center;object-position:center}.overflow-hidden{overflow:hidden}.p-1{padding:.25rem}.py-1{padding-top:.25rem;padding-bottom:.25rem}.px-3{padding-left:.75rem;padding-right:.75rem}.py-4{padding-top:1rem;padding-bottom:1rem}.px-4{padding-left:1rem;padding-right:1rem}.px-6{padding-left:1.5rem;padding-right:1.5rem}.px-10{padding-left:2.5rem;padding-right:2.5rem}.pl-3{padding-left:.75rem}.shadow-lg{box-shadow:0 10px 15px -3px rgba(0,0,0,.1),0 4px 6px -2px rgba(0,0,0,.05)}.text-center{text-align:center}.text-black{--text-opacity:1;color:#000;color:rgba(0,0,0,var(--text-opacity))}.text-gray-700{--text-opacity:1;color:#4a5568;color:rgba(74,85,104,var(--text-opacity))}.text-gray-800{--text-opacity:1;color:#2d3748;color:rgba(45,55,72,var(--text-opacity))}.whitespace-no-wrap{white-space:nowrap}.w-full{width:100%}.text-tiny{font-size:.5rem!important}body{color:#000!important}h1,h3,h4{color:#000!important}.lesson{padding-left:10px;padding-right:10px;--bg-opacity:1;background-color:#edf2f7;background-color:rgba(237,242,247,var(--bg-opacity))}.lesson-footer{margin-top:50px;margin-top:20px}.lesson ol{list-style:decimal;list-style-position:inside;margin-left:1em}.lesson ul{list-style:none!important;list-style-position:inside;margin-left:1em}.lesson ul li{padding-left:1em;text-indent:-1em}.lesson ul li::before{content:"â—";padding-right:5px}span{white-space:nowrap}p.new{padding-top:.5em;padding-bottom:.5em}.h-18rem{height:17rem}h1,h3,h4{font-weight:700;margin-bottom:.25em;font-family:Georgia,Cambria,"Times New Roman",Times,serif!important}h1{margin-top:.5em;font-size:2em!important}h3{margin-top:.5em;font-size:1.25em!important}h4{margin-top:.25em;font-size:1em!important}p.new a{text-decoration:underline}blockquote{font-size:1em;background:#f9f9f9;border-left:10px solid #ccc;margin:1.5em 10px;padding:.5em 10px;border-left-color:#ffcd69;border-right-color:#f6ba59;quotes:"\201C""\201D""\2018""\2019"}blockquote:before{color:#ccc;content:open-quote;font-size:4em;line-height:.1em;margin-right:.25em;vertical-align:-.4em}blockquote p{display:inline}img.med{width:66.666667%;margin-left:1.25rem;border:1px solid #021a40}img:not([class]){width:66.666667%;margin-left:1.25rem;border:1px solid #021a40}code{font-size:smaller}pre code{font-size:15px}pre code:not(.line-number){background:#f4f4f4;font-family:monospace;font-size:15px;-webkit-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none;cursor:default;touch-action:none;-webkit-touch-callout:none;-webkit-tap-highlight-color:transparent;border:1px solid #ddd;color:#666;page-break-inside:avoid;display:block;min-width:740px;max-width:740px;line-height:1.6;margin-bottom:1.6em;padding:1em 1.5em;-moz-tab-size:2;-o-tab-size:2;tab-size:2;overflow-x:auto;word-wrap:break-word;white-space:pre-wrap;border-left:3px solid #f36d33}div.code-starter>pre code{border-left:3px solid #fdff44!important;background-image:radial-gradient(rgba(0,150,0,.75),#000 120%);color:#fff;font:.9rem Inconsolata,monospace}div.code-starter>pre code::after{content:"\a$_"}pre{counter-reset:line}</style><script src="https://kit.fontawesome.com/7efc4bcee2.js" crossOrigin="anonymous"></script></head><body class="lesson"><div class="main-content bg-gray-200 text-black p-1 pl-3 text-xl font-serif"><div class="md-inner">
<h3 id="2-scikit-learn-for-naive-bayes-">2. Scikit-learn for Naive Bayes </h3><p class="new">Now that you understand the logic behind the algorithm, we can do some
large-scale analysis. For this, we are going to use Scikit-learn. <strong>Scikit-learn</strong>
is a python module with tools to performe machine learning operations, like class
prediction based on <a href="https://scikit-learn.org/stable/modules/naive_bayes.html#naive-bayes" target="_blank">Naive Bayes</a>. </p><p class="new">As any supervised machine learning method, Scikit-learn for Naive Bayes, need as input different datasets to train and test its classifier. </p><h3 id="training-set">Training set</h3><p class="new">The training set should be composed by a list of observations <em>X_train</em> (like the emails from our previous example), 
and a list of labels with the pre-assigned class for each observation <em>y_train</em>. It is important to highlight 
that the observations need to be structured based on their common freatures in order to fit a model made in 
Scikit-learn.</p><p class="new">Therefore, to structure the training set observations for our spam example, we would need to use a entire 
dictionary of words to <em>encode</em> every document. Then, instead of having a list of words for each email, we 
will need a vector with the lenght of the whole dictionaty, that accounts the frequency of each word in the 
specific email. For example, a email's vectore have in position 0 the count of how many times it presents 
the word 'win', position 1 will account for the word 'prise', and so on. This means that the vectors are filled 
mostly with zeros, but that is fine. </p><p class="new">Then, insted of having this:</p><pre><code>X_train = [['click', 'here', 'and', 'win', 'a', 'huge', 'prise'],
            ['dear', 'friend', 'i', 'miss', 'you'],  
            ['i', 'can','meet', 'next', 'week', 'i', 'need', 'an', 'hour'],
            ... ]</code></pre><p class="new">We need something like this:</p><pre><code>X_train = [[1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, ...],
          [0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, ...],
          [0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 2, 0, 0, ...],
          ...]</code></pre><p class="new">The X_train is then a matrix with a row for each observation, and a column for each feature, in this case, 
each possible word from a dictionary. </p><p class="new">Then, the dictionary will be a list of words, where the positions of each word in the array is the same for the 
encoded vector, like this:</p><pre><code>d = ['win', 'prise', 'money', ' dear', ...]</code></pre><p class="new">Moreover, we have to make something similar for the lables, representing normal emails with 0s, and spam 
emails with 1s. Then, the training lables should look something like this:</p><pre><code>y_train = [1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, ...]</code></pre><p class="new">y_train should be a vector with the length of the number of observations in your training set. </p><h3 id="3-the-multinomial-naive-bayes">3. The Multinomial Naive Bayes</h3><p class="new">Sklearn.naive_bayes provides different variants of the naive bayes, based of the nature of the data you are 
working with. <a href="https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.MultinomialNB.html#sklearn.naive_bayes.MultinomialNB" target="_blank">The Multinomial Naive Bayes</a> 
is one of the two classic naive Bayes variants used in text classification (where the data are typically 
represented as word vector counts, although tf-idf vectors are also known to work well in practice). </p><p class="new">To use MultinomialNB, we can import it from the scikit-learn like this:</p><pre><code>from sklearn.naive_bayes import MultinomialNB</code></pre><p class="new">Then, assuming that our training set is in good shape, we could used X<em>train and y</em>train to train our classifier like this:</p><pre><code>clf = MultinomialNB()
clf.fit(X_train, y_train)
MultinomialNB()</code></pre><p class="new">The method <code>fit( )</code> receives the training set's elements, and by calling the <code>MultinomialNB()</code>, the model is 
trained. You can change the smoothing method, and the prior used by specifying it insider of the 
MultinomialNB method. By default, the method has the values <code>MultinomialNB(alpha = 1, fit_prior = True, class_prior= None)</code>, 
where the fit_prior paramether allow the model to calculate the prior based on the training data if set to True. 
When False, you can add a different prior in the paramether class_prior, by creating an array with a 
customized proportions for each lable.</p><p class="new">Now that you have your model, you could predict the class of an email with a list of words 'x'. Remember that you 
need to <em>encode</em> it with the dictionary that you created. If x_test is the encoded version of the new email you 
could use <code>predict()</code>  to get the predicted class of it.</p><pre><code>predicted_lable = clf.predict([x_test])</code></pre><p class="new">Notice that the predict method receives a list of observations. Then, if you want to predict the class of 
only one encoded email, you will need to pass it into a list.</p><h3 id="4-gaussian-naive-bayes-classifier">4. Gaussian Naive Bayes Classifier</h3><p class="new">For our previous example, we used words as features. Finding the frequency of a value is way easier when that 
value is discrete. But when we think about features with continous values, like a person's high or weight, 
calculating its likelihood by counting how many times another person had its exact high or weight makes no 
sense anymore. </p><p class="new">Therefore, for continuous features, the <a href="https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.GaussianNB.html#sklearn.naive_bayes.GaussianNB" target="_blank">Gaussian Naive Bayes Classifier</a> 
uses new assumption: The distribution of the features' values are gaussian. What does this mean again? 
Well, this basically means that the algorithm assumes that the data from each feature in a class will follow a 
normal distribution. And the likelihood of finding a feature value given that it belongs to a class y can be 
calculated by:</p><img alt="math?math=%5Clarge%20%5Cbegin%7Baligned%7D%20P(x_i%7Cy)%20%3D%20%5Cfrac%7B1%7D%7B%5Csqrt%7B2%5Cpi%5Csigma%5E2_y%7D%7Dexp%20(-%5Cfrac%7B(x_i%20-%20%5Cmu_y)%5E2%7D%7B2%5Csigma%5E2_y%7D)%20%5Cend%7Baligned%7D" class="jax" src="https://render.githubusercontent.com/render/math?math=%5Clarge%20%5Cbegin%7Baligned%7D%20P(x_i%7Cy)%20%3D%20%5Cfrac%7B1%7D%7B%5Csqrt%7B2%5Cpi%5Csigma%5E2_y%7D%7Dexp%20(-%5Cfrac%7B(x_i%20-%20%5Cmu_y)%5E2%7D%7B2%5Csigma%5E2_y%7D)%20%5Cend%7Baligned%7D"/><p class="new">Luckily this calculation is automatically made by the <strong>GaussianNB</strong> model from Scikit-learn. But you should 
have an idea of how this calculation is made. Therefore, I invite you to think about flowers, specifically 
about the Iris. </p><p class="new">An Iris can be classify in one of three groups inside the Iris Family: </p><table><thead><tr><th style="text-align:left">Class</th><th style="text-align:center">Iris-Setosa</th><th style="text-align:center">Iris-Versicolor</th><th style="text-align:center">Iris-Virginica</th></tr></thead><tbody><tr><td style="text-align:left">Example</td><td style="text-align:center"><img alt="iris_baby_blue.jpg?1500653527" class="med" src="https://github.com/NSF-EC/INFO490Assets/raw/master/src/dmap/lessons/naive-bayes/html/a2.jpg"/></td><td style="text-align:center"><img alt="IRIS_VERSICOLOR.JPG?1495391088" class="med" src="https://github.com/NSF-EC/INFO490Assets/raw/master/src/dmap/lessons/naive-bayes/html/a3.png"/></td><td style="text-align:center"><img alt="8050.jpg" class="med max-width" src="https://github.com/NSF-EC/INFO490Assets/raw/master/src/dmap/lessons/naive-bayes/html/8050.jpg"/></td></tr><tr><td style="text-align:left">Ave. sepal length</td><td style="text-align:center">4.3</td><td style="text-align:center">7.9</td><td style="text-align:center">5.84</td></tr><tr><td style="text-align:left">Ave. sepal width</td><td style="text-align:center">2.0</td><td style="text-align:center">4.4</td><td style="text-align:center">3.05</td></tr><tr><td style="text-align:left">Ave. petal length</td><td style="text-align:center">1.0</td><td style="text-align:center">6.9</td><td style="text-align:center">3.76</td></tr><tr><td style="text-align:left">Ave. petal width</td><td style="text-align:center">0.1</td><td style="text-align:center">2.5</td><td style="text-align:center">1.20</td></tr></tbody></table><p class="new">The averages from the table are taken from a dataset that we can be accessed through the Scikit-learn <a href="https://scikit-learn.org/stable/datasets/index.html" target="_blank">Iris Dataset</a></p><pre><code>from sklearn.datasets import load_iris</code></pre><p class="new">Now imagine that you want to classify an Iris, that has a pethal lenght of <img alt="math?math=%5Clarge%20x_i%20%3D%205%20cm" class="jax" src="https://render.githubusercontent.com/render/math?math=%5Clarge%20x_i%20%3D%205%20cm" style="display:inline-block"/></p><p class="new">If we graph a normal distribution of the Pethal lenght per class, and look for the interesection of each 
distribution with the value of 5 cm, we will get the likelihood of getting that value, given that the your 
new iris belongs to each class.</p><p class="new"><img alt="Gaussian Likelihood" src="https://github.com/NSF-EC/INFO490Assets/raw/master/src/dmap/lessons/naive-bayes/html/a4.png"/></p><p class="new">That is basically what happens for each feature in order to classify the new Iris. </p><p class="new">With the exception of this, the general formula to calculate the probability of having a specific iris 
given the flower features remains the same:</p><img alt="math?math=%5Clarge%20P(y%7CX)%20%3D%20log(P(y))%20%2B%20log(P(x_%7B1%7D%7Cy))%20%2B%20log(P(x_%7B2%7D%7Cy))%20%2B%20log(P(x_%7B3%7D%7Cy))%20..." class="jax" src="https://render.githubusercontent.com/render/math?math=%5Clarge%20P(y%7CX)%20%3D%20log(P(y))%20%2B%20log(P(x_%7B1%7D%7Cy))%20%2B%20log(P(x_%7B2%7D%7Cy))%20%2B%20log(P(x_%7B3%7D%7Cy))%20..."/><p class="new">It also used the logarithm to make the processing lighter and also to avoid errors produced by likelihoods 
too closed to 0 (Like the one for the Iris-Setosa class in the previous example). Moreover, the gaussian 
classifier also uses a smoothing variable, with a default value way smaller than the Laplace 
smotther (<img alt="math?math=%5Clarge%20%5Calpha%20%3D%201e%5E%7B-9%7D" class="jax" src="https://render.githubusercontent.com/render/math?math=%5Clarge%20%5Calpha%20%3D%201e%5E%7B-9%7D" style="display:inline-block"/>).</p><p class="new">Let's explore a little more of the scikit-lern and the Iris dataset. We can import and load it 
calling the load_iris method from the library. </p><pre><code>from sklearn.datasets import load_iris
X, y = load_iris(return_X_y=True)
print(X.shape)</code></pre><p class="new">By printing the the shape of X, you will be able to know how many observations are listed in the dataset, 
and how many features each observation has.</p><p class="new">Sklearn also included a tool to split a dataset between training and testing data.</p><pre><code>from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5)</code></pre><p class="new">The <code>test_size</code> paramether in <code>train_test_split()</code>receives the proportion that we want to have for 
the test dataset, while the rest will be used to train the classifier.</p><p class="new">By doing that, you can test the precision of your GaussianNB classifier:</p><pre><code>from sklearn.naive_bayes import GaussianNB

gnb = GaussianNB()
y_pred = gnb.fit(X_train, y_train).predict(X_test)</code></pre><p class="new">Now, test the library yourself:</p></div></div></body></html>