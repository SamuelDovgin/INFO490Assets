<!DOCTYPE html><html lang='en'><head><title>Naive Bayes</title><meta charset="utf-8"><style>/*! normalize.css v8.0.1 | MIT License | github.com/necolas/normalize.css */html{line-height:1.15;-webkit-text-size-adjust:100%}body{margin:0}h1{font-size:2em;margin:.67em 0}hr{box-sizing:content-box;height:0;overflow:visible}pre{font-family:monospace,monospace;font-size:1em}a{background-color:transparent}strong{font-weight:bolder}code{font-family:monospace,monospace;font-size:1em}img{border-style:none}::-webkit-file-upload-button{-webkit-appearance:button;font:inherit}blockquote,h1,h3,h4,hr,p,pre{margin:0}ol,ul{list-style:none;margin:0;padding:0}html{font-family:system-ui,-apple-system,BlinkMacSystemFont,"Segoe UI",Roboto,"Helvetica Neue",Arial,"Noto Sans",sans-serif,"Apple Color Emoji","Segoe UI Emoji","Segoe UI Symbol","Noto Color Emoji";line-height:1.5}*,::after,::before{box-sizing:border-box;border-width:0;border-style:solid;border-color:#e2e8f0}hr{border-top-width:1px}img{border-style:solid}table{border-collapse:collapse}h1,h3,h4{font-size:inherit;font-weight:inherit}a{color:inherit;text-decoration:inherit}code,pre{font-family:Menlo,Monaco,Consolas,"Liberation Mono","Courier New",monospace}img{display:block;vertical-align:middle}img{max-width:100%;height:auto}.bg-gray-200{--bg-opacity:1;background-color:#edf2f7;background-color:rgba(237,242,247,var(--bg-opacity))}.bg-gray-300{--bg-opacity:1;background-color:#e2e8f0;background-color:rgba(226,232,240,var(--bg-opacity))}.border-gray-400{--border-opacity:1;border-color:#cbd5e0;border-color:rgba(203,213,224,var(--border-opacity))}.border-gray-500{--border-opacity:1;border-color:#a0aec0;border-color:rgba(160,174,192,var(--border-opacity))}.rounded{border-radius:.25rem}.rounded-full{border-radius:9999px}.border-solid{border-style:solid}.border-t{border-top-width:1px}.inline-block{display:inline-block}.flex{display:flex}.justify-center{justify-content:center}.justify-around{justify-content:space-around}.font-serif{font-family:Georgia,Cambria,"Times New Roman",Times,serif}.font-semibold{font-weight:600}.font-bold{font-weight:700}.h-64{height:16rem}.text-sm{font-size:.875rem}.text-base{font-size:1rem}.text-xl{font-size:1.25rem}.m-2{margin:.5rem}.mx-auto{margin-left:auto;margin-right:auto}.mr-2{margin-right:.5rem}.mb-3{margin-bottom:.75rem}.ml-3{margin-left:.75rem}.mt-4{margin-top:1rem}.mt-6{margin-top:1.5rem}.max-w-sm{max-width:24rem}.max-w-2xl{max-width:42rem}.object-contain{-o-object-fit:contain;object-fit:contain}.object-center{-o-object-position:center;object-position:center}.overflow-hidden{overflow:hidden}.p-1{padding:.25rem}.py-1{padding-top:.25rem;padding-bottom:.25rem}.px-3{padding-left:.75rem;padding-right:.75rem}.py-4{padding-top:1rem;padding-bottom:1rem}.px-4{padding-left:1rem;padding-right:1rem}.px-6{padding-left:1.5rem;padding-right:1.5rem}.px-10{padding-left:2.5rem;padding-right:2.5rem}.pl-3{padding-left:.75rem}.shadow-lg{box-shadow:0 10px 15px -3px rgba(0,0,0,.1),0 4px 6px -2px rgba(0,0,0,.05)}.text-center{text-align:center}.text-black{--text-opacity:1;color:#000;color:rgba(0,0,0,var(--text-opacity))}.text-gray-700{--text-opacity:1;color:#4a5568;color:rgba(74,85,104,var(--text-opacity))}.text-gray-800{--text-opacity:1;color:#2d3748;color:rgba(45,55,72,var(--text-opacity))}.whitespace-no-wrap{white-space:nowrap}.w-full{width:100%}.text-tiny{font-size:.5rem!important}.lesson{padding-left:10px;padding-right:10px;--bg-opacity:1;background-color:#edf2f7;background-color:rgba(237,242,247,var(--bg-opacity))}.lesson-footer{margin-top:50px;margin-top:20px}.lesson ol{list-style:decimal;list-style-position:inside;margin-left:1em}.lesson ul{list-style:none!important;list-style-position:inside;margin-left:1em}.lesson ul li{padding-left:1em;text-indent:-1em}.lesson ul li::before{content:"â—";padding-right:5px}span{white-space:nowrap}p.new{padding-top:.5em;padding-bottom:.5em}.h-18rem{height:17rem}h1,h3,h4{font-weight:700;margin-bottom:.25em;font-family:Georgia,Cambria,"Times New Roman",Times,serif!important}h1{margin-top:.5em;font-size:2em!important}h3{margin-top:.5em;font-size:1.25em!important}h4{margin-top:.25em;font-size:1em!important}p.new a{text-decoration:underline}blockquote{font-size:1em;background:#f9f9f9;border-left:10px solid #ccc;margin:1.5em 10px;padding:.5em 10px;border-left-color:#ffcd69;border-right-color:#f6ba59;quotes:"\201C""\201D""\2018""\2019"}blockquote:before{color:#ccc;content:open-quote;font-size:4em;line-height:.1em;margin-right:.25em;vertical-align:-.4em}blockquote p{display:inline}img.med{width:66.666667%;margin-left:1.25rem;border:1px solid #021a40}img:not([class]){width:66.666667%;margin-left:1.25rem;border:1px solid #021a40}code{font-size:smaller}pre code{font-size:15px}pre code:not(.line-number){background:#f4f4f4;font-family:monospace;font-size:15px;-webkit-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none;cursor:default;touch-action:none;-webkit-touch-callout:none;-webkit-tap-highlight-color:transparent;border:1px solid #ddd;color:#666;page-break-inside:avoid;display:block;min-width:740px;max-width:740px;line-height:1.6;margin-bottom:1.6em;padding:1em 1.5em;-moz-tab-size:2;-o-tab-size:2;tab-size:2;overflow-x:auto;word-wrap:break-word;white-space:pre-wrap;border-left:3px solid #f36d33}div.code-starter>pre code{border-left:3px solid #fdff44!important;background-image:radial-gradient(rgba(0,150,0,.75),#000 120%);color:#fff;font:.9rem Inconsolata,monospace}div.code-starter>pre code::after{content:"\a$_"}pre{counter-reset:line}</style><script src="https://kit.fontawesome.com/7efc4bcee2.js" crossOrigin="anonymous"></script></head><body class="lesson"><div class="main-content bg-gray-200 text-black p-1 pl-3 text-xl font-serif"><div class="md-inner">
<p class="new">But... why would we do all this? </p><p class="new">Well, imagine you receive a new email that says:</p><blockquote><p class="new">"Dear Friend, Click here and win a huge prize!"</p></blockquote><p class="new">Now, you want an algorithm to identify if that message is spam or a normal email.</p><p class="new">Here is where the Bayes theorem can help you to predict the class variable of
your new email with those specific features.</p><h4 id="bayes-theorem-says">Bayes theorem says:</h4><img alt="math?math=%5Clarge%20%5Cbegin%7Baligned%7D%20P(y%20%7CX)%20%3D%20%5Cfrac%7BP(y)P(X%7Cy)%7D%7BP(X)%7D%20%5Cend%7Baligned%7D" class="jax" src="https://render.githubusercontent.com/render/math?math=%5Clarge%20%5Cbegin%7Baligned%7D%20P(y%20%7CX)%20%3D%20%5Cfrac%7BP(y)P(X%7Cy)%7D%7BP(X)%7D%20%5Cend%7Baligned%7D"/><p class="new">Where <img alt="math?math=%5Clarge%20P(y%7CX)" class="jax" src="https://render.githubusercontent.com/render/math?math=%5Clarge%20P(y%7CX)" style="display:inline-block"/> is the probability of an observation (an email) belonging to 
certain class <img alt="math?math=%5Clarge%20y" class="jax" src="https://render.githubusercontent.com/render/math?math=%5Clarge%20y" style="display:inline-block"/> (Spam or norma)l, given that it has certain list of 
features (word counts). <img alt="math?math=%5Clarge%20P(y)" class="jax" src="https://render.githubusercontent.com/render/math?math=%5Clarge%20P(y)" style="display:inline-block"/> is the general probability of finding this 
class in an observation, also called the <em>prior probability</em>. <img alt="math?math=%5Clarge%20P(X%20%7C%20y)" class="jax" src="https://render.githubusercontent.com/render/math?math=%5Clarge%20P(X%20%7C%20y)" style="display:inline-block"/> is 
the <em>likelihood</em> of seeing those features in an observation with that class, 
i.e. "Dear Friend, ..." in spam emails. Finally, <img alt="math?math=%5Clarge%20P(X)" class="jax" src="https://render.githubusercontent.com/render/math?math=%5Clarge%20P(X)" style="display:inline-block"/>, is the 
probability of findind those words in general, also called <em>marginalization</em>. </p><p class="new">This seems kind of tricky, because the general probability of finding a spam 
email <img alt="math?math=%5Clarge%20P(y)" class="jax" src="https://render.githubusercontent.com/render/math?math=%5Clarge%20P(y)" style="display:inline-block"/> could be anything. But we can use our training set, and take 
the estimation we did before as a prior!</p><p class="new">Then our priors are: </p><img alt="math?math=%5Clarge%20P(spam)%20%3D%20%200.333" class="jax" src="https://render.githubusercontent.com/render/math?math=%5Clarge%20P(spam)%20%3D%20%200.333"/><img alt="math?math=%5Clarge%20P(normal)%20%3D%200.666" class="jax" src="https://render.githubusercontent.com/render/math?math=%5Clarge%20P(normal)%20%3D%200.666"/><p class="new"><strong>*Note:</strong> If we decide not to use the data to calculate the prior, we can always 
make a simple estimation of the prior. For a binary classification, it is common to 
use 0.5. for both classes.*</p><p class="new">On the other hand, the general probability of the features happening <img alt="math?math=%5CLarge%20P(X)" class="jax" src="https://render.githubusercontent.com/render/math?math=%5CLarge%20P(X)" style="display:inline-block"/> will be 
the same no matter if the new email is spam or not, so we don't really need to worry 
about that either. </p><p class="new">Anyways, calculating the probability of these exact features happening in a spam emails 
sounds still tricky. If we could be a litle more... <em>naive</em>, it will be simpler.</p><h4 id="how-naive">How Naive?</h4><p class="new">The naive bayes assumes <em>conditional independence</em> for the attributes given the class. 
This means that this technique ignores the possible relation of the attributes of an 
observation, e.g. the words in a text, given that the text belongs to a specific class. 
Then:</p><img alt="math?math=%5Clarge%20P(y%7CX)%20%3D%20P(y)%20%5Ctimes%20P(x_%7B1%7D%7Cy)%20%5Ctimes%20P(x_%7B2%7D%7Cy)%20%5Ctimes%20P(x_%7B3%7D%7Cy)" class="jax" src="https://render.githubusercontent.com/render/math?math=%5Clarge%20P(y%7CX)%20%3D%20P(y)%20%5Ctimes%20P(x_%7B1%7D%7Cy)%20%5Ctimes%20P(x_%7B2%7D%7Cy)%20%5Ctimes%20P(x_%7B3%7D%7Cy)"/><p class="new">So, given that the new email is composed by the features:  </p><p class="new"><code>W = ['dear', 'friend','click', 'here', 'and', 'win', 'a', 'huge', 'prize']</code></p><p class="new">The probability of this email being spam is:</p><img alt="math?math=%5Clarge%20P(spam%20%7C%20W)%20%3D%20P(spam)%20%5Ctimes%20P('dear'%7Cspam)%20%5Ctimes%20P('friend'%7Cspam)%20%5Ctimes%20P('click'%7Cspam)%20..." class="jax" src="https://render.githubusercontent.com/render/math?math=%5Clarge%20P(spam%20%7C%20W)%20%3D%20P(spam)%20%5Ctimes%20P('dear'%7Cspam)%20%5Ctimes%20P('friend'%7Cspam)%20%5Ctimes%20P('click'%7Cspam)%20..."/><br/><img alt="math?math=%5Clarge%20P(spam%20%7C%20W)%20%3D%200.333%20%5Ctimes%20%5Cfrac%7B0%7D%7B150%7D%20%5Ctimes%20%5Cfrac%7B0%7D%7B150%7D%20%5Ctimes%20%5Cfrac%7B3%7D%7B150%7D%20%5Ctimes%20%5Cfrac%7B2%7D%7B150%7D%20%5Ctimes%20%5Cfrac%7B3%7D%7B150%7D%20%5Ctimes%20%5Cfrac%7B4%7D%7B150%7D%20%5Ctimes%20%5Cfrac%7B4%7D%7B150%7D%20%5Ctimes%20%5Cfrac%7B2%7D%7B150%7D%20%5Ctimes%20%5Cfrac%7B2%7D%7B150%7D%20%3D%200" class="jax" src="https://render.githubusercontent.com/render/math?math=%5Clarge%20P(spam%20%7C%20W)%20%3D%200.333%20%5Ctimes%20%5Cfrac%7B0%7D%7B150%7D%20%5Ctimes%20%5Cfrac%7B0%7D%7B150%7D%20%5Ctimes%20%5Cfrac%7B3%7D%7B150%7D%20%5Ctimes%20%5Cfrac%7B2%7D%7B150%7D%20%5Ctimes%20%5Cfrac%7B3%7D%7B150%7D%20%5Ctimes%20%5Cfrac%7B4%7D%7B150%7D%20%5Ctimes%20%5Cfrac%7B4%7D%7B150%7D%20%5Ctimes%20%5Cfrac%7B2%7D%7B150%7D%20%5Ctimes%20%5Cfrac%7B2%7D%7B150%7D%20%3D%200"/><br/><img alt="math?math=%5Clarge%20P(normal%20%7C%20W)%20%3D%20P(normal)%20%5Ctimes%20P('dear'%7Cnormal)%20%5Ctimes%20P('friend'%7Cnormal)..." class="jax" src="https://render.githubusercontent.com/render/math?math=%5Clarge%20P(normal%20%7C%20W)%20%3D%20P(normal)%20%5Ctimes%20P('dear'%7Cnormal)%20%5Ctimes%20P('friend'%7Cnormal)..."/><br/><img alt="math?math=%5Clarge%20P(normal%20%7C%20W)%20%3D%200.666%20%5Ctimes%20%5Cfrac%7B4%7D%7B350%7D%20%5Ctimes%20%5Cfrac%7B3%7D%7B350%7D%20%5Ctimes%20%5Cfrac%7B1%7D%7B350%7D%20%5Ctimes%20%5Cfrac%7B2%7D%7B350%7D%20%5Ctimes%20%5Cfrac%7B3%7D%7B350%7D%20%5Ctimes%20%5Cfrac%7B0%7D%7B350%7D%20%5Ctimes%20%5Cfrac%7B4%7D%7B350%7D%20%5Ctimes%20%5Cfrac%7B2%7D%7B350%7D%20%5Ctimes%20%5Cfrac%7B1%7D%7B150%7D%20%3D%200" class="jax" src="https://render.githubusercontent.com/render/math?math=%5Clarge%20P(normal%20%7C%20W)%20%3D%200.666%20%5Ctimes%20%5Cfrac%7B4%7D%7B350%7D%20%5Ctimes%20%5Cfrac%7B3%7D%7B350%7D%20%5Ctimes%20%5Cfrac%7B1%7D%7B350%7D%20%5Ctimes%20%5Cfrac%7B2%7D%7B350%7D%20%5Ctimes%20%5Cfrac%7B3%7D%7B350%7D%20%5Ctimes%20%5Cfrac%7B0%7D%7B350%7D%20%5Ctimes%20%5Cfrac%7B4%7D%7B350%7D%20%5Ctimes%20%5Cfrac%7B2%7D%7B350%7D%20%5Ctimes%20%5Cfrac%7B1%7D%7B150%7D%20%3D%200"/><br/><p class="new">Maybe you noticed that this is not insightfull at all, what happened? 
Well, as our new email contains words that haven't been counted in some of the 
two classes, the probability is multiplied by 0, and it ended up being 0 for 
both clases. </p><p class="new">There are two things that we need to implement, a <em>smoothing method</em> in the counts 
and then use the logarithm of each value.</p><p class="new">First, the <strong>smoothing priors</strong>
<img alt="math?math=%5CLarge%20%5Calpha%20%5Cgt%200" class="jax" src="https://render.githubusercontent.com/render/math?math=%5CLarge%20%5Calpha%20%5Cgt%200" style="display:inline-block"/> accounts for features not present in the learning 
samples and prevents zero probabilities in further computations. Setting <img alt="math?math=%5Clarge%20%5Calpha%20%3D%201" class="jax" src="https://render.githubusercontent.com/render/math?math=%5Clarge%20%5Calpha%20%3D%201" style="display:inline-block"/> is called 
Laplace smoothing, while <img alt="math?math=%5Clarge%20P(X%7Cy)" class="jax" src="https://render.githubusercontent.com/render/math?math=%5Clarge%20P(X%7Cy)" style="display:inline-block"/> are estimated by a smoothed version of maximum likelihood (the relative 
frequency counting):</p><img alt="math?math=%5Clarge%20P(x_%7B1%7D%7Cy)%20%3D%20%5Cfrac%7BN_%7Byi%7D%2B%5Calpha%7D%7BN_%7By%7D%2B%5Calpha%20n%7D" class="jax" src="https://render.githubusercontent.com/render/math?math=%5Clarge%20P(x_%7B1%7D%7Cy)%20%3D%20%5Cfrac%7BN_%7Byi%7D%2B%5Calpha%7D%7BN_%7By%7D%2B%5Calpha%20n%7D"/><p class="new">where <img alt="math?math=%5Clarge%20n" class="jax" src="https://render.githubusercontent.com/render/math?math=%5Clarge%20n" style="display:inline-block"/> is the number of features in X.</p><p class="new">Then, we calculate the logarithm for each part of the bayes theorem, changing the multiplication to a sum. 
This not only helps us to avoid multiplying by 0, but also make the computation way simpler. </p><img alt="math?math=%5Clarge%20P(y%7CX)%20%3D%20log(P(y))%20%2B%20log(P(x_%7B1%7D%7Cy))%20%2B%20log(P(x_%7B2%7D%7Cy))%20%2B%20log(P(x_%7B3%7D%7Cy))%20..." class="jax" src="https://render.githubusercontent.com/render/math?math=%5Clarge%20P(y%7CX)%20%3D%20log(P(y))%20%2B%20log(P(x_%7B1%7D%7Cy))%20%2B%20log(P(x_%7B2%7D%7Cy))%20%2B%20log(P(x_%7B3%7D%7Cy))%20..."/><p class="new">Please, calculate again the probabilities of P(spam|W) and P(normal|W) including these two new considerations. </p><p class="new"><strong><em>Tip:</em></strong> <em>You can use Numpy for this. Don't forget to use a smothing prior</em></p></div></div></body></html>