<!DOCTYPE html><html lang='en'><head><title>Naive Bayes</title><meta charset="utf-8"><style>/*! normalize.css v8.0.1 | MIT License | github.com/necolas/normalize.css */html{line-height:1.15;-webkit-text-size-adjust:100%}body{margin:0}h1{font-size:2em;margin:.67em 0}hr{box-sizing:content-box;height:0;overflow:visible}pre{font-family:monospace,monospace;font-size:1em}a{background-color:transparent}strong{font-weight:bolder}code{font-family:monospace,monospace;font-size:1em}img{border-style:none}::-webkit-file-upload-button{-webkit-appearance:button;font:inherit}blockquote,h1,h3,h4,hr,p,pre{margin:0}ol,ul{list-style:none;margin:0;padding:0}html{font-family:system-ui,-apple-system,BlinkMacSystemFont,"Segoe UI",Roboto,"Helvetica Neue",Arial,"Noto Sans",sans-serif,"Apple Color Emoji","Segoe UI Emoji","Segoe UI Symbol","Noto Color Emoji";line-height:1.5}*,::after,::before{box-sizing:border-box;border-width:0;border-style:solid;border-color:#e2e8f0}hr{border-top-width:1px}img{border-style:solid}table{border-collapse:collapse}h1,h3,h4{font-size:inherit;font-weight:inherit}a{color:inherit;text-decoration:inherit}code,pre{font-family:Menlo,Monaco,Consolas,"Liberation Mono","Courier New",monospace}img{display:block;vertical-align:middle}img{max-width:100%;height:auto}.bg-gray-200{--bg-opacity:1;background-color:#edf2f7;background-color:rgba(237,242,247,var(--bg-opacity))}.bg-gray-300{--bg-opacity:1;background-color:#e2e8f0;background-color:rgba(226,232,240,var(--bg-opacity))}.border-gray-400{--border-opacity:1;border-color:#cbd5e0;border-color:rgba(203,213,224,var(--border-opacity))}.border-gray-500{--border-opacity:1;border-color:#a0aec0;border-color:rgba(160,174,192,var(--border-opacity))}.rounded{border-radius:.25rem}.rounded-full{border-radius:9999px}.border-solid{border-style:solid}.border-t{border-top-width:1px}.inline-block{display:inline-block}.flex{display:flex}.justify-center{justify-content:center}.justify-around{justify-content:space-around}.font-serif{font-family:Georgia,Cambria,"Times New Roman",Times,serif}.font-semibold{font-weight:600}.font-bold{font-weight:700}.h-64{height:16rem}.text-sm{font-size:.875rem}.text-base{font-size:1rem}.text-xl{font-size:1.25rem}.m-2{margin:.5rem}.mx-auto{margin-left:auto;margin-right:auto}.mr-2{margin-right:.5rem}.mb-3{margin-bottom:.75rem}.ml-3{margin-left:.75rem}.mt-4{margin-top:1rem}.mt-6{margin-top:1.5rem}.max-w-sm{max-width:24rem}.max-w-2xl{max-width:42rem}.object-contain{-o-object-fit:contain;object-fit:contain}.object-center{-o-object-position:center;object-position:center}.overflow-hidden{overflow:hidden}.p-1{padding:.25rem}.py-1{padding-top:.25rem;padding-bottom:.25rem}.px-3{padding-left:.75rem;padding-right:.75rem}.py-4{padding-top:1rem;padding-bottom:1rem}.px-4{padding-left:1rem;padding-right:1rem}.px-6{padding-left:1.5rem;padding-right:1.5rem}.px-10{padding-left:2.5rem;padding-right:2.5rem}.pl-3{padding-left:.75rem}.shadow-lg{box-shadow:0 10px 15px -3px rgba(0,0,0,.1),0 4px 6px -2px rgba(0,0,0,.05)}.text-center{text-align:center}.text-black{--text-opacity:1;color:#000;color:rgba(0,0,0,var(--text-opacity))}.text-gray-700{--text-opacity:1;color:#4a5568;color:rgba(74,85,104,var(--text-opacity))}.text-gray-800{--text-opacity:1;color:#2d3748;color:rgba(45,55,72,var(--text-opacity))}.whitespace-no-wrap{white-space:nowrap}.w-full{width:100%}.text-tiny{font-size:.5rem!important}body{color:#000!important}.lesson{padding-left:10px;padding-right:10px;--bg-opacity:1;background-color:#edf2f7;background-color:rgba(237,242,247,var(--bg-opacity))}.lesson-footer{margin-top:50px;margin-top:20px}.lesson ol{list-style:decimal;list-style-position:inside;margin-left:1em}.lesson ul{list-style:none!important;list-style-position:inside;margin-left:1em}.lesson ul li{padding-left:1em;text-indent:-1em}.lesson ul li::before{content:"‚óè";padding-right:5px}span{white-space:nowrap}p.new{padding-top:.5em;padding-bottom:.5em}.h-18rem{height:17rem}h1,h3,h4{font-weight:700;margin-bottom:.25em;font-family:Georgia,Cambria,"Times New Roman",Times,serif!important}h1{margin-top:.5em;font-size:2em!important}h3{margin-top:.5em;font-size:1.25em!important}h4{margin-top:.25em;font-size:1em!important}p.new a{text-decoration:underline}blockquote{font-size:1em;background:#f9f9f9;border-left:10px solid #ccc;margin:1.5em 10px;padding:.5em 10px;border-left-color:#ffcd69;border-right-color:#f6ba59;quotes:"\201C""\201D""\2018""\2019"}blockquote:before{color:#ccc;content:open-quote;font-size:4em;line-height:.1em;margin-right:.25em;vertical-align:-.4em}blockquote p{display:inline}img.med{width:66.666667%;margin-left:1.25rem;border:1px solid #021a40}img:not([class]){width:66.666667%;margin-left:1.25rem;border:1px solid #021a40}code{font-size:smaller}pre code{font-size:15px}pre code:not(.line-number){background:#f4f4f4;font-family:monospace;font-size:15px;-webkit-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none;cursor:default;touch-action:none;-webkit-touch-callout:none;-webkit-tap-highlight-color:transparent;border:1px solid #ddd;color:#666;page-break-inside:avoid;display:block;min-width:740px;max-width:740px;line-height:1.6;margin-bottom:1.6em;padding:1em 1.5em;-moz-tab-size:2;-o-tab-size:2;tab-size:2;overflow-x:auto;word-wrap:break-word;white-space:pre-wrap;border-left:3px solid #f36d33}div.code-starter>pre code{border-left:3px solid #fdff44!important;background-image:radial-gradient(rgba(0,150,0,.75),#000 120%);color:#fff;font:.9rem Inconsolata,monospace}div.code-starter>pre code::after{content:"\a$_"}pre{counter-reset:line}</style><script src="https://kit.fontawesome.com/7efc4bcee2.js" crossOrigin="anonymous"></script></head><body class="lesson"><div class="main-content bg-gray-200 text-black p-1 pl-3 text-xl font-serif"><div class="md-inner">
<h1 class="overview"></h1><div class="lesson-overview bg-gray-200 flex justify-center"><div class="displaycard max-w-sm rounded overflow-hidden shadow-lg"><div>¬†</div><img alt="Text" class="object-contain h-64 w-full" src="https://github.com/NSF-EC/INFO490Assets/raw/master/src/dmap/lessons/naive-bayes/html/MachineLearningV1-sm.png"/><div class="px-6 py-4"><div class="text-center font-bold text-xl">Naive Bayes</div><p class="text-center text-gray-800 text-xl">Event Probability</p><div class="text-gray-700 text-base">¬†</div><div class="text-center mb-3"><span class="inline-block bg-gray-300 rounded-full px-3 py-1 text-sm font-semibold text-gray-700 mr-2">#info490</span><span class="inline-block bg-gray-300 rounded-full px-3 py-1 text-sm font-semibold text-gray-700 mr-2">#machine learning</span><span class="inline-block bg-gray-300 rounded-full px-3 py-1 text-sm font-semibold text-gray-700 mr-2">#supervised</span></div><div class="flex border-t border-solid border-gray-500 justify-around bg-gray-200"><div class="text-gray-700 text-center px-4 m-2 text-sm"><span class="whitespace-no-wrap">D.MÔπ†üêç</span></div><div class="text-gray-700 text-center px-4 m-2 text-sm"><span class="whitespace-no-wrap"><strong>Version:</strong> <!-- -->Fall 2020<!-- --> <!-- -->08.20.20</span></div></div><div class="text-gray-700 text-center text-tiny">All Rights Reserved</div></div></div><div class="ml-3 mb-3 h-18rem rounded overflow-hidden shadow-lg"><div class="px-6 py-4"><div class="text-center font-bold text-xl">Colab Notes</div><p class="max-w-sm text-gray-800 text-sm">1. <strong>Copy</strong> this notebook into your google drive</p><p class="max-w-sm text-gray-800 text-sm">2. <strong>Share</strong> the notebook, and copy the share ID</p><p class="max-w-sm text-gray-800 text-sm">3. <strong>Set</strong> the NOTEBOOK_ID variable to the share ID and <strong>SAVE the notebook</strong> (again)</p><p class="max-w-sm text-gray-800 text-sm">4. <strong>Run</strong> the code cell that installs the INFO 490 IDE</p><div class="text-center font-bold text-xl">¬†</div><div class="text-center font-bold text-xl">Jupyter/PyCharm Notes</div><p class="max-w-sm text-gray-800 text-sm">The testing framework does <strong>not work</strong> (at this time) for Jupyter  notebooks or local code development.</p></div></div></div><h1 class="section" id="section1">Naive Bayes</h1><p class="new">Naive Bayes methods are a set of supervised learning algorithms based on applying
Bayes‚Äô theorem with the ‚Äúnaive‚Äù assumption.</p><p class="new">Do you remember Bayes' Theorem? Or What is the difference between the Bayes'
followers and the Frequentist? </p><p class="new">Well! in this class we will give you a quick overview of these concepts and you
will be able to build your own Naive Bayes classifier.  </p><h3 id="content">Content</h3><ol start="1"><li>Bayes‚Äô Theorem and Naive Bayes classifier</li><li>Scikit-learn for Naive Bayes</li><li>Multinomial Naive Bayes Classifier</li><li>Gaussian Naive Bayes Classifier</li><li>When to use Naive Bayes</li><li>Assignment</li></ol><h3 id="1-bayes-theorem-and-naive-bayes-classifier">1. Bayes‚Äô Theorem and Naive Bayes classifier</h3><p class="new">While frequentists used to make inferences (to predict results) based only on
the <em>likelihood</em> of the events,  Bayes' theorem also included the concepts of
<em>prior</em> and <em>posterior</em>. </p><p class="new">The <strong>likelihood</strong> expresses how possible it is getting a result based on a set
of observations. To illustrate this, let's think about a simple but typical
example. Imagine you have a several emails, what we will call the <strong>training set</strong>.
These are classified as <em>Spam</em> or <em>Normal</em> messages. </p><img alt="uc?id=16EcZz1t7EBw9g-acFZtKEl8_3sF_MZTR" class="med" src="https://github.com/NSF-EC/INFO490Assets/raw/master/src/dmap/lessons/naive-bayes/html/a0.png"/><p class="new">We could estimate how likely it is to randomly select an email with the label <img alt="math?math=%5Clarge%20Y" class="jax" src="https://render.githubusercontent.com/render/math?math=%5Clarge%20Y" style="display:inline-block"/>(Spam or normal) from a set of emails. This likelihood can be calculated by
dividing the number emails with the spacific lable (<img alt="math?math=%5Clarge%20N_%7By%7D" class="jax" src="https://render.githubusercontent.com/render/math?math=%5Clarge%20N_%7By%7D" style="display:inline-block"/>), by the
total number of emails in the set (<img alt="math?math=%5Clarge%20N" class="jax" src="https://render.githubusercontent.com/render/math?math=%5Clarge%20N" style="display:inline-block"/>):</p><div class="object-center mx-auto px-10 object-center"><img alt="math?math=%5Clarge%20P(y)%20%3D%20%5Cfrac%7BN_%7By%7D%7D%7BN%7D" class="jax" src="https://render.githubusercontent.com/render/math?math=%5Clarge%20P(y)%20%3D%20%5Cfrac%7BN_%7By%7D%7D%7BN%7D"/></div><p class="new">Then, the likelihood to randomly select a spam email is:</p><img alt="math?math=%5Clarge%20P(spam)%3D%5Cfrac%7B2%7D%7B6%7D%20%3D%200.333" class="jax" src="https://render.githubusercontent.com/render/math?math=%5Clarge%20P(spam)%3D%5Cfrac%7B2%7D%7B6%7D%20%3D%200.333"/><p class="new">Now calculate the likelihood of randomly selecting a normal email!</p><div class="ide code-starter blink"><pre><code># Replace the 'None' with the right operation (you can simply use the division operation '/').
# If your equation is right, the result should be 0.666!
p_normal = None
print('The likelihood of randomly selet a normal email is: ',p_normal)</code></pre></div><hr/><p class="new">On the other hand, Bayes proposed that probabilities can be updated every time
new evidence is found. Then, the initial evidence is used as the <strong>Prior</strong>, while
new likelihoods are found in new evidence.</p><p class="new">Let's say now we open the emails and study their content. We could say that the
words contained in the emails are their <strong>features</strong> or 
characteristics. These features can help us to identify spam emails too.
To do so, we need to register the counts of each unique word with their
frequency for each email group (Spam or normal). </p><img alt="uc?id=1TX3qTrilbYpL64nUfJaDhiSihOv-UGs1" class="med" src="https://github.com/NSF-EC/INFO490Assets/raw/master/src/dmap/lessons/naive-bayes/html/a1.png"/><table><thead><tr><th style="text-align:left">Word</th><th style="text-align:center">Count in Spam</th><th style="text-align:center">Count in Normal</th></tr></thead><tbody><tr><td style="text-align:left">Win</td><td style="text-align:center">4</td><td style="text-align:center">0</td></tr><tr><td style="text-align:left">Prize</td><td style="text-align:center">2</td><td style="text-align:center">1</td></tr><tr><td style="text-align:left">Money</td><td style="text-align:center">6</td><td style="text-align:center">1</td></tr><tr><td style="text-align:left">Dear</td><td style="text-align:center">0</td><td style="text-align:center">4</td></tr><tr><td style="text-align:left">Friend</td><td style="text-align:center">0</td><td style="text-align:center">3</td></tr><tr><td style="text-align:left">Prepare</td><td style="text-align:center">2</td><td style="text-align:center">3</td></tr><tr><td style="text-align:left">For</td><td style="text-align:center">2</td><td style="text-align:center">2</td></tr><tr><td style="text-align:left">The</td><td style="text-align:center">4</td><td style="text-align:center">4</td></tr><tr><td style="text-align:left">A</td><td style="text-align:center">4</td><td style="text-align:center">4</td></tr><tr><td style="text-align:left">Click</td><td style="text-align:center">3</td><td style="text-align:center">1</td></tr><tr><td style="text-align:left">Here</td><td style="text-align:center">2</td><td style="text-align:center">2</td></tr><tr><td style="text-align:left">And</td><td style="text-align:center">3</td><td style="text-align:center">3</td></tr><tr><td style="text-align:left">Huge</td><td style="text-align:center">2</td><td style="text-align:center">2</td></tr><tr><td style="text-align:left">...</td><td style="text-align:center">...</td><td style="text-align:center">...</td></tr><tr><td style="text-align:left">Total</td><td style="text-align:center">150</td><td style="text-align:center">350</td></tr></tbody></table><p class="new">By doing this, we can calculate the likelihood of finding a specific word in a 
document of a specific class. This conditional probability is calculated like this:</p><img alt="math?math=%5Clarge%20%5Cbegin%7Baligned%7D%20P(x_%7Bi%7D%7Cy)%20%3D%20%5Cfrac%7BN_%7Byi%7D%7D%7BN_%7By%7D%7D%20%5Cend%7Baligned%7D" class="jax" src="https://render.githubusercontent.com/render/math?math=%5Clarge%20%5Cbegin%7Baligned%7D%20P(x_%7Bi%7D%7Cy)%20%3D%20%5Cfrac%7BN_%7Byi%7D%7D%7BN_%7By%7D%7D%20%5Cend%7Baligned%7D"/><p class="new">Where <img alt="math?math=%5Clarge%20N_%7Byi%7D%20%3D%20%5Csum_%7Bx%5Cin%20T%7D" class="jax" src="https://render.githubusercontent.com/render/math?math=%5Clarge%20N_%7Byi%7D%20%3D%20%5Csum_%7Bx%5Cin%20T%7D" style="display:inline-block"/><img alt="math?math=%5Clarge%20x_%7Bi%7D" class="jax" src="https://render.githubusercontent.com/render/math?math=%5Clarge%20x_%7Bi%7D" style="display:inline-block"/> is the number of times 
feature <img alt="math?math=%5Clarge%20i" class="jax" src="https://render.githubusercontent.com/render/math?math=%5Clarge%20i" style="display:inline-block"/> appears in the sample of class <img alt="math?math=%5Clarge%20y" class="jax" src="https://render.githubusercontent.com/render/math?math=%5Clarge%20y" style="display:inline-block"/>and <img alt="math?math=%5Clarge%20N_%7By%7D%20%3D%20%5Csum_%7Bi%3D1%7D%5E%7Bn%7D" class="jax" src="https://render.githubusercontent.com/render/math?math=%5Clarge%20N_%7By%7D%20%3D%20%5Csum_%7Bi%3D1%7D%5E%7Bn%7D" style="display:inline-block"/><img alt="math?math=%5Clarge%20N_%7By%7D" class="jax" src="https://render.githubusercontent.com/render/math?math=%5Clarge%20N_%7By%7D" style="display:inline-block"/> is the total count of all features for class.</p><p class="new">Then, we could calculate where is more likely to find the word 'money'.</p><img alt="math?math=%5Clarge%20%5Cbegin%7Baligned%7D%20P('money'%7Cspam)%20%3D%20%5Cfrac%7BFreqMoneyInSpam%7D%7BTotalWordsInSpam%7D%20%5Cend%7Baligned%7D" class="jax" src="https://render.githubusercontent.com/render/math?math=%5Clarge%20%5Cbegin%7Baligned%7D%20P('money'%7Cspam)%20%3D%20%5Cfrac%7BFreqMoneyInSpam%7D%7BTotalWordsInSpam%7D%20%5Cend%7Baligned%7D"/><img alt="math?math=%5Clarge%20%5Cbegin%7Baligned%7D%20P('money'%7Cnormal)%20%3D%20%5Cfrac%7BFreqMoneyInNormal%7D%7BTotalWordsInNormal%7D%20%5Cend%7Baligned%7D" class="jax" src="https://render.githubusercontent.com/render/math?math=%5Clarge%20%5Cbegin%7Baligned%7D%20P('money'%7Cnormal)%20%3D%20%5Cfrac%7BFreqMoneyInNormal%7D%7BTotalWordsInNormal%7D%20%5Cend%7Baligned%7D"/><p class="new">Complete the code to find where is more likely to observe the word 'money':</p><div class="ide code-starter blink"><pre><code># Give the variables the right values
p_money_in_spam = None
p_money_in_normal = None
print("It is more likely to find the word 'money' in:", 'Normal emails' if p_money_in_normal &amp;gt; p_money_in_spam else 'Spam emails')</code></pre></div><p class="new">But... why would we do all this? </p><p class="new">Well, imagine you receive a new email that says:</p><blockquote><p class="new">"Dear Friend, Click here and win a huge prize!"</p></blockquote><p class="new">Now, you want an algorithm to identify if that message is spam or a normal email.</p><p class="new">Here is where the Bayes theorem can help you to predict the class variable of
your new email with those specific features.</p><h4 id="bayes-theorem-says">Bayes theorem says:</h4><img alt="math?math=%5Clarge%20%5Cbegin%7Baligned%7D%20P(y%20%7CX)%20%3D%20%5Cfrac%7BP(y)P(X%7Cy)%7D%7BP(X)%7D%20%5Cend%7Baligned%7D" class="jax" src="https://render.githubusercontent.com/render/math?math=%5Clarge%20%5Cbegin%7Baligned%7D%20P(y%20%7CX)%20%3D%20%5Cfrac%7BP(y)P(X%7Cy)%7D%7BP(X)%7D%20%5Cend%7Baligned%7D"/><p class="new">Where <img alt="math?math=%5Clarge%20P(y%7CX)" class="jax" src="https://render.githubusercontent.com/render/math?math=%5Clarge%20P(y%7CX)" style="display:inline-block"/> is the probability of an observation (an email) belonging to 
certain class <img alt="math?math=%5Clarge%20y" class="jax" src="https://render.githubusercontent.com/render/math?math=%5Clarge%20y" style="display:inline-block"/> (Spam or norma)l, given that it has certain list of 
features (word counts). <img alt="math?math=%5Clarge%20P(y)" class="jax" src="https://render.githubusercontent.com/render/math?math=%5Clarge%20P(y)" style="display:inline-block"/> is the general probability of finding this 
class in an observation, also called the <em>prior probability</em>. <img alt="math?math=%5Clarge%20P(X%20%7C%20y)" class="jax" src="https://render.githubusercontent.com/render/math?math=%5Clarge%20P(X%20%7C%20y)" style="display:inline-block"/> is 
the <em>likelihood</em> of seeing those features in an observation with that class, 
i.e. "Dear Friend, ..." in spam emails. Finally, <img alt="math?math=%5Clarge%20P(X)" class="jax" src="https://render.githubusercontent.com/render/math?math=%5Clarge%20P(X)" style="display:inline-block"/>, is the 
probability of findind those words in general, also called <em>marginalization</em>. </p><p class="new">This seems kind of tricky, because the general probability of finding a spam 
email <img alt="math?math=%5Clarge%20P(y)" class="jax" src="https://render.githubusercontent.com/render/math?math=%5Clarge%20P(y)" style="display:inline-block"/> could be anything. But we can use our training set, and take 
the estimation we did before as a prior!</p><p class="new">Then our priors are: </p><img alt="math?math=%5Clarge%20P(spam)%20%3D%20%200.333" class="jax" src="https://render.githubusercontent.com/render/math?math=%5Clarge%20P(spam)%20%3D%20%200.333"/><img alt="math?math=%5Clarge%20P(normal)%20%3D%200.666" class="jax" src="https://render.githubusercontent.com/render/math?math=%5Clarge%20P(normal)%20%3D%200.666"/><p class="new"><strong>*Note:</strong> If we decide not to use the data to calculate the prior, we can always 
make a simple estimation of the prior. For a binary classification, it is common to 
use 0.5. for both classes.*</p><p class="new">On the other hand, the general probability of the features happening <img alt="math?math=%5CLarge%20P(X)" class="jax" src="https://render.githubusercontent.com/render/math?math=%5CLarge%20P(X)" style="display:inline-block"/> will be 
the same no matter if the new email is spam or not, so we don't really need to worry 
about that either. </p><p class="new">Anyways, calculating the probability of these exact features happening in a spam emails 
sounds still tricky. If we could be a litle more... <em>naive</em>, it will be simpler.</p><h4 id="how-naive">How Naive?</h4><p class="new">The naive bayes assumes <em>conditional independence</em> for the attributes given the class. 
This means that this technique ignores the possible relation of the attributes of an 
observation, e.g. the words in a text, given that the text belongs to a specific class. 
Then:</p><img alt="math?math=%5Clarge%20P(y%7CX)%20%3D%20P(y)%20%5Ctimes%20P(x_%7B1%7D%7Cy)%20%5Ctimes%20P(x_%7B2%7D%7Cy)%20%5Ctimes%20P(x_%7B3%7D%7Cy)" class="jax" src="https://render.githubusercontent.com/render/math?math=%5Clarge%20P(y%7CX)%20%3D%20P(y)%20%5Ctimes%20P(x_%7B1%7D%7Cy)%20%5Ctimes%20P(x_%7B2%7D%7Cy)%20%5Ctimes%20P(x_%7B3%7D%7Cy)"/><p class="new">So, given that the new email is composed by the features:  </p><p class="new"><code>W = ['dear', 'friend','click', 'here', 'and', 'win', 'a', 'huge', 'prize']</code></p><p class="new">The probability of this email being spam is:</p><img alt="math?math=%5Clarge%20P(spam%20%7C%20W)%20%3D%20P(spam)%20%5Ctimes%20P('dear'%7Cspam)%20%5Ctimes%20P('friend'%7Cspam)%20%5Ctimes%20P('click'%7Cspam)%20..." class="jax" src="https://render.githubusercontent.com/render/math?math=%5Clarge%20P(spam%20%7C%20W)%20%3D%20P(spam)%20%5Ctimes%20P('dear'%7Cspam)%20%5Ctimes%20P('friend'%7Cspam)%20%5Ctimes%20P('click'%7Cspam)%20..."/><br/><img alt="math?math=%5Clarge%20P(spam%20%7C%20W)%20%3D%200.333%20%5Ctimes%20%5Cfrac%7B0%7D%7B150%7D%20%5Ctimes%20%5Cfrac%7B0%7D%7B150%7D%20%5Ctimes%20%5Cfrac%7B3%7D%7B150%7D%20%5Ctimes%20%5Cfrac%7B2%7D%7B150%7D%20%5Ctimes%20%5Cfrac%7B3%7D%7B150%7D%20%5Ctimes%20%5Cfrac%7B4%7D%7B150%7D%20%5Ctimes%20%5Cfrac%7B4%7D%7B150%7D%20%5Ctimes%20%5Cfrac%7B2%7D%7B150%7D%20%5Ctimes%20%5Cfrac%7B2%7D%7B150%7D%20%3D%200" class="jax" src="https://render.githubusercontent.com/render/math?math=%5Clarge%20P(spam%20%7C%20W)%20%3D%200.333%20%5Ctimes%20%5Cfrac%7B0%7D%7B150%7D%20%5Ctimes%20%5Cfrac%7B0%7D%7B150%7D%20%5Ctimes%20%5Cfrac%7B3%7D%7B150%7D%20%5Ctimes%20%5Cfrac%7B2%7D%7B150%7D%20%5Ctimes%20%5Cfrac%7B3%7D%7B150%7D%20%5Ctimes%20%5Cfrac%7B4%7D%7B150%7D%20%5Ctimes%20%5Cfrac%7B4%7D%7B150%7D%20%5Ctimes%20%5Cfrac%7B2%7D%7B150%7D%20%5Ctimes%20%5Cfrac%7B2%7D%7B150%7D%20%3D%200"/><br/><img alt="math?math=%5Clarge%20P(normal%20%7C%20W)%20%3D%20P(normal)%20%5Ctimes%20P('dear'%7Cnormal)%20%5Ctimes%20P('friend'%7Cnormal)..." class="jax" src="https://render.githubusercontent.com/render/math?math=%5Clarge%20P(normal%20%7C%20W)%20%3D%20P(normal)%20%5Ctimes%20P('dear'%7Cnormal)%20%5Ctimes%20P('friend'%7Cnormal)..."/><br/><img alt="math?math=%5Clarge%20P(normal%20%7C%20W)%20%3D%200.666%20%5Ctimes%20%5Cfrac%7B4%7D%7B350%7D%20%5Ctimes%20%5Cfrac%7B3%7D%7B350%7D%20%5Ctimes%20%5Cfrac%7B1%7D%7B350%7D%20%5Ctimes%20%5Cfrac%7B2%7D%7B350%7D%20%5Ctimes%20%5Cfrac%7B3%7D%7B350%7D%20%5Ctimes%20%5Cfrac%7B0%7D%7B350%7D%20%5Ctimes%20%5Cfrac%7B4%7D%7B350%7D%20%5Ctimes%20%5Cfrac%7B2%7D%7B350%7D%20%5Ctimes%20%5Cfrac%7B1%7D%7B150%7D%20%3D%200" class="jax" src="https://render.githubusercontent.com/render/math?math=%5Clarge%20P(normal%20%7C%20W)%20%3D%200.666%20%5Ctimes%20%5Cfrac%7B4%7D%7B350%7D%20%5Ctimes%20%5Cfrac%7B3%7D%7B350%7D%20%5Ctimes%20%5Cfrac%7B1%7D%7B350%7D%20%5Ctimes%20%5Cfrac%7B2%7D%7B350%7D%20%5Ctimes%20%5Cfrac%7B3%7D%7B350%7D%20%5Ctimes%20%5Cfrac%7B0%7D%7B350%7D%20%5Ctimes%20%5Cfrac%7B4%7D%7B350%7D%20%5Ctimes%20%5Cfrac%7B2%7D%7B350%7D%20%5Ctimes%20%5Cfrac%7B1%7D%7B150%7D%20%3D%200"/><br/><p class="new">Maybe you noticed that this is not insightfull at all, what happened? 
Well, as our new email contains words that haven't been counted in some of the 
two classes, the probability is multiplied by 0, and it ended up being 0 for 
both clases. </p><p class="new">There are two things that we need to implement, a <em>smoothing method</em> in the counts 
and then use the logarithm of each value.</p><p class="new">First, the <strong>smoothing priors</strong>
<img alt="math?math=%5CLarge%20%5Calpha%20%5Cgt%200" class="jax" src="https://render.githubusercontent.com/render/math?math=%5CLarge%20%5Calpha%20%5Cgt%200" style="display:inline-block"/> accounts for features not present in the learning 
samples and prevents zero probabilities in further computations. Setting <img alt="math?math=%5Clarge%20%5Calpha%20%3D%201" class="jax" src="https://render.githubusercontent.com/render/math?math=%5Clarge%20%5Calpha%20%3D%201" style="display:inline-block"/> is called 
Laplace smoothing, while <img alt="math?math=%5Clarge%20P(X%7Cy)" class="jax" src="https://render.githubusercontent.com/render/math?math=%5Clarge%20P(X%7Cy)" style="display:inline-block"/> are estimated by a smoothed version of maximum likelihood (the relative 
frequency counting):</p><img alt="math?math=%5Clarge%20P(x_%7B1%7D%7Cy)%20%3D%20%5Cfrac%7BN_%7Byi%7D%2B%5Calpha%7D%7BN_%7By%7D%2B%5Calpha%20n%7D" class="jax" src="https://render.githubusercontent.com/render/math?math=%5Clarge%20P(x_%7B1%7D%7Cy)%20%3D%20%5Cfrac%7BN_%7Byi%7D%2B%5Calpha%7D%7BN_%7By%7D%2B%5Calpha%20n%7D"/><p class="new">where <img alt="math?math=%5Clarge%20n" class="jax" src="https://render.githubusercontent.com/render/math?math=%5Clarge%20n" style="display:inline-block"/> is the number of features in X.</p><p class="new">Then, we calculate the logarithm for each part of the bayes theorem, changing the multiplication to a sum. 
This not only helps us to avoid multiplying by 0, but also make the computation way simpler. </p><img alt="math?math=%5Clarge%20P(y%7CX)%20%3D%20log(P(y))%20%2B%20log(P(x_%7B1%7D%7Cy))%20%2B%20log(P(x_%7B2%7D%7Cy))%20%2B%20log(P(x_%7B3%7D%7Cy))%20..." class="jax" src="https://render.githubusercontent.com/render/math?math=%5Clarge%20P(y%7CX)%20%3D%20log(P(y))%20%2B%20log(P(x_%7B1%7D%7Cy))%20%2B%20log(P(x_%7B2%7D%7Cy))%20%2B%20log(P(x_%7B3%7D%7Cy))%20..."/><p class="new">Please, calculate again the probabilities of P(spam|W) and P(normal|W) including these two new considerations. </p><p class="new"><strong><em>Tip:</em></strong> <em>You can use Numpy for this. Don't forget to use a smothing prior</em></p><div class="ide code-starter blink"><pre><code>import numpy as np
spam_prior = 0.333
normal_prior = 0.666

total_words_in_spam = 150
total_words_in_normal = 350

laplace_smoothing = 1

#For the words: 'dear', 'friend','click', 'here', 'and', 'win', 'a', 'huge', 'prise'
freq_words_in_spam = np.array([0, 0, 3, 2, 3, 4, 4, 2, 2]) 
freq_words_in_normal = np.array([4, 3, 1, 2, 3, 0, 4, 2, 1])


p_spam_given_words = None
p_normal_given_words = None

print("The predicted email class, given its words is:", 'Normal' if p_normal_given_words &amp;gt; p_spam_given_words else 'Spam ')</code></pre></div><h3 id="2-scikit-learn-for-naive-bayes-">2. Scikit-learn for Naive Bayes </h3><p class="new">Now that you understand the logic behind the algorithm, we can do some
large-scale analysis. For this, we are going to use Scikit-learn. <strong>Scikit-learn</strong>
is a python module with tools to performe machine learning operations, like class
prediction based on <a href="https://scikit-learn.org/stable/modules/naive_bayes.html#naive-bayes" target="_blank">Naive Bayes</a>. </p><p class="new">As any supervised machine learning method, Scikit-learn for Naive Bayes, need as input different datasets to train and test its classifier. </p><h3 id="training-set">Training set</h3><p class="new">The training set should be composed by a list of observations <em>X_train</em> (like the emails from our previous example), 
and a list of labels with the pre-assigned class for each observation <em>y_train</em>. It is important to highlight 
that the observations need to be structured based on their common freatures in order to fit a model made in 
Scikit-learn.</p><p class="new">Therefore, to structure the training set observations for our spam example, we would need to use a entire 
dictionary of words to <em>encode</em> every document. Then, instead of having a list of words for each email, we 
will need a vector with the lenght of the whole dictionaty, that accounts the frequency of each word in the 
specific email. For example, a email's vectore have in position 0 the count of how many times it presents 
the word 'win', position 1 will account for the word 'prise', and so on. This means that the vectors are filled 
mostly with zeros, but that is fine. </p><p class="new">Then, insted of having this:</p><pre><code>X_train = [['click', 'here', 'and', 'win', 'a', 'huge', 'prise'],
            ['dear', 'friend', 'i', 'miss', 'you'],  
            ['i', 'can','meet', 'next', 'week', 'i', 'need', 'an', 'hour'],
            ... ]</code></pre><p class="new">We need something like this:</p><pre><code>X_train = [[1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, ...],
          [0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, ...],
          [0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 2, 0, 0, ...],
          ...]</code></pre><p class="new">The X_train is then a matrix with a row for each observation, and a column for each feature, in this case, 
each possible word from a dictionary. </p><p class="new">Then, the dictionary will be a list of words, where the positions of each word in the array is the same for the 
encoded vector, like this:</p><pre><code>d = ['win', 'prise', 'money', ' dear', ...]</code></pre><p class="new">Moreover, we have to make something similar for the lables, representing normal emails with 0s, and spam 
emails with 1s. Then, the training lables should look something like this:</p><pre><code>y_train = [1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, ...]</code></pre><p class="new">y_train should be a vector with the length of the number of observations in your training set. </p><h3 id="3-the-multinomial-naive-bayes">3. The Multinomial Naive Bayes</h3><p class="new">Sklearn.naive_bayes provides different variants of the naive bayes, based of the nature of the data you are 
working with. <a href="https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.MultinomialNB.html#sklearn.naive_bayes.MultinomialNB" target="_blank">The Multinomial Naive Bayes</a> 
is one of the two classic naive Bayes variants used in text classification (where the data are typically 
represented as word vector counts, although tf-idf vectors are also known to work well in practice). </p><p class="new">To use MultinomialNB, we can import it from the scikit-learn like this:</p><pre><code>from sklearn.naive_bayes import MultinomialNB</code></pre><p class="new">Then, assuming that our training set is in good shape, we could used X<em>train and y</em>train to train our classifier like this:</p><pre><code>clf = MultinomialNB()
clf.fit(X_train, y_train)
MultinomialNB()</code></pre><p class="new">The method <code>fit( )</code> receives the training set's elements, and by calling the <code>MultinomialNB()</code>, the model is 
trained. You can change the smoothing method, and the prior used by specifying it insider of the 
MultinomialNB method. By default, the method has the values <code>MultinomialNB(alpha = 1, fit_prior = True, class_prior= None)</code>, 
where the fit_prior paramether allow the model to calculate the prior based on the training data if set to True. 
When False, you can add a different prior in the paramether class_prior, by creating an array with a 
customized proportions for each lable.</p><p class="new">Now that you have your model, you could predict the class of an email with a list of words 'x'. Remember that you 
need to <em>encode</em> it with the dictionary that you created. If x_test is the encoded version of the new email you 
could use <code>predict()</code>  to get the predicted class of it.</p><pre><code>predicted_lable = clf.predict([x_test])</code></pre><p class="new">Notice that the predict method receives a list of observations. Then, if you want to predict the class of 
only one encoded email, you will need to pass it into a list.</p><h3 id="4-gaussian-naive-bayes-classifier">4. Gaussian Naive Bayes Classifier</h3><p class="new">For our previous example, we used words as features. Finding the frequency of a value is way easier when that 
value is discrete. But when we think about features with continous values, like a person's high or weight, 
calculating its likelihood by counting how many times another person had its exact high or weight makes no 
sense anymore. </p><p class="new">Therefore, for continuous features, the <a href="https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.GaussianNB.html#sklearn.naive_bayes.GaussianNB" target="_blank">Gaussian Naive Bayes Classifier</a> 
uses new assumption: The distribution of the features' values are gaussian. What does this mean again? 
Well, this basically means that the algorithm assumes that the data from each feature in a class will follow a 
normal distribution. And the likelihood of finding a feature value given that it belongs to a class y can be 
calculated by:</p><img alt="math?math=%5Clarge%20%5Cbegin%7Baligned%7D%20P(x_i%7Cy)%20%3D%20%5Cfrac%7B1%7D%7B%5Csqrt%7B2%5Cpi%5Csigma%5E2_y%7D%7Dexp%20(-%5Cfrac%7B(x_i%20-%20%5Cmu_y)%5E2%7D%7B2%5Csigma%5E2_y%7D)%20%5Cend%7Baligned%7D" class="jax" src="https://render.githubusercontent.com/render/math?math=%5Clarge%20%5Cbegin%7Baligned%7D%20P(x_i%7Cy)%20%3D%20%5Cfrac%7B1%7D%7B%5Csqrt%7B2%5Cpi%5Csigma%5E2_y%7D%7Dexp%20(-%5Cfrac%7B(x_i%20-%20%5Cmu_y)%5E2%7D%7B2%5Csigma%5E2_y%7D)%20%5Cend%7Baligned%7D"/><p class="new">Luckily this calculation is automatically made by the <strong>GaussianNB</strong> model from Scikit-learn. But you should 
have an idea of how this calculation is made. Therefore, I invite you to think about flowers, specifically 
about the Iris. </p><p class="new">An Iris can be classify in one of three groups inside the Iris Family: </p><table><thead><tr><th style="text-align:left">Class</th><th style="text-align:center">Iris-Setosa</th><th style="text-align:center">Iris-Versicolor</th><th style="text-align:center">Iris-Virginica</th></tr></thead><tbody><tr><td style="text-align:left">Example</td><td style="text-align:center"><img alt="iris_baby_blue.jpg?1500653527" class="med" src="https://github.com/NSF-EC/INFO490Assets/raw/master/src/dmap/lessons/naive-bayes/html/a2.jpg"/></td><td style="text-align:center"><img alt="IRIS_VERSICOLOR.JPG?1495391088" class="med" src="https://github.com/NSF-EC/INFO490Assets/raw/master/src/dmap/lessons/naive-bayes/html/a3.png"/></td><td style="text-align:center"><img alt="8050.jpg" class="med max-width" src="https://github.com/NSF-EC/INFO490Assets/raw/master/src/dmap/lessons/naive-bayes/html/8050.jpg"/></td></tr><tr><td style="text-align:left">Ave. sepal length</td><td style="text-align:center">4.3</td><td style="text-align:center">7.9</td><td style="text-align:center">5.84</td></tr><tr><td style="text-align:left">Ave. sepal width</td><td style="text-align:center">2.0</td><td style="text-align:center">4.4</td><td style="text-align:center">3.05</td></tr><tr><td style="text-align:left">Ave. petal length</td><td style="text-align:center">1.0</td><td style="text-align:center">6.9</td><td style="text-align:center">3.76</td></tr><tr><td style="text-align:left">Ave. petal width</td><td style="text-align:center">0.1</td><td style="text-align:center">2.5</td><td style="text-align:center">1.20</td></tr></tbody></table><p class="new">The averages from the table are taken from a dataset that we can be accessed through the Scikit-learn <a href="https://scikit-learn.org/stable/datasets/index.html" target="_blank">Iris Dataset</a></p><pre><code>from sklearn.datasets import load_iris</code></pre><p class="new">Now imagine that you want to classify an Iris, that has a pethal lenght of <img alt="math?math=%5Clarge%20x_i%20%3D%205%20cm" class="jax" src="https://render.githubusercontent.com/render/math?math=%5Clarge%20x_i%20%3D%205%20cm" style="display:inline-block"/></p><p class="new">If we graph a normal distribution of the Pethal lenght per class, and look for the interesection of each 
distribution with the value of 5 cm, we will get the likelihood of getting that value, given that the your 
new iris belongs to each class.</p><p class="new"><img alt="Gaussian Likelihood" src="https://github.com/NSF-EC/INFO490Assets/raw/master/src/dmap/lessons/naive-bayes/html/a4.png"/></p><p class="new">That is basically what happens for each feature in order to classify the new Iris. </p><p class="new">With the exception of this, the general formula to calculate the probability of having a specific iris 
given the flower features remains the same:</p><img alt="math?math=%5Clarge%20P(y%7CX)%20%3D%20log(P(y))%20%2B%20log(P(x_%7B1%7D%7Cy))%20%2B%20log(P(x_%7B2%7D%7Cy))%20%2B%20log(P(x_%7B3%7D%7Cy))%20..." class="jax" src="https://render.githubusercontent.com/render/math?math=%5Clarge%20P(y%7CX)%20%3D%20log(P(y))%20%2B%20log(P(x_%7B1%7D%7Cy))%20%2B%20log(P(x_%7B2%7D%7Cy))%20%2B%20log(P(x_%7B3%7D%7Cy))%20..."/><p class="new">It also used the logarithm to make the processing lighter and also to avoid errors produced by likelihoods 
too closed to 0 (Like the one for the Iris-Setosa class in the previous example). Moreover, the gaussian 
classifier also uses a smoothing variable, with a default value way smaller than the Laplace 
smotther (<img alt="math?math=%5Clarge%20%5Calpha%20%3D%201e%5E%7B-9%7D" class="jax" src="https://render.githubusercontent.com/render/math?math=%5Clarge%20%5Calpha%20%3D%201e%5E%7B-9%7D" style="display:inline-block"/>).</p><p class="new">Let's explore a little more of the scikit-lern and the Iris dataset. We can import and load it 
calling the load_iris method from the library. </p><pre><code>from sklearn.datasets import load_iris
X, y = load_iris(return_X_y=True)
print(X.shape)</code></pre><p class="new">By printing the the shape of X, you will be able to know how many observations are listed in the dataset, 
and how many features each observation has.</p><p class="new">Sklearn also included a tool to split a dataset between training and testing data.</p><pre><code>from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5)</code></pre><p class="new">The <code>test_size</code> paramether in <code>train_test_split()</code>receives the proportion that we want to have for 
the test dataset, while the rest will be used to train the classifier.</p><p class="new">By doing that, you can test the precision of your GaussianNB classifier:</p><pre><code>from sklearn.naive_bayes import GaussianNB

gnb = GaussianNB()
y_pred = gnb.fit(X_train, y_train).predict(X_test)</code></pre><p class="new">Now, test the library yourself:</p><div class="ide code-starter blink"><pre><code># Use this print to see the efectivity of your classifier
print("Number of mislabeled points out of a total %d points : %d"
... % (X_test.shape[0], (y_test != y_pred).sum()))</code></pre></div><hr/><h3 id="5-assignment">5. Assignment</h3><p class="new">We exemplified how a Multinomial NB classifier could be set to classify spam emails based only on their content. 
But, it is common to use some metadata as features to classify this kind of text. </p><p class="new">For this lesson's assignment, you are going to use the dataset
<a href="https://github.com/jbryer/DATA606Spring2020/blob/master/course_data/os3_data/email.csv" target="_blank"><strong><em>email.csv</em></strong></a>.</p><p class="new">This dataset includes the data for 3921 emails and 20 features for each. 
The class label column contains a numeric label -- 0 for a normal email, and 1 for spam.</p><ol start="1"><li>In the <code>load_data(path)</code> method, open and read the CSV file. Use Pandas library to return a dataframe with the data.</li></ol><div class="ide code-starter blink"><pre><code>import pandas as pd 
def load_data(data_path):
    df = None
    #Load data
    return df
    
path = ide.reader.path_for_data('email.csv')
df = load_data(path)</code></pre></div><br/><ol start="2"><li>Explore the data frame. Identify the column with the labels, and create a method selectLabels(df) that takes a dataframe, and returns the column with the label only.</li></ol><div class="ide code-starter blink"><pre><code>def selectLabels(df):
    y = None
    return y</code></pre></div><ol start="3"><li>Now, explore the resto of the dataframe and select only 5 features to consider in your Multinomial classifier. Create a method <code>selectAttributes(df)</code> that receive a dataframe, and return a new dataframe with only 5 columns.</li></ol><div class="ide code-starter blink"><pre><code>def selectAttributes(df):
    X = None
    return X</code></pre></div><ol start="4"><li>With the method <code>prepareData(X, y)</code> Split your labels and features in a training and testing set with a proportion of 0.333 for the testing. Your method should return a tupple with four variables (X<em>train, X</em>test, y<em>train, y</em>test).</li></ol><div class="ide code-starter blink"><pre><code>from sklearn.model_selection import train_test_split
def prepareData(X, y):
    # In train_test_split method, use random_state = 44 for testing purposes
    return (None, None, None, None)</code></pre></div><ol start="5"><li>Train a MultinomialNB classifier with your training data in <code>trainClassifier(X_train, y_train)</code> and return the classifier.</li></ol><div class="ide code-starter blink"><pre><code>from sklearn.naive_bayes import MultinomialNB
def trainClassifier(X_train, y_train):
    clf = None
    return clf</code></pre></div><ol start="6"><li>Create the method  <code>classifyEmails(clf, X_test)</code> that receives the classifier and a testing set, and return a set of predicted lables.</li></ol><div class="ide code-starter blink"><pre><code>def classifyEmails(clf, X_test):
    y_pred = None
    return y_pred</code></pre></div><p class="new">You should experiment with different variables as features until you get <strong>no more than 127 mislabeled emails</strong> from your testing set of 1306. You can use the following code to test you classifier:</p><div class="ide code-starter blink"><pre><code>path = ide.reader.path_for_data('email.csv')
emailsData = load_data(path)

y = selectLabels(emailsData)
X = selectAttributes(emailsData)

X_train, X_test, y_train, y_test = prepareData(X, y)

clf = trainClassifier(X_train, y_train)

y_pred = classifyEmails(clf, X_test)

print("Number of mislabeled points out of a total %d points : %d" % (X_test.shape[0], (y_test != y_pred).sum()))</code></pre></div><p class="new"><strong>Tips:</strong></p><ul><li>You can check <a href="https://www.rdocumentation.org/packages/openintro/versions/1.7.1/topics/email" target="_blank">this</a> website 
for a better explanation the dataset's variables (Yes, this dataset was originally designed to learn 
and practice R, but we are all the data community)</li><li>If you want to use features like <em>Winner</em> or <em>Number</em>, which have categorical data, remember that you need 
to map these categories to numeric values. You can use <code>map()</code> for this.</li><li>If you really want to use the email's <em>Time</em> as a feature, you probably have to come up with a more 
sophisticated mapping method.</li></ul><p class="new"><strong>Note:</strong> Different combinations of features might work better than others. Some variables together might provide redundant or opposite information that compromises the quality of your model. One of the most important decisions to make when developing machine learning methods is to identify variables that provide valuable information to the model.</p><h1>Test and Submit</h1><p>Once done you can download your code (via <code>ide.tester</code>) and upload that file to Gradescope:</p><div class=""><pre><code><strong># to test your code (either works)</strong><br/>print(ide.tester.test_notebook())<br/>print(ide.tester.test_notebook(verbose=True))<br/><strong># to prepare and download your code</strong><br/>ide.tester.download_solution()</code></pre></div><div class="lesson-footer flex bg-gray-200 justify-center"><div class="displaycard border-t border-gray-400 max-w-2xl rounded overflow-hidden shadow-lg"><div class="px-6 py-4"><div class="text-center font-bold text-xl">Naive Bayes</div><p class="text-center text-gray-800 text-xl">Event Probability</p><div class="text-center mt-6 text-xl"><i aria-hidden="true" class="fas fa-tags"></i> any questions on Piazza with <span class="font-bold">naive-bayes.md</span></div><div class="text-gray-700 text-base">¬†</div><div></div><div class="flex mt-4 border-t border-solid border-gray-500 justify-around bg-gray-200"><div class="text-gray-700 text-center px-4 m-2 text-sm">D.MÔπ†üêç</div><div class="text-gray-700 text-center px-4 m-2 text-sm"><strong>Version:</strong> <!-- -->Fall 2020<!-- --> <!-- -->08.20.20</div></div><div class="text-gray-700 text-center text-tiny">All Rights Reserved</div><div class="text-gray-700 text-center text-tiny">Do not distribute this notebook outside of the class</div></div></div></div><div>¬†</div><div class="ide code-starter blink"><pre><code># print(ide.tester.test_notebook()) 
# print(ide.tester.test_notebook(verbose=True)) 

ide.tester.download_solution()</code></pre></div></div></div></body></html>