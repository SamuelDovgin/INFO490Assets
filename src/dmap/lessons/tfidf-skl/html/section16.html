<!DOCTYPE html><html lang='en'><head><title>TF•IDF (part 2)</title><meta charset="utf-8"><style>/*! normalize.css v8.0.1 | MIT License | github.com/necolas/normalize.css */html{line-height:1.15;-webkit-text-size-adjust:100%}body{margin:0}h1{font-size:2em;margin:.67em 0}pre{font-family:monospace,monospace;font-size:1em}a{background-color:transparent}strong{font-weight:bolder}code{font-family:monospace,monospace;font-size:1em}img{border-style:none}::-webkit-file-upload-button{-webkit-appearance:button;font:inherit}blockquote,h1,h2,h3,p,pre{margin:0}html{font-family:system-ui,-apple-system,BlinkMacSystemFont,"Segoe UI",Roboto,"Helvetica Neue",Arial,"Noto Sans",sans-serif,"Apple Color Emoji","Segoe UI Emoji","Segoe UI Symbol","Noto Color Emoji";line-height:1.5}*,::after,::before{box-sizing:border-box;border-width:0;border-style:solid;border-color:#e2e8f0}img{border-style:solid}h1,h2,h3{font-size:inherit;font-weight:inherit}a{color:inherit;text-decoration:inherit}code,pre{font-family:Menlo,Monaco,Consolas,"Liberation Mono","Courier New",monospace}img{display:block;vertical-align:middle}img{max-width:100%;height:auto}.bg-gray-200{--bg-opacity:1;background-color:#edf2f7;background-color:rgba(237,242,247,var(--bg-opacity))}.bg-gray-300{--bg-opacity:1;background-color:#e2e8f0;background-color:rgba(226,232,240,var(--bg-opacity))}.border-gray-400{--border-opacity:1;border-color:#cbd5e0;border-color:rgba(203,213,224,var(--border-opacity))}.border-gray-500{--border-opacity:1;border-color:#a0aec0;border-color:rgba(160,174,192,var(--border-opacity))}.border-green-500{--border-opacity:1;border-color:#48bb78;border-color:rgba(72,187,120,var(--border-opacity))}.rounded{border-radius:.25rem}.rounded-full{border-radius:9999px}.border-solid{border-style:solid}.border-2{border-width:2px}.border{border-width:1px}.border-t{border-top-width:1px}.inline-block{display:inline-block}.flex{display:flex}.justify-center{justify-content:center}.justify-around{justify-content:space-around}.float-left{float:left}.clearfix:after{content:"";display:table;clear:both}.clear-both{clear:both}.font-serif{font-family:Georgia,Cambria,"Times New Roman",Times,serif}.font-semibold{font-weight:600}.font-bold{font-weight:700}.h-64{height:16rem}.text-sm{font-size:.875rem}.text-base{font-size:1rem}.text-xl{font-size:1.25rem}.m-2{margin:.5rem}.mt-0{margin-top:0}.mt-2{margin-top:.5rem}.mr-2{margin-right:.5rem}.mb-2{margin-bottom:.5rem}.mr-3{margin-right:.75rem}.mb-3{margin-bottom:.75rem}.ml-3{margin-left:.75rem}.mt-4{margin-top:1rem}.mt-6{margin-top:1.5rem}.max-w-sm{max-width:24rem}.max-w-2xl{max-width:42rem}.object-contain{-o-object-fit:contain;object-fit:contain}.overflow-hidden{overflow:hidden}.p-1{padding:.25rem}.p-2{padding:.5rem}.py-1{padding-top:.25rem;padding-bottom:.25rem}.px-3{padding-left:.75rem;padding-right:.75rem}.py-4{padding-top:1rem;padding-bottom:1rem}.px-4{padding-left:1rem;padding-right:1rem}.px-6{padding-left:1.5rem;padding-right:1.5rem}.pl-3{padding-left:.75rem}.shadow-lg{box-shadow:0 10px 15px -3px rgba(0,0,0,.1),0 4px 6px -2px rgba(0,0,0,.05)}.text-center{text-align:center}.text-black{--text-opacity:1;color:#000;color:rgba(0,0,0,var(--text-opacity))}.text-gray-700{--text-opacity:1;color:#4a5568;color:rgba(74,85,104,var(--text-opacity))}.text-gray-800{--text-opacity:1;color:#2d3748;color:rgba(45,55,72,var(--text-opacity))}.whitespace-no-wrap{white-space:nowrap}.w-full{width:100%}.text-tiny{font-size:.5rem!important}body{color:#000!important;font-size:1.25rem!important}h1,h2,h3{color:#000!important}.lesson{padding-left:10px!important;padding-right:10px!important;--bg-opacity:1;background-color:#edf2f7;background-color:rgba(237,242,247,var(--bg-opacity))}.lesson-footer{margin-top:50px;margin-top:20px}span{white-space:nowrap}p.new{text-indent:1.5em;padding-top:0;padding-bottom:.5em}.h-18rem{height:17rem}h1,h2,h3{font-weight:700;margin-bottom:.25em;font-family:Georgia,Cambria,"Times New Roman",Times,serif!important}h1{margin-top:.5em;font-size:2em!important;clear:both}h2{margin-top:1em;font-size:1.5em!important;clear:both}h3{margin-top:.5em;font-size:1.25em!important}p.new a{text-decoration:underline}.lesson a{text-decoration:underline}blockquote{font-size:1em;background:#f9f9f9;border-left:10px solid #ccc;margin:.5em 10px;padding:.5em 10px;border-left-color:#ffcd69;border-right-color:#f6ba59;quotes:"\201C""\201D""\2018""\2019"}blockquote:before{color:#ccc;content:open-quote;font-size:4em;line-height:.1em;margin-right:.25em;vertical-align:-.4em}blockquote p{display:inline}img.wrap{margin-right:1rem!important;border:1px solid #021a40}img.border{border:1px solid #021a40}img:not([class]){width:66.666667%;margin-left:1.25rem;border:1px solid #021a40}code{font-size:smaller}pre code{font-size:15px}pre code:not(.line-number){background:#f4f4f4;font-family:monospace;font-size:15px;-webkit-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none;cursor:default;touch-action:none;-webkit-touch-callout:none;-webkit-tap-highlight-color:transparent;clear:both;border:1px solid #ddd;color:#666;page-break-inside:avoid;display:block;min-width:740px;max-width:740px;line-height:1.6;margin-bottom:1.6em;padding:1em 1.5em;-moz-tab-size:2;-o-tab-size:2;tab-size:2;overflow-x:auto;word-wrap:break-word;white-space:pre-wrap;border-left:3px solid #f36d33}div.code-starter>pre code{border-left:3px solid #fdff44!important;background-image:radial-gradient(rgba(0,150,0,.75),#000 120%);color:#fff;font:.9rem Inconsolata,monospace}div.code-starter>pre code::after{content:"\a$_"}pre{counter-reset:line}</style><script src="https://kit.fontawesome.com/7efc4bcee2.js" crossOrigin="anonymous"></script></head><body class="lesson"><div class="main-content bg-gray-200 text-black p-1 pl-3 font-serif"><div class="md-inner">
<h1 class="section" id="section4">The Sparse Matrix</h1><p class="new">Perhaps you wondering exactly is the thing/object/data returned from either <code>transform</code> or <code>fit_transform</code>?</p><pre><code>tf_idf_vector = tfidf_transformer.transform(doc_term_matrix)</code></pre><p class="new">It is actually a <em>sparse</em> matrix <a href="https://docs.scipy.org/doc/scipy/reference/sparse.html" target="_blank">sklearn doc</a>. 
As you can imagine, as the set of documents become larger, there will a large set of words that only belong
to a small set of documents.  The tf•idf matrix will contain mostly zeros:
<img alt="sparse_doc.png" class="float-left wrap sm" src="https://github.com/NSF-EC/INFO490Assets/raw/master/src/dmap/lessons/tfidf-skl/html/sparse_doc.png"/></p><p class="new">The sparse matrix efficiently stores the necessary data.  Many of sklearn's algorithms can work with a sparse matrix
as well as regular Numpy vectors.  However, when you use libraries like Pandas, you will have to convert these
sparse matrices into regular Numpy arrays:</p><pre><code>df = pd.DataFrame(data=wm.toarray(), index=doc_names, columns=feature_names)</code></pre><blockquote><p class="new"><em><strong>Coder's Log</strong></em>
As you become more familiar with using sparse matrices, you may come across references to indptr, data, indices. 
The image below helps depict sklearn's implementation:
<img alt="sparse_matrix.png" class="clear-both" src="https://github.com/NSF-EC/INFO490Assets/raw/master/src/dmap/lessons/tfidf-skl/html/sparse_matrix.png"/><br class="clear-both"/></p></blockquote><h2 id="gensim">Gensim</h2><p class="new">Another popular library for working with text is <a href="https://radimrehurek.com/gensim/" target="_blank">gensim</a>.  It also
has TF•IDF implementations as well.  Although we will not be using this library now, it's important to know
that it is an option:</p><pre><code>corpus  = [tokenize(doc) for doc in corpus]
lexicon = gensim.corpora.Dictionary(corpus)
tfidf   = gensim.models.TfidfModel(dictionary=lexicon, normalize=True)
vectors = [tfidf[lexicon.doc2bow(doc)] for doc in corpus]</code></pre><h2 id="clustering-and-distance-document-similarity">Clustering and Distance Document Similarity</h2><p class="new">Now that we have a matrix where the rows are documents (document vectors) of normalized TF•IDF values
we can "easily" compare two documents and see how similar (or different they are).</p><p class="new">The distance between two document vectors is the essence behind using TF•IDF for document 
clustering (an unsupervised machine learning algorithm), as well as trying to retrieved relevant 
documents by search terms.  Your search terms become a 'document' and gets converted to a vector.
Distances are then calculated for each document to the query/search vector.</p><p class="new">Much of this will be covered in a separate lesson on distance metrics. There's a lot more to cover.</p></div></div></body></html>