<!DOCTYPE html><html lang='en'><head><title>TF•IDF (part 2)</title><meta charset="utf-8"><style>/*! normalize.css v8.0.1 | MIT License | github.com/necolas/normalize.css */html{line-height:1.15;-webkit-text-size-adjust:100%}body{margin:0}h1{font-size:2em;margin:.67em 0}pre{font-family:monospace,monospace;font-size:1em}a{background-color:transparent}strong{font-weight:bolder}code{font-family:monospace,monospace;font-size:1em}img{border-style:none}::-webkit-file-upload-button{-webkit-appearance:button;font:inherit}h1,h2,h3,p,pre{margin:0}ol{list-style:none;margin:0;padding:0}html{font-family:system-ui,-apple-system,BlinkMacSystemFont,"Segoe UI",Roboto,"Helvetica Neue",Arial,"Noto Sans",sans-serif,"Apple Color Emoji","Segoe UI Emoji","Segoe UI Symbol","Noto Color Emoji";line-height:1.5}*,::after,::before{box-sizing:border-box;border-width:0;border-style:solid;border-color:#e2e8f0}img{border-style:solid}h1,h2,h3{font-size:inherit;font-weight:inherit}a{color:inherit;text-decoration:inherit}code,pre{font-family:Menlo,Monaco,Consolas,"Liberation Mono","Courier New",monospace}img{display:block;vertical-align:middle}img{max-width:100%;height:auto}.bg-gray-200{--bg-opacity:1;background-color:#edf2f7;background-color:rgba(237,242,247,var(--bg-opacity))}.border-green-500{--border-opacity:1;border-color:#48bb78;border-color:rgba(72,187,120,var(--border-opacity))}.border-2{border-width:2px}.float-left{float:left}.font-serif{font-family:Georgia,Cambria,"Times New Roman",Times,serif}.text-xl{font-size:1.25rem}.mt-0{margin-top:0}.mt-2{margin-top:.5rem}.mr-2{margin-right:.5rem}.mb-2{margin-bottom:.5rem}.mr-3{margin-right:.75rem}.max-w-sm{max-width:24rem}.max-w-2xl{max-width:42rem}.p-1{padding:.25rem}.p-3{padding:.75rem}.pl-3{padding-left:.75rem}.lesson{padding-left:10px;padding-right:10px;--bg-opacity:1;background-color:#edf2f7;background-color:rgba(237,242,247,var(--bg-opacity))}p.new{padding-top:.5em;padding-bottom:.5em}h1,h2,h3{font-weight:700;margin-bottom:.5em;font-family:Georgia,Cambria,"Times New Roman",Times,serif!important}h1{margin-top:1.5em;font-size:2em!important}h2{margin-top:1em;font-size:1.5em!important}h3{margin-top:.5em;font-size:1.25em!important}p.new a{text-decoration:underline}img:not([class]){width:66.666667%;margin-left:1.25rem;border:1px solid #021a40}pre code{font-size:15px}p code{font-size:smaller}.code-large{background:#f4f4f4;font-family:monospace;font-size:15px;margin:10px;font-size:15px}pre code:not(.line-number){background:#f4f4f4;font-family:monospace;font-size:15px;-webkit-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none;cursor:default;touch-action:none;-webkit-touch-callout:none;-webkit-tap-highlight-color:transparent;border:1px solid #ddd;color:#666;page-break-inside:avoid;display:block;min-width:740px;max-width:740px;line-height:1.6;margin-bottom:1.6em;padding:1em 1.5em;-moz-tab-size:2;-o-tab-size:2;tab-size:2;overflow-x:auto;word-wrap:break-word;white-space:pre-wrap;border-left:3px solid #f36d33}div.code-starter>pre code{border-left:3px solid #fdff44!important;background-image:radial-gradient(rgba(0,150,0,.75),#000 120%);color:#fff;font:.9rem Inconsolata,monospace}div.code-starter>pre code::after{content:"\a$_"}pre{counter-reset:line}
div.code-starter {display:none;}</style><script src="https://kit.fontawesome.com/7efc4bcee2.js" crossOrigin="anonymous"></script></head><body class="lesson"><div class="main-content bg-gray-200 p-1 pl-3 text-xl font-serif"><div class="md-inner"><div class="ide code-starter blink"><pre><code>def split_into_tokens(data, normalize=True, min_length=0):
    # copy in your solution from Part 1 (or re-implement it here)
    pass

def cv_demo2():

    corpus = get_corpus()

    # pass in our own tokenizer
    cvec = CountVectorizer(tokenizer=split_into_tokens)

    # convert the documents into a document-term matrix
    doc_term_matrix = cvec.fit_transform(corpus)

    # get the terms found in the corpus
    tokens = cvec.get_feature_names()
    
    return doc_term_matrix, tokens

dtm, tokens = cv_demo2()
print(tokens)
print(dtm.toarray())</code></pre></div><p class="new">Note we are passing in the same tokenizer you wrote (that had a default of <code>min_length=0</code>).  You can
re-implement it or just copy&amp;paste it from your previous tf•idf lesson.</p><p class="new">After you re-run the code, you should get the following output.</p><pre><code>['am', 'and', 'anywhere', 'do', 'eggs', 'green', 'ham', 'here', 'i', 'like', 'not', 'or', 'sam', 'them', 'there', 'would', 'you']
[[0 1 0 1 1 1 1 0 0 1 0 0 0 0 0 0 1]
 [1 1 0 2 1 1 1 0 3 2 2 0 1 1 0 0 0]
 [0 0 0 0 0 0 0 1 0 1 0 1 0 1 1 1 1]
 [0 0 1 0 0 0 0 1 2 2 2 1 0 2 1 2 0]]</code></pre><h2 id="visualize-with-pandas">'Visualize' with Pandas</h2><p class="new">We can use Pandas to help make this a bit easier to look at.  We are also going to reuse <code>cv_demo2</code> 
to allow us to focus on the new code. </p></div></div></body></html>