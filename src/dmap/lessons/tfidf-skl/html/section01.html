<!DOCTYPE html><html lang='en'><head><title>TF•IDF (part 2)</title><meta charset="utf-8"><style>/*! normalize.css v8.0.1 | MIT License | github.com/necolas/normalize.css */html{line-height:1.15;-webkit-text-size-adjust:100%}body{margin:0}h1{font-size:2em;margin:.67em 0}pre{font-family:monospace,monospace;font-size:1em}a{background-color:transparent}strong{font-weight:bolder}code{font-family:monospace,monospace;font-size:1em}img{border-style:none}::-webkit-file-upload-button{-webkit-appearance:button;font:inherit}h1,h2,h3,p,pre{margin:0}ol{list-style:none;margin:0;padding:0}html{font-family:system-ui,-apple-system,BlinkMacSystemFont,"Segoe UI",Roboto,"Helvetica Neue",Arial,"Noto Sans",sans-serif,"Apple Color Emoji","Segoe UI Emoji","Segoe UI Symbol","Noto Color Emoji";line-height:1.5}*,::after,::before{box-sizing:border-box;border-width:0;border-style:solid;border-color:#e2e8f0}img{border-style:solid}h1,h2,h3{font-size:inherit;font-weight:inherit}a{color:inherit;text-decoration:inherit}code,pre{font-family:Menlo,Monaco,Consolas,"Liberation Mono","Courier New",monospace}img{display:block;vertical-align:middle}img{max-width:100%;height:auto}.bg-gray-200{--bg-opacity:1;background-color:#edf2f7;background-color:rgba(237,242,247,var(--bg-opacity))}.border-green-500{--border-opacity:1;border-color:#48bb78;border-color:rgba(72,187,120,var(--border-opacity))}.border-2{border-width:2px}.float-left{float:left}.font-serif{font-family:Georgia,Cambria,"Times New Roman",Times,serif}.text-xl{font-size:1.25rem}.mt-0{margin-top:0}.mt-2{margin-top:.5rem}.mr-2{margin-right:.5rem}.mb-2{margin-bottom:.5rem}.mr-3{margin-right:.75rem}.max-w-sm{max-width:24rem}.max-w-2xl{max-width:42rem}.p-1{padding:.25rem}.p-3{padding:.75rem}.pl-3{padding-left:.75rem}.lesson{padding-left:10px;padding-right:10px;--bg-opacity:1;background-color:#edf2f7;background-color:rgba(237,242,247,var(--bg-opacity))}p.new{padding-top:.5em;padding-bottom:.5em}h1,h2,h3{font-weight:700;margin-bottom:.5em;font-family:Georgia,Cambria,"Times New Roman",Times,serif!important}h1{margin-top:1.5em;font-size:2em!important}h2{margin-top:1em;font-size:1.5em!important}h3{margin-top:.5em;font-size:1.25em!important}p.new a{text-decoration:underline}img:not([class]){width:66.666667%;margin-left:1.25rem;border:1px solid #021a40}pre code{font-size:15px}p code{font-size:smaller}.code-large{background:#f4f4f4;font-family:monospace;font-size:15px;margin:10px;font-size:15px}pre code:not(.line-number){background:#f4f4f4;font-family:monospace;font-size:15px;-webkit-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none;cursor:default;touch-action:none;-webkit-touch-callout:none;-webkit-tap-highlight-color:transparent;border:1px solid #ddd;color:#666;page-break-inside:avoid;display:block;min-width:740px;max-width:740px;line-height:1.6;margin-bottom:1.6em;padding:1em 1.5em;-moz-tab-size:2;-o-tab-size:2;tab-size:2;overflow-x:auto;word-wrap:break-word;white-space:pre-wrap;border-left:3px solid #f36d33}div.code-starter>pre code{border-left:3px solid #fdff44!important;background-image:radial-gradient(rgba(0,150,0,.75),#000 120%);color:#fff;font:.9rem Inconsolata,monospace}div.code-starter>pre code::after{content:"\a$_"}pre{counter-reset:line}
div.code-starter {display:none;}</style><script src="https://kit.fontawesome.com/7efc4bcee2.js" crossOrigin="anonymous"></script></head><body class="lesson"><div class="main-content bg-gray-200 p-1 pl-3 text-xl font-serif"><div class="md-inner"><h1 class="section" id="section1">TF•IDF (part 2)</h1><p class="new">This lesson continues our learning of TF•IDF, specifically how to use the powerful scikit-learn library to 
perform a TF•IDF analysis.  Not only will you learn how to use the library but also how it calculates TF and IDF.
Our goal for this lesson is to build a td•idf representation using scikit-learn.</p><h2 id="welcome-to-scikit-learn">Welcome to Scikit-learn</h2><img alt="scikit-learn-logo-small.png" class="sm mr-2 float-left max-w-2xl" src="https://github.com/NSF-EC/INFO490Assets/raw/master/src/dmap/lessons/tfidf-skl/html/scikit-learn-logo-small.png"/><div class="mt-0"><p class="new">The Python library scikit-learn (pronounced sy-kit -- as in science) is a set of modules 
(think Python classes) that can be used to build data analysis and machine learning software.
It is just one toolkit of a family (see <a href="https://scikits.appspot.com/scikits">https://scikits.appspot.com/scikits</a>). SciKit-learn
uses Numpy, SciPy and Matplotlib -- all libraries you worked with in INFO 490.  It's another
mountain to climb, but we will explore it slowly and try to see as much of it as possible.<br/>The Python package uses the prefix <code>sklearn</code>, so we may use either <code>sklearn</code>, sci-kit, or scikit-learn to reference the software. </p></div><h2 id="feature-vectors">Feature Vectors</h2><p class="new">Luckily, scikit-learn has a submodule that specializes in building feature vectors (a numerical
representation) for text documents.  When you use a collections.Counter to count occurrences of words,
you are building a simple feature vector. A feature vector contains information describing
an object's more important characteristics.   </p><img alt="featurevector.png" class="float-left mt-2 mr-3 sm border-2 border-green-500" src="https://github.com/NSF-EC/INFO490Assets/raw/master/src/dmap/lessons/tfidf-skl/html/featurevector.png"/><p class="new">A feature vector is almost always numerical and there is a mapping between the original item and it's numerical 
representation. For now you can think of a feature vector as a set of columns (attributes) for a 
row (an instance/observation). We will point out feature vectors throughout this class.  </p><h3 id="vectorization-">Vectorization </h3><p class="new">Scikit uses the word <em>vectorization</em> as the general process of turning a 
collection of text documents into numerical feature vectors.  We will start with
the <code>CountVectorizer</code> class that builds a fancy version of the <code>collections.Counter</code>.</p><h2 id="the-countvectorizer-class">The <code>CountVectorizer</code> Class</h2><img alt="green_eggs-300.png" class="float-left p-3 max-w-sm" src="https://github.com/NSF-EC/INFO490Assets/raw/master/src/dmap/lessons/tfidf-skl/html/green_eggs-300.png"/><p class="new">We will start our tour using a simple, reduced four chapter story. The function
<code>get_corpus</code> returns this data (and you won't have to type it in!).</p><pre><code>def get_corpus():
    c1 = "Do you like Green eggs and ham"
    c2 = "I do not like them Sam I am  I do not like Green eggs and ham"
    c3 = "Would you like them Here or there"
    c4 = "I would not like them Here or there I would not like them Anywhere"
    return [c1, c2, c3, c4]</code></pre><p class="new">Let's now see how we can use the <code>CountVectorizer</code> class</p><pre><code>from sklearn.feature_extraction.text import CountVectorizer

def cv_demo1():

    corpus = get_corpus()

    # normalize all the words to lowercase
    cvec = CountVectorizer(lowercase=True)

    # convert the documents into a document-term matrix
    doc_term_matrix = cvec.fit_transform(corpus)

    # get the terms found in the corpus
    print(cvec.get_feature_names())

    # get the counts
    print(doc_term_matrix.toarray())

cv_demo1()</code></pre><p class="new">Type in the above code, run it (fix any errors) </p></div></div></body></html>