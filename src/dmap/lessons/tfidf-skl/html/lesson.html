<!DOCTYPE html><html lang='en'><head><title>TF•IDF (part 2)</title><meta charset="utf-8"><style>/*! normalize.css v8.0.1 | MIT License | github.com/necolas/normalize.css */html{line-height:1.15;-webkit-text-size-adjust:100%}body{margin:0}h1{font-size:2em;margin:.67em 0}pre{font-family:monospace,monospace;font-size:1em}a{background-color:transparent}strong{font-weight:bolder}code{font-family:monospace,monospace;font-size:1em}img{border-style:none}::-webkit-file-upload-button{-webkit-appearance:button;font:inherit}h1,h2,h3,p,pre{margin:0}ol{list-style:none;margin:0;padding:0}html{font-family:system-ui,-apple-system,BlinkMacSystemFont,"Segoe UI",Roboto,"Helvetica Neue",Arial,"Noto Sans",sans-serif,"Apple Color Emoji","Segoe UI Emoji","Segoe UI Symbol","Noto Color Emoji";line-height:1.5}*,::after,::before{box-sizing:border-box;border-width:0;border-style:solid;border-color:#e2e8f0}img{border-style:solid}h1,h2,h3{font-size:inherit;font-weight:inherit}a{color:inherit;text-decoration:inherit}code,pre{font-family:Menlo,Monaco,Consolas,"Liberation Mono","Courier New",monospace}img{display:block;vertical-align:middle}img{max-width:100%;height:auto}.bg-gray-200{--bg-opacity:1;background-color:#edf2f7;background-color:rgba(237,242,247,var(--bg-opacity))}.border-green-500{--border-opacity:1;border-color:#48bb78;border-color:rgba(72,187,120,var(--border-opacity))}.border-2{border-width:2px}.float-left{float:left}.font-serif{font-family:Georgia,Cambria,"Times New Roman",Times,serif}.text-xl{font-size:1.25rem}.mt-0{margin-top:0}.mt-2{margin-top:.5rem}.mr-2{margin-right:.5rem}.mb-2{margin-bottom:.5rem}.mr-3{margin-right:.75rem}.max-w-sm{max-width:24rem}.max-w-2xl{max-width:42rem}.p-1{padding:.25rem}.p-3{padding:.75rem}.pl-3{padding-left:.75rem}.lesson{padding-left:10px;padding-right:10px;--bg-opacity:1;background-color:#edf2f7;background-color:rgba(237,242,247,var(--bg-opacity))}p.new{padding-top:.5em;padding-bottom:.5em}h1,h2,h3{font-weight:700;margin-bottom:.5em;font-family:Georgia,Cambria,"Times New Roman",Times,serif!important}h1{margin-top:1.5em;font-size:2em!important}h2{margin-top:1em;font-size:1.5em!important}h3{margin-top:.5em;font-size:1.25em!important}p.new a{text-decoration:underline}img:not([class]){width:66.666667%;margin-left:1.25rem;border:1px solid #021a40}pre code{font-size:15px}p code{font-size:smaller}.code-large{background:#f4f4f4;font-family:monospace;font-size:15px;margin:10px;font-size:15px}pre code:not(.line-number){background:#f4f4f4;font-family:monospace;font-size:15px;-webkit-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none;cursor:default;touch-action:none;-webkit-touch-callout:none;-webkit-tap-highlight-color:transparent;border:1px solid #ddd;color:#666;page-break-inside:avoid;display:block;min-width:740px;max-width:740px;line-height:1.6;margin-bottom:1.6em;padding:1em 1.5em;-moz-tab-size:2;-o-tab-size:2;tab-size:2;overflow-x:auto;word-wrap:break-word;white-space:pre-wrap;border-left:3px solid #f36d33}div.code-starter>pre code{border-left:3px solid #fdff44!important;background-image:radial-gradient(rgba(0,150,0,.75),#000 120%);color:#fff;font:.9rem Inconsolata,monospace}div.code-starter>pre code::after{content:"\a$_"}pre{counter-reset:line}</style><script src="https://kit.fontawesome.com/7efc4bcee2.js" crossOrigin="anonymous"></script></head><body class="lesson"><div class="main-content bg-gray-200 p-1 pl-3 text-xl font-serif"><div class="md-inner"><h1 class="section" id="section1">TF•IDF (part 2)</h1><p class="new">This lesson continues our learning of TF•IDF, specifically how to use the powerful scikit-learn library to 
perform a TF•IDF analysis.  Not only will you learn how to use the library but also how it calculates TF and IDF.
Our goal for this lesson is to build a td•idf representation using scikit-learn.</p><h2 id="welcome-to-scikit-learn">Welcome to Scikit-learn</h2><img alt="scikit-learn-logo-small.png" class="sm mr-2 float-left max-w-2xl" src="https://github.com/NSF-EC/INFO490Assets/raw/master/src/dmap/lessons/tfidf-skl/html/scikit-learn-logo-small.png"/><div class="mt-0"><p class="new">The Python library scikit-learn (pronounced sy-kit -- as in science) is a set of modules 
(think Python classes) that can be used to build data analysis and machine learning software.
It is just one toolkit of a family (see <a href="https://scikits.appspot.com/scikits">https://scikits.appspot.com/scikits</a>). SciKit-learn
uses Numpy, SciPy and Matplotlib -- all libraries you worked with in INFO 490.  It's another
mountain to climb, but we will explore it slowly and try to see as much of it as possible.<br/>The Python package uses the prefix <code>sklearn</code>, so we may use either <code>sklearn</code>, sci-kit, or scikit-learn to reference the software. </p></div><h2 id="feature-vectors">Feature Vectors</h2><p class="new">Luckily, scikit-learn has a submodule that specializes in building feature vectors (a numerical
representation) for text documents.  When you use a collections.Counter to count occurrences of words,
you are building a simple feature vector. A feature vector contains information describing
an object's more important characteristics.   </p><img alt="featurevector.png" class="float-left mt-2 mr-3 sm border-2 border-green-500" src="https://github.com/NSF-EC/INFO490Assets/raw/master/src/dmap/lessons/tfidf-skl/html/featurevector.png"/><p class="new">A feature vector is almost always numerical and there is a mapping between the original item and it's numerical 
representation. For now you can think of a feature vector as a set of columns (attributes) for a 
row (an instance/observation). We will point out feature vectors throughout this class.  </p><h3 id="vectorization-">Vectorization </h3><p class="new">Scikit uses the word <em>vectorization</em> as the general process of turning a 
collection of text documents into numerical feature vectors.  We will start with
the <code>CountVectorizer</code> class that builds a fancy version of the <code>collections.Counter</code>.</p><h2 id="the-countvectorizer-class">The <code>CountVectorizer</code> Class</h2><img alt="green_eggs-300.png" class="float-left p-3 max-w-sm" src="https://github.com/NSF-EC/INFO490Assets/raw/master/src/dmap/lessons/tfidf-skl/html/green_eggs-300.png"/><p class="new">We will start our tour using a simple, reduced four chapter story. The function
<code>get_corpus</code> returns this data (and you won't have to type it in!).</p><pre><code>def get_corpus():
    c1 = "Do you like Green eggs and ham"
    c2 = "I do not like them Sam I am  I do not like Green eggs and ham"
    c3 = "Would you like them Here or there"
    c4 = "I would not like them Here or there I would not like them Anywhere"
    return [c1, c2, c3, c4]</code></pre><p class="new">Let's now see how we can use the <code>CountVectorizer</code> class</p><pre><code>from sklearn.feature_extraction.text import CountVectorizer

def cv_demo1():

    corpus = get_corpus()

    # normalize all the words to lowercase
    cvec = CountVectorizer(lowercase=True)

    # convert the documents into a document-term matrix
    doc_term_matrix = cvec.fit_transform(corpus)

    # get the terms found in the corpus
    print(cvec.get_feature_names())

    # get the counts
    print(doc_term_matrix.toarray())

cv_demo1()</code></pre><p class="new">Type in the above code, run it (fix any errors) </p><div class="ide code-starter blink"><pre><code>def get_corpus():
    c1 = "Do you like Green eggs and ham"
    c2 = "I do not like them Sam I am  I do not like Green eggs and ham"
    c3 = "Would you like them Here or there"
    c4 = "I would not like them Here or there I would not like them Anywhere"
    return [c1, c2, c3, c4]
    
def cv_demo1():
    pass
    
cv_demo1()</code></pre></div><p class="new">Your output should match the following:</p><pre><code>['am', 'and', 'anywhere', 'do', 'eggs', 'green', 'ham', 'here', 'like', 'not', 'or', 'sam', 'them', 'there', 'would', 'you']
[[0 1 0 1 1 1 1 0 1 0 0 0 0 0 0 1]
 [1 1 0 2 1 1 1 0 2 2 0 1 1 0 0 0]
 [0 0 0 0 0 0 0 1 1 0 1 0 1 1 1 1]
 [0 0 1 0 0 0 0 1 2 2 1 0 2 1 2 0]]</code></pre><p class="new">The one issue is that the token 'I' is missing.  The <code>CountVectorizer</code> ignores tokens less than 2
characters long by default.  You can pass in your own tokernizer (among many parameters)
to customize how it parses the text.  </p><p class="new">Update the code below (see the comments), and run <code>cv_demo2</code> (note that the function has been updated to return both 
the tokens and the document matrix).</p><div class="ide code-starter blink"><pre><code>def split_into_tokens(data, normalize=True, min_length=0):
    # copy in your solution from Part 1 (or re-implement it here)
    pass

def cv_demo2():

    corpus = get_corpus()

    # pass in our own tokenizer
    cvec = CountVectorizer(tokenizer=split_into_tokens)

    # convert the documents into a document-term matrix
    doc_term_matrix = cvec.fit_transform(corpus)

    # get the terms found in the corpus
    tokens = cvec.get_feature_names()
    
    return doc_term_matrix, tokens

dtm, tokens = cv_demo2()
print(tokens)
print(dtm.toarray())</code></pre></div><p class="new">Note we are passing in the same tokenizer you wrote (that had a default of <code>min_length=0</code>).  You can
re-implement it or just copy&amp;paste it from your previous tf•idf lesson.</p><p class="new">After you re-run the code, you should get the following output.</p><pre><code>['am', 'and', 'anywhere', 'do', 'eggs', 'green', 'ham', 'here', 'i', 'like', 'not', 'or', 'sam', 'them', 'there', 'would', 'you']
[[0 1 0 1 1 1 1 0 0 1 0 0 0 0 0 0 1]
 [1 1 0 2 1 1 1 0 3 2 2 0 1 1 0 0 0]
 [0 0 0 0 0 0 0 1 0 1 0 1 0 1 1 1 1]
 [0 0 1 0 0 0 0 1 2 2 2 1 0 2 1 2 0]]</code></pre><h2 id="visualize-with-pandas">'Visualize' with Pandas</h2><p class="new">We can use Pandas to help make this a bit easier to look at.  We are also going to reuse <code>cv_demo2</code> 
to allow us to focus on the new code. </p><div class="ide code-starter blink"><pre><code>import pandas as pd

def word_matrix_to_df(wm, feature_names):
    # create an index for each row
    doc_names = ['Doc{:d}'.format(idx+1) for idx, _ in enumerate(wm)]
    df = pd.DataFrame(data=wm.toarray(), index=doc_names, columns=feature_names)
    return df

def cv_demo3():

    doc_term_matrix, tokens = cv_demo2()
    df = word_matrix_to_df(doc_term_matrix, tokens)
    
    return df

df = cv_demo3()
print(df.head())</code></pre></div><p class="new">You should see data that matches the table below:
<img alt="tfidf-cv.png" class="mt-2 mr-3 sm" src="https://github.com/NSF-EC/INFO490Assets/raw/master/src/dmap/lessons/tfidf-skl/html/tfidf-cv.png"/></p><p class="new">Take a look at sklearn's <a href="https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html#sklearn.feature_extraction.text.CountVectorizer">documentation</a>.
The <code>CountVectorizer</code> allows you to customize many of the parsing routines:</p><pre><code>cvec = CountVectorizer(tokenizer=split_into_tokens,  # custom tokenizer
                       ngram_range=(1, 2),   # both single and bi-grams
                       stop_words=['and', 'but'])   # custom stop words</code></pre><h2 id="tfidftransformer"><code>TfidfTransformer</code></h2><p class="new">Now that we have a vector of word counts, we can transform those into a TF•IDF matrix (essentially the 
same process as the previous lesson).</p><pre><code>from sklearn.feature_extraction.text import TfidfTransformer
def cv_demo_idf():
    
    # get the data from the CountVectorizer
    doc_term_matrix, tokens = cv_demo2()
    
    # create a transformer
    tfidf_transformer=TfidfTransformer()
    
    # transform the doc_term_matrix into TF*IDF
    tfidf_transformer.fit(doc_term_matrix)

    # make it a dataframe for easy viewing
    df = pd.DataFrame(tfidf_transformer.idf_,
                      index=tokens, columns=["idf_weights"])
    # sort ascending
    df.sort_values(by=['idf_weights'], inplace=True, ascending=False)

    return df

df = cv_demo_idf()
print(df.head(20))</code></pre><p class="new">Type in the code above and run it in the next code cell.</p><div class="ide code-starter blink"><pre><code></code></pre></div><p class="new">For easy comparision, here's the same IDF values from the previous lesson.  What do you notice?
<img alt="idf-cv.png" class="mt-2 mr-3 sm" src="https://github.com/NSF-EC/INFO490Assets/raw/master/src/dmap/lessons/tfidf-skl/html/idf-cv.png"/></p><p class="new">The calcuated values do not match (e.g. for the token 'i', we calculated 0.693; and sklearn's value is 1.511). 
but the relative magnitude seems about correct.  It's always good to 
confirm or validate your implementation with a reference.  </p><p class="new">In order to get the numbers to match, you will need adjust <em>both</em> your
TF and IDF calculations and adjust the calculations that <code>TfidfTransformer</code> uses.</p><p class="new">Let's go through all the adjustments.</p><h3 id="idf-formula">IDF Formula</h3><p class="new">We used the standard formula for idf as</p><img alt="math?math=%5CLarge%20%5Ctext%7B%0Atf(t%2Cd)%20%3D%20(count%20of%20term%20t%20in%20document%20d)%20%2F%20(number%20of%20words%20in%20document%20d)%0A%7D%0A" class="mb-2" src="https://render.githubusercontent.com/render/math?math=%5CLarge%20%5Ctext%7B%0Atf(t%2Cd)%20%3D%20(count%20of%20term%20t%20in%20document%20d)%20%2F%20(number%20of%20words%20in%20document%20d)%0A%7D%0A"/><p class="new">Sklearn's default idf formula is <strong>1 + math.log(N+1/n+1)</strong> where N is total number of documents and n is the 
number of documents for the term.  They add one to prevent division by zero and prevent the taking the log 
of zero.  Using the named parameter <code>smooth_idf</code>, the formula becomes 1 + ln(N/n).</p><pre><code>  # will use 1 + ln(N/n)
  tfidf_transformer=TfidfTransformer(smooth_idf=False)</code></pre><p class="new">If you go back to the previous lesson (and you should) and update the IDF formula to <code>1 + ln( (N+1)/(n + 1)</code>,
you should get 1.5108 for the word 'i'.  </p><p class="new">[✅] Matching IDF</p><p class="new">Let's now print out the tf*idf values and see where we are </p><div class="ide code-starter blink"><pre><code>def cv_demo_tf_idf():

    doc_term_matrix, tokens = cv_demo2()
    tfidf_transformer=TfidfTransformer(smooth_idf=True)

    # learn the IDF vector
    tfidf_transformer.fit(doc_term_matrix)
    idf = tfidf_transformer.idf_

    # transform the count matrix to tf-idf
    tf_idf_vector = tfidf_transformer.transform(doc_term_matrix)
    print(tf_idf_vector)</code></pre></div><p class="new">You should see the following, when you run the function.</p><pre><code>(0, 3)   0.39411340505265774 (doc 0, token w/index 3 has tf•idf = 0.3941)
(0, 1)   0.39411340505265774
(1, 13)  0.1568972451918658
(1, 12)  0.24580985322181814 (doc 1, token w/index 12 has tf•idf = 0.2458)</code></pre><p class="new">This isn't too easy to read, we can use Pandas to help make this easy.</p><div class="ide code-starter blink"><pre><code>def cv_demo_tf_idf():

    doc_term_matrix, tokens = cv_demo2()
    tfidf_transformer=TfidfTransformer(smooth_idf=True)

    # learn the IDF vector
    tfidf_transformer.fit(doc_term_matrix)
    idf = tfidf_transformer.idf_

    # transform the count matrix to tf-idf
    tf_idf_vector = tfidf_transformer.transform(doc_term_matrix)

    # print out the values 
    # for the token 'i' in the second document
    token = 'i'
    doc = 1
    df_idf = pd.DataFrame(idf, index=tokens, columns=["idf_weights"])
    df_idf.sort_values(by=['idf_weights'], inplace=True, ascending=False)
    idf_token = df_idf.loc[token]['idf_weights']

    doc_vector = tf_idf_vector[doc]
    df_tfidf = pd.DataFrame(doc_vector.T.todense(), index=tokens, columns=["tfidf"])
    df_tfidf.sort_values(by=["tfidf"], ascending=False, inplace=True)
    tfidf_token = df_tfidf.loc[token]['tfidf']

    # tfidf = tf * idf
    tf_token = tfidf_token / idf_token
    print('TF    {:s} {:2.4f}'.format(token, tf_token))
    print('IDF   {:s} {:2.4f}'.format(token, idf_token))
    print('TFIDF {:s} {:2.4f}'.format(token, tfidf_token))</code></pre></div><p class="new">Once you run the code you should see the following for the second document, token 'i':</p><pre><code>TF    i 0.3165
IDF   i 1.5108
TFIDF i 0.4781</code></pre><p class="new">However, in the previous lesson (with the updated IDF formula), you will see the following:</p><pre><code>TF:     0.1875
IDF:    1.5108
TFIDF:  0.28328</code></pre><p class="new">[❌] Matching TF (NO!!!)</p><p class="new">Since the IDF values match, and we can only really see the end result (TF•IDF), we can infer that the 
issue (two actually) must be with the TF formula.</p><h3 id="tf-formula-">TF Formula </h3><p class="new">By default sklearn is using the raw term counts.  You can adjust this slightly by setting the named paremter 
<code>sublinear_tf=True</code>.  This changes the formula to 1 + math.log(tf) where tf is the number of occurances
of the term in the document.</p><p class="new">We used a normalized term count (which was divided by the length of the document).  There's no option
in sklearn for this option.  So let's update our TF calculation to be the following:</p><p class="new">1 + math.log(tf) to match sci-kit learn's.</p><p class="new">Where <strong>tf</strong> is the count of how many times term appear in the current document</p><h3 id="normalization">Normalization</h3><p class="new">By Default, the final TF•IDF calculations are normalized using the L2 norm (a.k.a. Euclidian).<br/>The L2 norm is just the square root of the sum of the squared vector values:</p><img alt="math?math=%5CLarge%20v_%7Bnorm%7D%20%3D%20%5Cfrac%7Bv%7D%7B%7C%7Cv%7C%7C_2%7D%20%3D%20%5Cfrac%7Bv%7D%7B%5Csqrt%7Bv_1%5E2%20%2B%20v_2%5E2%20%2B%20%5Cdots%20%2B%20v_n%5E2%7D%7D" class="jax" src="https://render.githubusercontent.com/render/math?math=%5CLarge%20v_%7Bnorm%7D%20%3D%20%5Cfrac%7Bv%7D%7B%7C%7Cv%7C%7C_2%7D%20%3D%20%5Cfrac%7Bv%7D%7B%5Csqrt%7Bv_1%5E2%20%2B%20v_2%5E2%20%2B%20%5Cdots%20%2B%20v_n%5E2%7D%7D"/><p class="new">As an example, let's look at how L2 normalization is done using TF•IDF values for the word 'I':</p><pre><code>L2_norm = sqrt(tfidf('am')²    + tfidf('and')² + .. + tfidf('i')² + ..  +
               tfidf('would')² + tfidf('you')²)

tfidf_norm('i') = tfidf('i')/L2_norm</code></pre><p class="new">The nice thing about applying L2 normalization is that it puts different features on the same scale.<br/>Also, mathmatically, the L2 Norm of the resulting normalized values is 1.0.  That is if you took the square root 
of the sum of the squared tfidif norm values (e.g. <code>tfidf_norm('i')</code>), it would be 1.0.</p><p class="new">The L1 norm uses the sum of the absolute values (a.k.a. Manhattan distance).  We will see more examples in the future of using 
L2 normalization.  A closely related topic, L2 regularization will be discussed later as well.</p><p class="new">We will NOT do this to our code in the previous lesson -- it would require a lot of code refactoring.  Instead, we 
will turn normalization off when we create a <code>TfidfTransformer</code>.</p><p class="new">In order to turn off normalization (remember out goal is to get sklearn's output to match our output from
the previous lesson), we can set the norm parameter to None:</p><pre><code>tfidf_transformer=TfidfTransformer(smooth_idf=True, sublinear_tf=True, norm=None)</code></pre><p class="new">Now we should see the following after running <code>cv_demo_tf_idf</code>. These numbers will match the output
of your code in the previous lesson if you made to two respective changes.</p><pre><code>TF    i 2.0986
IDF   i 1.5108
TFIDF i 3.1706</code></pre><p class="new">[✅] Matching IDF<br/>[✅] Matching TF</p><img alt="tfidf-geah.png" src="https://github.com/NSF-EC/INFO490Assets/raw/master/src/dmap/lessons/tfidf-skl/html/tfidf-geah.png"/><h1 class="section" id="section2">The <code>TfidVectorizer</code></h1><p class="new">Another class provided by scikit-learn is the <code>TfidVectorizer</code> class. This class creates its own <code>CountVectorizer</code> to use.
You use it essentailly the same, but it's bit more compact:</p><pre><code>cv = TfidfVectorizer(smooth_idf=True, use_idf=True, tokenizer=split_into_tokens, norm=None)
tfidf = cv.fit_transform(corpus)
tokens = cv.get_feature_names()
idf = cv.idf_
values = tfidf.todense().tolist()</code></pre><h1 class="section" id="section3">What is the data returned from fit_transform?</h1><p class="new">Sparse Matrix ... </p><h2 id="tf">TF</h2><p class="new">For the TF calculation, TfidfTransformer uses </p><p class="new">  There's a few  </p><p class="new">However, we can 
adjust both our implementation and<br/>Since we 
can't </p><h1 class="section" id="section4">Clustering and Distance Document Simularity</h1><p class="new">now that we have a matrix where the rows are documents (document vectors) of normalized tf*idf values
we can easily compare two documents and see how similar (or different they are).</p><p class="new">The distance between two document vectors is the essence behind using tf*idf for document clustering (an unsupervised machine
learning algorithm), as well as trying to retrived releent documents by search terms.  Your search terms become
a 'document' which gets converted to a vector which can easily find other document vectors that are close to it.</p><p class="new">.... <a href="http://brandonrose.org/clustering#Tf-idf-and-document-similarity">http://brandonrose.org/clustering#Tf-idf-and-document-similarity</a></p><p class="new">attributes 
# TO BE USED
ADD TIMING Section</p><p class="new">TF: It is the number of times the word appears in a document (not the relative freq)</p><ol start="1"><li>• Use the following formula (discussed in detail soon) to calculate the idf of 
a term: <code class="large"><p class="new">term_idf[term] = 1 + log( (N + 1)/(1 + doc_term_count))</p></code></li></ol><h2 id="protecting-the-math-">Protecting The Math </h2><p class="new">At first glance, it might seem 'easy' to calculate TF•IDF (we are almost there). However, for 
both weights there are different ways to calculate them.   The TF we have been describing is 
called the raw count of a term in a document.  A common alternative  is to scale the TF by the 
document length so that long documents don't skew the results (e.g. a long chapter will have 
more words in it, but may be no more important than a shorter chapter). </p><code class="code-large">TF = (raw count of term in a document)/len(document)</code><p class="new">For IDF, there are many variants as well.  The one issue for calculating IDF is that the 
denominator could be zero (if a term does not show up in a document).  In most situations, 
all the terms are coming from the documents themselves, so this can never happen.  </p><p class="new">Some implementations will calculate the 'inverse' going from this equation: </p><code class="code-large">term_idf[term] = log(N/term_count)</code><br/><p class="new">to this equation:<br/><code class="code-large">term_idf[term] = -1.0 * log(term_count/N)

</code><br/></p><p class="new">If you remember your 'log' rules (here's a small 'proof' for the right hand side):<br/><span><img alt="math?math=%5Clarge%201.%5Cquad%20log(%5Cdfrac%7Ba%7D%7Bb%7D)%20%3D%20log(a)%20-%20log(b)" class="jax" src="https://render.githubusercontent.com/render/math?math=%5Clarge%201.%5Cquad%20log(%5Cdfrac%7Ba%7D%7Bb%7D)%20%3D%20log(a)%20-%20log(b)" style="display:inline-block"/> (by definition)

</span><br/><span><img alt="math?math=%5Clarge%202.%5Cquad%20log(%5Cdfrac%7Bb%7D%7Ba%7D)%20%3D%20log(b)%20-%20log(a)" class="jax" src="https://render.githubusercontent.com/render/math?math=%5Clarge%202.%5Cquad%20log(%5Cdfrac%7Bb%7D%7Ba%7D)%20%3D%20log(b)%20-%20log(a)" style="display:inline-block"/> (by definition)

</span><br/><span><img alt="math?math=%5Clarge%203.%5Cquad%20log(a)%20%3D%20log(b)%20-%20log(%5Cdfrac%7Bb%7D%7Ba%7D)" class="jax" src="https://render.githubusercontent.com/render/math?math=%5Clarge%203.%5Cquad%20log(a)%20%3D%20log(b)%20-%20log(%5Cdfrac%7Bb%7D%7Ba%7D)" style="display:inline-block"/> (rearrange terms in #2)

</span><br/><span><img alt="math?math=%5Clarge%204.%5Cquad%20log(a)%20-%20log(b)%20%3D%20-log(%5Cdfrac%7Bb%7D%7Ba%7D)" class="jax" src="https://render.githubusercontent.com/render/math?math=%5Clarge%204.%5Cquad%20log(a)%20-%20log(b)%20%3D%20-log(%5Cdfrac%7Bb%7D%7Ba%7D)" style="display:inline-block"/> (rearrange terms in #3)

</span><br/><strong>SO</strong><br/><span><img alt="math?math=%5Clarge%205.%5Cquad%20log(%5Cdfrac%7Ba%7D%7Bb%7D)%20%3D%20-1.0%20%5Ctimes%20log(%5Cdfrac%7Bb%7D%7Ba%7D)" class="jax" src="https://render.githubusercontent.com/render/math?math=%5Clarge%205.%5Cquad%20log(%5Cdfrac%7Ba%7D%7Bb%7D)%20%3D%20-1.0%20%5Ctimes%20log(%5Cdfrac%7Bb%7D%7Ba%7D)" style="display:inline-block"/></span><br/></p><p class="new">Some will add 1 to the result or to the denominator (to set a lower bound on common words and 
avoid giving a weight of zero to common terms) or both.</p><p class="new">For the most part, if your documents are long and the number of terms large, the calculation will not 
matter for comparison purposes.  However, for simple test data, it can matter.  In practice, everyone 
seems to have their favorite tweaks to the classic formula. </p><p class="new">The Python library <code>sklearn</code> (coming just around the corner) uses the formula you are usingin <code>build_idf</code>.</p><p class="new">  TIMING CODE</p></div></div></body></html>