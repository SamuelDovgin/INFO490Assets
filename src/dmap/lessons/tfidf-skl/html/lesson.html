<!DOCTYPE html><html lang='en'><head><title>TF‚Ä¢IDF (part 2)</title><meta charset="utf-8"><style>/*! normalize.css v8.0.1 | MIT License | github.com/necolas/normalize.css */html{line-height:1.15;-webkit-text-size-adjust:100%}body{margin:0}h1{font-size:2em;margin:.67em 0}pre{font-family:monospace,monospace;font-size:1em}a{background-color:transparent}strong{font-weight:bolder}code{font-family:monospace,monospace;font-size:1em}img{border-style:none}::-webkit-file-upload-button{-webkit-appearance:button;font:inherit}blockquote,h1,h2,h3,p,pre{margin:0}html{font-family:system-ui,-apple-system,BlinkMacSystemFont,"Segoe UI",Roboto,"Helvetica Neue",Arial,"Noto Sans",sans-serif,"Apple Color Emoji","Segoe UI Emoji","Segoe UI Symbol","Noto Color Emoji";line-height:1.5}*,::after,::before{box-sizing:border-box;border-width:0;border-style:solid;border-color:#e2e8f0}img{border-style:solid}h1,h2,h3{font-size:inherit;font-weight:inherit}a{color:inherit;text-decoration:inherit}code,pre{font-family:Menlo,Monaco,Consolas,"Liberation Mono","Courier New",monospace}img{display:block;vertical-align:middle}img{max-width:100%;height:auto}.bg-gray-200{--bg-opacity:1;background-color:#edf2f7;background-color:rgba(237,242,247,var(--bg-opacity))}.bg-gray-300{--bg-opacity:1;background-color:#e2e8f0;background-color:rgba(226,232,240,var(--bg-opacity))}.border-gray-400{--border-opacity:1;border-color:#cbd5e0;border-color:rgba(203,213,224,var(--border-opacity))}.border-gray-500{--border-opacity:1;border-color:#a0aec0;border-color:rgba(160,174,192,var(--border-opacity))}.border-green-500{--border-opacity:1;border-color:#48bb78;border-color:rgba(72,187,120,var(--border-opacity))}.rounded{border-radius:.25rem}.rounded-full{border-radius:9999px}.border-solid{border-style:solid}.border-2{border-width:2px}.border{border-width:1px}.border-t{border-top-width:1px}.inline-block{display:inline-block}.flex{display:flex}.justify-center{justify-content:center}.justify-around{justify-content:space-around}.float-left{float:left}.clearfix:after{content:"";display:table;clear:both}.clear-both{clear:both}.font-serif{font-family:Georgia,Cambria,"Times New Roman",Times,serif}.font-semibold{font-weight:600}.font-bold{font-weight:700}.h-64{height:16rem}.text-sm{font-size:.875rem}.text-base{font-size:1rem}.text-xl{font-size:1.25rem}.m-2{margin:.5rem}.mt-0{margin-top:0}.mt-2{margin-top:.5rem}.mr-2{margin-right:.5rem}.mb-2{margin-bottom:.5rem}.mr-3{margin-right:.75rem}.mb-3{margin-bottom:.75rem}.ml-3{margin-left:.75rem}.mt-4{margin-top:1rem}.mt-6{margin-top:1.5rem}.max-w-sm{max-width:24rem}.max-w-2xl{max-width:42rem}.object-contain{-o-object-fit:contain;object-fit:contain}.overflow-hidden{overflow:hidden}.p-1{padding:.25rem}.p-2{padding:.5rem}.py-1{padding-top:.25rem;padding-bottom:.25rem}.px-3{padding-left:.75rem;padding-right:.75rem}.py-4{padding-top:1rem;padding-bottom:1rem}.px-4{padding-left:1rem;padding-right:1rem}.px-6{padding-left:1.5rem;padding-right:1.5rem}.pl-3{padding-left:.75rem}.shadow-lg{box-shadow:0 10px 15px -3px rgba(0,0,0,.1),0 4px 6px -2px rgba(0,0,0,.05)}.text-center{text-align:center}.text-black{--text-opacity:1;color:#000;color:rgba(0,0,0,var(--text-opacity))}.text-gray-700{--text-opacity:1;color:#4a5568;color:rgba(74,85,104,var(--text-opacity))}.text-gray-800{--text-opacity:1;color:#2d3748;color:rgba(45,55,72,var(--text-opacity))}.whitespace-no-wrap{white-space:nowrap}.w-full{width:100%}.text-tiny{font-size:.5rem!important}body{color:#000!important;font-size:1.25rem!important}h1,h2,h3{color:#000!important}.lesson{padding-left:10px!important;padding-right:10px!important;--bg-opacity:1;background-color:#edf2f7;background-color:rgba(237,242,247,var(--bg-opacity))}.lesson-footer{margin-top:50px;margin-top:20px}span{white-space:nowrap}p.new{text-indent:1.5em;padding-top:0;padding-bottom:.5em}.h-18rem{height:17rem}h1,h2,h3{font-weight:700;margin-bottom:.25em;font-family:Georgia,Cambria,"Times New Roman",Times,serif!important}h1{margin-top:.5em;font-size:2em!important;clear:both}h2{margin-top:1em;font-size:1.5em!important;clear:both}h3{margin-top:.5em;font-size:1.25em!important}p.new a{text-decoration:underline}.lesson a{text-decoration:underline}blockquote{font-size:1em;background:#f9f9f9;border-left:10px solid #ccc;margin:.5em 10px;padding:.5em 10px;border-left-color:#ffcd69;border-right-color:#f6ba59;quotes:"\201C""\201D""\2018""\2019"}blockquote:before{color:#ccc;content:open-quote;font-size:4em;line-height:.1em;margin-right:.25em;vertical-align:-.4em}blockquote p{display:inline}img.wrap{margin-right:1rem!important;border:1px solid #021a40}img.border{border:1px solid #021a40}img:not([class]){width:66.666667%;margin-left:1.25rem;border:1px solid #021a40}code{font-size:smaller}pre code{font-size:15px}pre code:not(.line-number){background:#f4f4f4;font-family:monospace;font-size:15px;-webkit-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none;cursor:default;touch-action:none;-webkit-touch-callout:none;-webkit-tap-highlight-color:transparent;clear:both;border:1px solid #ddd;color:#666;page-break-inside:avoid;display:block;min-width:740px;max-width:740px;line-height:1.6;margin-bottom:1.6em;padding:1em 1.5em;-moz-tab-size:2;-o-tab-size:2;tab-size:2;overflow-x:auto;word-wrap:break-word;white-space:pre-wrap;border-left:3px solid #f36d33}div.code-starter>pre code{border-left:3px solid #fdff44!important;background-image:radial-gradient(rgba(0,150,0,.75),#000 120%);color:#fff;font:.9rem Inconsolata,monospace}div.code-starter>pre code::after{content:"\a$_"}pre{counter-reset:line}</style><script src="https://kit.fontawesome.com/7efc4bcee2.js" crossOrigin="anonymous"></script></head><body class="lesson"><div class="main-content bg-gray-200 text-black p-1 pl-3 font-serif"><div class="md-inner">
<h1 class="overview"></h1><div class="lesson-overview bg-gray-200 flex justify-center"><div class="displaycard max-w-sm rounded overflow-hidden shadow-lg"><div>¬†</div><img alt="Text" class="object-contain h-64 w-full" src="https://github.com/NSF-EC/INFO490Assets/raw/master/src/dmap/lessons/tfidf-skl/html/TextTripping-sm.png"/><div class="px-6 py-4"><div class="text-center font-bold text-xl">TF‚Ä¢IDF (part 2)</div><p class="text-center text-gray-800 text-xl">Sklearn's TF‚Ä¢IDF</p><div class="text-gray-700 text-base">¬†</div><div class="text-center mb-3"><span class="inline-block bg-gray-300 rounded-full px-3 py-1 text-sm font-semibold text-gray-700 mr-2">#tfidf</span><span class="inline-block bg-gray-300 rounded-full px-3 py-1 text-sm font-semibold text-gray-700 mr-2">#text</span></div><div class="flex border-t border-solid border-gray-500 justify-around bg-gray-200"><div class="text-gray-700 text-center px-4 m-2 text-sm"><span class="whitespace-no-wrap">D.MÔπ†üêç</span></div><div class="text-gray-700 text-center px-4 m-2 text-sm"><span class="whitespace-no-wrap"><strong>Version:</strong> <!-- -->8/13/2020</span></div></div><div class="text-gray-700 text-center text-tiny">All Rights Reserved</div></div></div><div class="ml-3 mb-3 h-18rem rounded overflow-hidden shadow-lg"><div class="px-6 py-4"><div class="text-center font-bold text-xl">Colab Notes</div><p class="max-w-sm text-gray-800 text-sm">1. <strong>Copy</strong> this notebook into your google drive</p><p class="max-w-sm text-gray-800 text-sm">2. <strong>Share</strong> the notebook, and copy the share ID</p><p class="max-w-sm text-gray-800 text-sm">3. <strong>Set</strong> the NOTEBOOK_ID variable to the share ID and <strong>SAVE the notebook</strong> (again)</p><p class="max-w-sm text-gray-800 text-sm">4. <strong>Run</strong> the code cell that installs the INFO 490 IDE</p><div class="text-center font-bold text-xl">¬†</div><div class="text-center font-bold text-xl">Jupyter/PyCharm Notes</div><p class="max-w-sm text-gray-800 text-sm">The testing framework does <strong>not work</strong> (at this time) for Jupyter  notebooks or local code development.</p></div></div></div><h1 class="section" id="section1">TF‚Ä¢IDF (part 2) </h1><p class="new">This lesson continues our learning of TF‚Ä¢IDF, specifically how to use the powerful scikit-learn library to 
perform a TF‚Ä¢IDF analysis.  Not only will you learn how to use the library but also how it calculates TF and IDF.
Our goal for this lesson is to build a td‚Ä¢idf representation using scikit-learn.</p><h2 id="welcome-to-scikit-learn">Welcome to Scikit-learn</h2><img alt="scikit-learn-logo-small.png" class="sm mr-2 float-left max-w-2xl" src="https://github.com/NSF-EC/INFO490Assets/raw/master/src/dmap/lessons/tfidf-skl/html/scikit-learn-logo-small.png"/><div class="mt-0"><p class="new">The Python library scikit-learn (pronounced sy-kit -- as in science) is a set of modules 
(think Python classes) that can be used to build data analysis and machine learning software.
It is just one toolkit of a family (see <a href="https://scikits.appspot.com/scikits" target="_blank">https://scikits.appspot.com/scikits</a>). SciKit-learn
uses Numpy, SciPy and Matplotlib -- all libraries you worked with in INFO 490.  It's another
mountain to climb, but we will explore it slowly and try to see as much of it as possible.<br/>The Python package uses the prefix <code>sklearn</code>, so we may use either <code>sklearn</code>, sci-kit, or scikit-learn to reference the software. </p></div><h2 id="feature-vectors">Feature Vectors</h2><p class="new">Luckily, scikit-learn has a sub-module that specializes in building feature vectors (a numerical
representation) for text documents.  When you use a collections.Counter to count occurrences of words,
you are building a simple feature vector. A feature vector contains information describing
an object's more important characteristics.   </p><img alt="featurevector.png" class="float-left mt-2 mr-3 sm border-2 border-green-500" src="https://github.com/NSF-EC/INFO490Assets/raw/master/src/dmap/lessons/tfidf-skl/html/featurevector.png"/><p class="new">A feature vector is almost always numerical and there is a mapping between the original item and it's numerical 
representation. For now you can think of a feature vector as a set of columns (attributes) for a 
row (an instance/observation). We will point out feature vectors throughout this class.  </p><h3 id="vectorization-">Vectorization </h3><p class="new">Scikit uses the word <em>vectorization</em> as the general process of turning a 
collection of text documents into numerical feature vectors.  We will start with
the <code>CountVectorizer</code> class that builds a fancy version of the <code>collections.Counter</code>.</p><h2 id="the-countvectorizer-class">The <code>CountVectorizer</code> Class</h2><img alt="green_eggs-300.png" class="float-left mb-3 wrap" src="https://github.com/NSF-EC/INFO490Assets/raw/master/src/dmap/lessons/tfidf-skl/html/green_eggs-300.png"/><p class="new">We will start our tour using a simple, reduced four chapter story. The function
<code>get_corpus</code> returns this data (and you won't have to type it in!).</p><pre><code>def get_corpus():
    c1 = "Do you like Green eggs and ham"
    c2 = "I do not like them Sam I am  I do not like Green eggs and ham"
    c3 = "Would you like them Here or there"
    c4 = "I would not like them Here or there I would not like them Anywhere"
    return [c1, c2, c3, c4]</code></pre><p class="new">Let's now see how we can use the <code>CountVectorizer</code> class</p><pre><code>from sklearn.feature_extraction.text import CountVectorizer

def cv_demo1():

    corpus = get_corpus()

    # normalize all the words to lowercase
    cvec = CountVectorizer(lowercase=True)

    # convert the documents into a document-term matrix
    doc_term_matrix = cvec.fit_transform(corpus)

    # get the terms found in the corpus
    print(cvec.get_feature_names())

    # get the counts
    print(doc_term_matrix.toarray())

cv_demo1()</code></pre><p class="new">Type in the above code, run it (fix any errors) </p><div class="ide code-starter clearfix"><pre><code>def get_corpus():
    c1 = "Do you like Green eggs and ham"
    c2 = "I do not like them Sam I am  I do not like Green eggs and ham"
    c3 = "Would you like them Here or there"
    c4 = "I would not like them Here or there I would not like them Anywhere"
    return [c1, c2, c3, c4]
    
def cv_demo1():
    pass
    
cv_demo1()</code></pre></div><p class="new">Your output should match the following:</p><pre><code>['am', 'and', 'anywhere', 'do', 'eggs', 'green', 'ham', 'here', 'like', 'not', 'or', 'sam', 'them', 'there', 'would', 'you']
[[0 1 0 1 1 1 1 0 1 0 0 0 0 0 0 1]
 [1 1 0 2 1 1 1 0 2 2 0 1 1 0 0 0]
 [0 0 0 0 0 0 0 1 1 0 1 0 1 1 1 1]
 [0 0 1 0 0 0 0 1 2 2 1 0 2 1 2 0]]</code></pre><p class="new">The one issue is that the token 'I' is missing.  The <code>CountVectorizer</code> ignores tokens less than 2
characters long by default.  You can pass in your own tokenizer (among many parameters)
to customize how it parses the text.  </p><p class="new">Update the code below (see the comments), and run <code>cv_demo2</code> (note that the function has been updated to return both 
the tokens and the document matrix).</p><div class="ide code-starter clearfix"><pre><code>def split_into_tokens(data, normalize=True, min_length=0):
    # copy in your solution from Part 1 (or re-implement it here)
    pass

def cv_demo2():

    corpus = get_corpus()

    # pass in our own tokenizer
    cvec = CountVectorizer(tokenizer=split_into_tokens)

    # convert the documents into a document-term matrix
    doc_term_matrix = cvec.fit_transform(corpus)

    # get the terms found in the corpus
    tokens = cvec.get_feature_names()
    
    return doc_term_matrix, tokens

dtm, tokens = cv_demo2()
print(tokens)
print(dtm.toarray())</code></pre></div><p class="new">Note we are passing in the same tokenizer you wrote (that had a default of <code>min_length=0</code>).  You can
re-implement it or just copy&amp;paste it from your previous tf‚Ä¢idf lesson.</p><p class="new">After you re-run the code, you should get the following output.</p><pre><code>['am', 'and', 'anywhere', 'do', 'eggs', 'green', 'ham', 'here', 'i', 'like', 'not', 'or', 'sam', 'them', 'there', 'would', 'you']
[[0 1 0 1 1 1 1 0 0 1 0 0 0 0 0 0 1]
 [1 1 0 2 1 1 1 0 3 2 2 0 1 1 0 0 0]
 [0 0 0 0 0 0 0 1 0 1 0 1 0 1 1 1 1]
 [0 0 1 0 0 0 0 1 2 2 2 1 0 2 1 2 0]]</code></pre><h2 id="visualize-with-pandas">'Visualize' with Pandas</h2><p class="new">We can use Pandas to help make this a bit easier to look at.  We are also going to reuse <code>cv_demo2</code> 
to allow us to focus on the new code. </p><div class="ide code-starter clearfix"><pre><code>import pandas as pd

def word_matrix_to_df(wm, feature_names):
    # create an index for each row
    doc_names = ['Doc{:d}'.format(idx+1) for idx, _ in enumerate(wm)]
    df = pd.DataFrame(data=wm.toarray(), index=doc_names, columns=feature_names)
    return df

def cv_demo3():

    doc_term_matrix, tokens = cv_demo2()
    df = word_matrix_to_df(doc_term_matrix, tokens)
    
    return df

df = cv_demo3()
print(df.head())</code></pre></div><p class="new">You should see data that matches the table below:
<img alt="tfidf-cv.png" class="mt-2 mr-3 sm" src="https://github.com/NSF-EC/INFO490Assets/raw/master/src/dmap/lessons/tfidf-skl/html/tfidf-cv.png"/></p><p class="new">Take a look at sklearn's <a href="https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html#sklearn.feature_extraction.text.CountVectorizer" target="_blank">documentation</a>.
The <code>CountVectorizer</code> allows you to customize many of the parsing routines:</p><pre><code>cvec = CountVectorizer(tokenizer=split_into_tokens,  # custom tokenizer
                       ngram_range=(1, 2),   # both single and bi-grams
                       stop_words=['and', 'but'])   # custom stop words</code></pre><h2 id="tfidftransformer"><code>TfidfTransformer</code></h2><p class="new">Now that we have a vector of word counts, we can transform those into a TF‚Ä¢IDF matrix (essentially the 
same process as the previous lesson).</p><pre><code>from sklearn.feature_extraction.text import TfidfTransformer
def cv_demo_idf():
    
    # get the data from the CountVectorizer
    doc_term_matrix, tokens = cv_demo2()
    
    # create a transformer
    tfidf_transformer=TfidfTransformer()
    
    # transform the doc_term_matrix into TF‚Ä¢IDF
    tfidf_transformer.fit(doc_term_matrix)

    # make it a dataframe for easy viewing
    df = pd.DataFrame(tfidf_transformer.idf_,
                      index=tokens, columns=["idf_weights"])
    # sort ascending
    df.sort_values(by=['idf_weights'], inplace=True, ascending=False)

    return df

df = cv_demo_idf()
print(df.head(20))</code></pre><p class="new">Type in the code above and run it in the next code cell.</p><div class="ide code-starter clearfix"><pre><code></code></pre></div><p class="new">For easy comparison, here's the same IDF values from the previous lesson.  What do you notice?
<img alt="idf-cv.png" class="mt-2 mr-3 border" src="https://github.com/NSF-EC/INFO490Assets/raw/master/src/dmap/lessons/tfidf-skl/html/idf-cv.png"/></p><p class="new">The calculated values do not match (e.g. for the token 'i', we calculated 0.693; and sklearn's value is 1.511). 
but the relative magnitude seems about correct.  It's always good to 
confirm or validate your implementation with a reference.  </p><p class="new">In order to get the numbers to match, you will need adjust <em>both</em> your
TF and IDF calculations and adjust the calculations that <code>TfidfTransformer</code> uses.</p><p class="new">Let's go through all the adjustments.</p><h3 id="idf-formula">IDF Formula</h3><p class="new">We used the standard formula for idf as</p><img alt="math?math=%5CLarge%20%5Ctext%7B%0Atf(t%2Cd)%20%3D%20(count%20of%20term%20t%20in%20document%20d)%20%2F%20(number%20of%20words%20in%20document%20d)%0A%7D%0A" class="mb-2" src="https://render.githubusercontent.com/render/math?math=%5CLarge%20%5Ctext%7B%0Atf(t%2Cd)%20%3D%20(count%20of%20term%20t%20in%20document%20d)%20%2F%20(number%20of%20words%20in%20document%20d)%0A%7D%0A"/><p class="new">Sklearn's default idf formula is <strong>1 + math.log( (N+1)/(n+1) )</strong> where N is total number of documents and n is the 
number of documents for the term.  They add one to prevent division by zero and prevent the taking the log 
of zero.  When setting the named parameter <code>smooth_idf</code> to False, the formula becomes 1 + ln(N/n).</p><pre><code>  # will use 1 + ln(N/n)
  tfidf_transformer=TfidfTransformer(smooth_idf=False)</code></pre><p class="new">If you go back to the previous lesson (and you should) and update the IDF formula to <code>1 + ln( (N+1)/(n + 1)</code>,
you should get 1.5108 for the word 'i'.  </p><span>[‚úÖ] Matching IDF</span><p class="new">Let's now print out the TF‚Ä¢IDF values and see where we are </p><div class="ide code-starter clearfix"><pre><code>def cv_demo_tf_idf():

    doc_term_matrix, tokens = cv_demo2()
    tfidf_transformer=TfidfTransformer(smooth_idf=True)

    # learn the IDF vector
    tfidf_transformer.fit(doc_term_matrix)
    idf = tfidf_transformer.idf_

    # transform the count matrix to tf-idf
    tf_idf_vector = tfidf_transformer.transform(doc_term_matrix)
    print(tf_idf_vector)</code></pre></div><p class="new">You should see the following, when you run the function.</p><pre><code>(0, 3)   0.39411340505265774 (doc 0, token w/index 3 has tf‚Ä¢idf = 0.3941)
(0, 1)   0.39411340505265774
(1, 13)  0.1568972451918658
(1, 12)  0.24580985322181814 (doc 1, token w/index 12 has tf‚Ä¢idf = 0.2458)</code></pre><p class="new">This isn't too easy to read, we can use Pandas to help make this easy.</p><div class="ide code-starter clearfix"><pre><code>def cv_demo_tf_idf():

    doc_term_matrix, tokens = cv_demo2()
    tfidf_transformer=TfidfTransformer(smooth_idf=True)

    # learn the IDF vector
    tfidf_transformer.fit(doc_term_matrix)
    idf = tfidf_transformer.idf_

    # transform the count matrix to tf-idf
    tf_idf_vector = tfidf_transformer.transform(doc_term_matrix)

    # print out the values 
    # for the token 'i' in the second document
    token = 'i'
    doc = 1
    df_idf = pd.DataFrame(idf, index=tokens, columns=["idf_weights"])
    df_idf.sort_values(by=['idf_weights'], inplace=True, ascending=False)
    idf_token = df_idf.loc[token]['idf_weights']

    doc_vector = tf_idf_vector[doc]
    df_tfidf = pd.DataFrame(doc_vector.T.todense(), index=tokens, columns=["tfidf"])
    df_tfidf.sort_values(by=["tfidf"], ascending=False, inplace=True)
    tfidf_token = df_tfidf.loc[token]['tfidf']

    # tfidf = tf * idf
    tf_token = tfidf_token / idf_token
    print('TF    {:s} {:2.4f}'.format(token, tf_token))
    print('IDF   {:s} {:2.4f}'.format(token, idf_token))
    print('TFIDF {:s} {:2.4f}'.format(token, tfidf_token))</code></pre></div><p class="new">Once you run the code you should see the following for the second document, token 'i':</p><pre><code>TF    i 0.3165
IDF   i 1.5108
TFIDF i 0.4781</code></pre><p class="new">However, in the previous lesson (with the updated IDF formula), you will see the following:</p><pre><code>TF:     0.1875
IDF:    1.5108
TFIDF:  0.28328</code></pre><span>[‚ùå] Matching TF (NO!!!)</span><p class="new">Since the IDF values match, and we can only really see the end result (TF‚Ä¢IDF), we can infer that the 
issue (two actually) must be with the TF formula.</p><h3 id="tf-formula-">TF Formula </h3><p class="new">By default sklearn is using the raw term counts.  You can adjust this slightly by setting the named parameter 
<code>sublinear_tf=True</code>.  This changes the formula to 1 + math.log(tf) where tf is the number of occurrences
of the term in the document.</p><p class="new">We used a normalized term count (which was divided by the length of the document).  There's no option
in sklearn for this option.  So let's update our TF calculation to be the following:</p><p class="new">1 + math.log(tf) to match sci-kit learn's.</p><p class="new">Where <strong>tf</strong> is the count of how many times term appear in the current document</p><h3 id="normalization">Normalization</h3><p class="new">By Default, the final TF‚Ä¢IDF calculations are normalized using the L2 norm (a.k.a. Euclidean).<br/>The L2 norm is just the square root of the sum of the squared vector values:</p><img alt="math?math=%5CLarge%20v_%7Bnorm%7D%20%3D%20%5Cfrac%7Bv%7D%7B%7C%7Cv%7C%7C_2%7D%20%3D%20%5Cfrac%7Bv%7D%7B%5Csqrt%7Bv_1%5E2%20%2B%20v_2%5E2%20%2B%20%5Cdots%20%2B%20v_n%5E2%7D%7D" class="jax" src="https://render.githubusercontent.com/render/math?math=%5CLarge%20v_%7Bnorm%7D%20%3D%20%5Cfrac%7Bv%7D%7B%7C%7Cv%7C%7C_2%7D%20%3D%20%5Cfrac%7Bv%7D%7B%5Csqrt%7Bv_1%5E2%20%2B%20v_2%5E2%20%2B%20%5Cdots%20%2B%20v_n%5E2%7D%7D"/><p class="new">As an example, let's look at how L2 normalization is done using TF‚Ä¢IDF values for the word 'I':</p><pre><code>L2_norm = sqrt(tfidf('am')¬≤    + tfidf('and')¬≤ + .. + tfidf('i')¬≤ + ..  +
               tfidf('would')¬≤ + tfidf('you')¬≤)

tfidf_norm('i') = tfidf('i')/L2_norm</code></pre><p class="new">The nice thing about applying L2 normalization is that it puts different features on the same scale.<br/>Also, mathematically, the L2 Norm of the resulting normalized values is 1.0.  That is if you took the square root 
of the sum of the squared tfidf norm values (e.g. <code>tfidf_norm('i')</code>), it would be 1.0.</p><p class="new">The L1 norm uses the sum of the absolute values (a.k.a. Manhattan distance).  We will see more examples in the future of using 
L2 normalization.  A closely related topic, L2 regularization will be discussed later as well.</p><p class="new">We will NOT do this to our code in the previous lesson -- it would require a lot of code refactoring.  Instead, we 
will turn normalization off when we create a <code>TfidfTransformer</code>.</p><p class="new">In order to turn off normalization (remember out goal is to get sklearn's output to match our output from
the previous lesson), we can set the norm parameter to None:</p><pre><code>TfidfTransformer(smooth_idf=True, sublinear_tf=True, norm=None)</code></pre><p class="new">Now we should see the following after running <code>cv_demo_tf_idf</code>. These numbers will match the output
of your code in the previous lesson if you made to two respective changes.</p><pre><code>TF    i 2.0986
IDF   i 1.5108
TFIDF i 3.1706</code></pre><span>[‚úÖ] Matching IDF</span><br/><span>[‚úÖ] Matching TF</span><p class="new">Here's a quick summary of what we calculated (using the token 'i' for the example):
<img alt="tfidf-geah.png" src="https://github.com/NSF-EC/INFO490Assets/raw/master/src/dmap/lessons/tfidf-skl/html/tfidf-geah.png"/></p><h1 class="section" id="section2">The <code>TfidVectorizer</code></h1><p class="new">Another class provided by scikit-learn is the <code>TfidVectorizer</code> class. This class creates its own <code>CountVectorizer</code> to use.
You use it essentially the same, but it's bit more compact:</p><pre><code>cv = TfidfVectorizer(smooth_idf=True, use_idf=True, tokenizer=split_into_tokens, norm=None)
tfidf = cv.fit_transform(corpus)
tokens = cv.get_feature_names()
idf = cv.idf_
values = tfidf.todense().tolist()</code></pre><h1 class="section" id="section3">Fit vs Transform </h1><p class="new">As we have seen, there are both <code>fit</code> and <code>transform</code> methods on the <code>TfidfTransformer</code> (there's <code>fit_transform</code> too, 
but that's just the combination of the two).  The method <code>fit</code> attempts to build a model based on the data
given to it.  In this specific case, we give it the documents, and 'fit' builds an idf vector.  It essentially
transforms the raw data into normalized data. </p><p class="new">Usually for sklearn, <code>fit</code> is associated with the <em>training</em> phase in machine learning. 
Fitting is similar to finding the best parameters for a model.  These parameters are then
used to transform the data.  As we march forward, this will become a bit clearer.</p><p class="new">If 'fitting' attempts to fit a model and/or parameters for a model, the <code>transform</code> method applies what was fitted to incoming
data.  For this specific situation, transform creates the TF‚Ä¢IDF vectors.  However, for sklearn, transform is 
associated with the <em>testing</em> phase in machine learning.  That is the part where we test the accuracy of the model.</p><p class="new">This will become a bit clearer when we get to Machine Learning.  TF‚Ä¢IDF is actually preparing our data to 
be used (possibly) with other machine learning algorithms.  Since computers can't process text directly, changing
them into numeric vectors is an essential preparation stage for almost all machine learning algorithms.</p><blockquote><p class="new"><strong><em>Machine Learner's Log</em></strong>:  When building a machine learning 'model', you will typically split your data into
two parts: a <em>training set</em> and a <em>testing set</em>.  The 'fit' method works on the training set.  The 'transform' method 
works on the testing set.  The testing set helps determine the accuracy of the model (or the fitting part).</p></blockquote><p class="new">Sklearn is trying to use the fit/transform vocabulary in a non machine learning situation.  For us, it is
building a vector space model (VSM) or a sparse feature set.</p><h1 class="section" id="section4">The Sparse Matrix</h1><p class="new">Perhaps you wondering exactly is the thing/object/data returned from either <code>transform</code> or <code>fit_transform</code>?</p><pre><code>tf_idf_vector = tfidf_transformer.transform(doc_term_matrix)</code></pre><p class="new">It is actually a <em>sparse</em> matrix <a href="https://docs.scipy.org/doc/scipy/reference/sparse.html" target="_blank">sklearn doc</a>. 
As you can imagine, as the set of documents become larger, there will a large set of words that only belong
to a small set of documents.  The tf‚Ä¢idf matrix will contain mostly zeros:
<img alt="sparse_doc.png" class="float-left wrap sm" src="https://github.com/NSF-EC/INFO490Assets/raw/master/src/dmap/lessons/tfidf-skl/html/sparse_doc.png"/></p><p class="new">The sparse matrix efficiently stores the necessary data.  Many of sklearn's algorithms can work with a sparse matrix
as well as regular Numpy vectors.  However, when you use libraries like Pandas, you will have to convert these
sparse matrices into regular Numpy arrays:</p><pre><code>df = pd.DataFrame(data=wm.toarray(), index=doc_names, columns=feature_names)</code></pre><blockquote><p class="new"><em><strong>Coder's Log</strong></em>
As you become more familiar with using sparse matrices, you may come across references to indptr, data, indices. 
The image below helps depict sklearn's implementation:
<img alt="sparse_matrix.png" class="clear-both" src="https://github.com/NSF-EC/INFO490Assets/raw/master/src/dmap/lessons/tfidf-skl/html/sparse_matrix.png"/><br class="clear-both"/></p></blockquote><h2 id="gensim">Gensim</h2><p class="new">Another popular library for working with text is <a href="https://radimrehurek.com/gensim/" target="_blank">gensim</a>.  It also
has TF‚Ä¢IDF implementations as well.  Although we will not be using this library now, it's important to know
that it is an option:</p><pre><code>corpus  = [tokenize(doc) for doc in corpus]
lexicon = gensim.corpora.Dictionary(corpus)
tfidf   = gensim.models.TfidfModel(dictionary=lexicon, normalize=True)
vectors = [tfidf[lexicon.doc2bow(doc)] for doc in corpus]</code></pre><h2 id="clustering-and-distance-document-similarity">Clustering and Distance Document Similarity</h2><p class="new">Now that we have a matrix where the rows are documents (document vectors) of normalized TF‚Ä¢IDF values
we can "easily" compare two documents and see how similar (or different they are).</p><p class="new">The distance between two document vectors is the essence behind using TF‚Ä¢IDF for document 
clustering (an unsupervised machine learning algorithm), as well as trying to retrieved relevant 
documents by search terms.  Your search terms become a 'document' and gets converted to a vector.
Distances are then calculated for each document to the query/search vector.</p><p class="new">Much of this will be covered in a separate lesson on distance metrics. There's a lot more to cover.</p><h1>Test and Submit</h1><p>Once done you can download your code (via <code>ide.tester</code>) and upload that file to Gradescope:</p><div class=""><pre><code><strong># to test your code (either works)</strong><br/>print(ide.tester.test_notebook())<br/>print(ide.tester.test_notebook(verbose=True))<br/><strong># to prepare and download your code</strong><br/>ide.tester.download_solution()</code></pre></div><div class="lesson-footer flex bg-gray-200 justify-center"><div class="displaycard border-t border-gray-400 max-w-2xl rounded overflow-hidden shadow-lg"><div class="px-6 py-4"><div class="text-center font-bold text-xl">TF‚Ä¢IDF (part 2)</div><p class="text-center text-gray-800 text-xl">Sklearn's TF‚Ä¢IDF</p><div class="text-center mt-6 text-xl"><i aria-hidden="true" class="fas fa-tags"></i> any questions on Piazza with <span class="font-bold">tfidf-skl</span></div><div class="text-gray-700 text-base">¬†</div><div class="text-gray-700 text-base">References and Additional Readings</div><div class="text-sm p-2 border border-solid border-gray-500 bg-gray-300"> <div class="text-gray-700 px-4 m-2">‚Ä¢ towardsdatascience.com/how-sklearns-tf-idf-is-different-from-the-standard-tf-idf-275fa582e73d</div><div class="text-gray-700 px-4 m-2">‚Ä¢ scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html#sklearn.feature_extraction.text.TfidfVectorizer</div><div class="text-gray-700 px-4 m-2">‚Ä¢ blog.christianperone.com/2011/10/machine-learning-text-feature-extraction-tf-idf-part-ii</div></div><div class="flex mt-4 border-t border-solid border-gray-500 justify-around bg-gray-200"><div class="text-gray-700 text-center px-4 m-2 text-sm">D.MÔπ†üêç</div><div class="text-gray-700 text-center px-4 m-2 text-sm"><strong>Version:</strong> <!-- -->8/13/2020, 11:27:23 AM</div></div><div class="text-gray-700 text-center text-tiny">All Rights Reserved</div><div class="text-gray-700 text-center text-tiny">Do not distribute this notebook outside of the class</div></div></div></div><div>¬†</div><div class="ide code-starter clearfix"><pre><code># print(ide.tester.test_notebook()) 
# print(ide.tester.test_notebook(verbose=True)) 

ide.tester.download_solution()</code></pre></div></div></div></body></html>