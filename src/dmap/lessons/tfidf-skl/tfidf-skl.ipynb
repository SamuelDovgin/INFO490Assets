{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Welcome to Data, Machines and the üêç\n",
    "<img src=\"https://raw.githubusercontent.com/NSF-EC/INFO490Assets/master/src/dmap/lessons/tfidf-skl/html/section00.png\" align=\"left\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"install\"></a>\n",
    "## Notebook Preparation for Lesson\n",
    "Each lesson will start with a template (given in the course schedule).  \n",
    "Once you open the notebook:\n",
    "1. **save** in on your google drive (copy to drive) and share the notebook\n",
    "2. **copy** the share ID to the `NOTEBOOK_ID` (and re-save the notebook)\n",
    "3. **run** the next cell to install the IDE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "# After changing NOTEBOOK_ID to the shared ID\n",
    "# 1. SAVE THE NOTEBOOK cmd+s\n",
    "# 2. Re-RUN this code cell\n",
    "NOTEBOOK_ID  = '1GBamwb67eCSPp0YreXMIrFF-IZOT1nT3'  # change me!!\n",
    "LESSON_ID    = 'dmap:text:tfidf-skl'       # keep this as is\n",
    "VERSION_ID   = 19\n",
    "\n",
    "def install_ide(lesson_id, nb_id, reload=True):\n",
    "  import os\n",
    "  if not os.path.exists('Bootstrap.py'):\n",
    "     !wget 'https://raw.githubusercontent.com/NSF-EC/INFO490Assets/master/src/tools/Bootstrap.py' -O Bootstrap.py > out.txt 2>&1 \n",
    "  try:\n",
    "    import Bootstrap, importlib\n",
    "    importlib.reload(Bootstrap)\n",
    "\n",
    "    boot = Bootstrap.BootStrap()\n",
    "    return boot.create_ide(lesson_id, nb_id, reload)\n",
    "  except Exception as e:\n",
    "    class Nop(object):\n",
    "        def __init__(self, e): self.e = e\n",
    "        def nop(self, *args, **kw): return(\"unable to test:\" + self.e, None)\n",
    "        def __getattr__(self, _): return self.nop \n",
    "    class IDE():\n",
    "      tester=Nop(str(e))\n",
    "      reader=Nop(str(e)) # RemoteReader ??\n",
    "    return IDE()\n",
    "\n",
    "ide = install_ide(LESSON_ID, NOTEBOOK_ID)\n",
    "ide.tester.hello_world()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lesson TF‚Ä¢IDF (part 2)\n",
    "\n",
    "(run the next cell to read the first part of the lesson)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run to read the next section\n",
    "ide.reader.view_section(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_corpus():\n",
    "    c1 = \"Do you like Green eggs and ham\"\n",
    "    c2 = \"I do not like them Sam I am  I do not like Green eggs and ham\"\n",
    "    c3 = \"Would you like them Here or there\"\n",
    "    c4 = \"I would not like them Here or there I would not like them Anywhere\"\n",
    "    return [c1, c2, c3, c4]\n",
    "    \n",
    "def cv_demo1():\n",
    "    pass\n",
    "    \n",
    "cv_demo1()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run to read the next section\n",
    "ide.reader.view_section(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_into_tokens(data, normalize=True, min_length=0):\n",
    "    # copy in your solution from Part 1 (or re-implement it here)\n",
    "    pass\n",
    "\n",
    "def cv_demo2():\n",
    "\n",
    "    corpus = get_corpus()\n",
    "\n",
    "    # pass in our own tokenizer\n",
    "    cvec = CountVectorizer(tokenizer=split_into_tokens)\n",
    "\n",
    "    # convert the documents into a document-term matrix\n",
    "    doc_term_matrix = cvec.fit_transform(corpus)\n",
    "\n",
    "    # get the terms found in the corpus\n",
    "    tokens = cvec.get_feature_names()\n",
    "    \n",
    "    return doc_term_matrix, tokens\n",
    "\n",
    "dtm, tokens = cv_demo2()\n",
    "print(tokens)\n",
    "print(dtm.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run to read the next section\n",
    "ide.reader.view_section(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def word_matrix_to_df(wm, feature_names):\n",
    "    # create an index for each row\n",
    "    doc_names = ['Doc{:d}'.format(idx+1) for idx, _ in enumerate(wm)]\n",
    "    df = pd.DataFrame(data=wm.toarray(), index=doc_names, columns=feature_names)\n",
    "    return df\n",
    "\n",
    "def cv_demo3():\n",
    "\n",
    "    doc_term_matrix, tokens = cv_demo2()\n",
    "    df = word_matrix_to_df(doc_term_matrix, tokens)\n",
    "    \n",
    "    return df\n",
    "\n",
    "df = cv_demo3()\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run to read the next section\n",
    "ide.reader.view_section(7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "# type&run the above example/exercise in this cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run to read the next section\n",
    "ide.reader.view_section(9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cv_demo_tf_idf():\n",
    "\n",
    "    doc_term_matrix, tokens = cv_demo2()\n",
    "    tfidf_transformer=TfidfTransformer(smooth_idf=True)\n",
    "\n",
    "    # learn the IDF vector\n",
    "    tfidf_transformer.fit(doc_term_matrix)\n",
    "    idf = tfidf_transformer.idf_\n",
    "\n",
    "    # transform the count matrix to tf-idf\n",
    "    tf_idf_vector = tfidf_transformer.transform(doc_term_matrix)\n",
    "    print(tf_idf_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run to read the next section\n",
    "ide.reader.view_section(11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cv_demo_tf_idf():\n",
    "\n",
    "    doc_term_matrix, tokens = cv_demo2()\n",
    "    tfidf_transformer=TfidfTransformer(smooth_idf=True)\n",
    "\n",
    "    # learn the IDF vector\n",
    "    tfidf_transformer.fit(doc_term_matrix)\n",
    "    idf = tfidf_transformer.idf_\n",
    "\n",
    "    # transform the count matrix to tf-idf\n",
    "    tf_idf_vector = tfidf_transformer.transform(doc_term_matrix)\n",
    "\n",
    "    # print out the values \n",
    "    # for the token 'i' in the second document\n",
    "    token = 'i'\n",
    "    doc = 1\n",
    "    df_idf = pd.DataFrame(idf, index=tokens, columns=[\"idf_weights\"])\n",
    "    df_idf.sort_values(by=['idf_weights'], inplace=True, ascending=False)\n",
    "    idf_token = df_idf.loc[token]['idf_weights']\n",
    "\n",
    "    doc_vector = tf_idf_vector[doc]\n",
    "    df_tfidf = pd.DataFrame(doc_vector.T.todense(), index=tokens, columns=[\"tfidf\"])\n",
    "    df_tfidf.sort_values(by=[\"tfidf\"], ascending=False, inplace=True)\n",
    "    tfidf_token = df_tfidf.loc[token]['tfidf']\n",
    "\n",
    "    # tfidf = tf * idf\n",
    "    tf_token = tfidf_token / idf_token\n",
    "    print('TF    {:s} {:2.4f}'.format(token, tf_token))\n",
    "    print('IDF   {:s} {:2.4f}'.format(token, idf_token))\n",
    "    print('TFIDF {:s} {:2.4f}'.format(token, tfidf_token))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run to read the next section\n",
    "ide.reader.view_section(13)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The TfidVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run to read the next section\n",
    "ide.reader.view_section(14)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fit vs Transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run to read the next section\n",
    "ide.reader.view_section(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Sparse Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run to read the next section\n",
    "ide.reader.view_section(16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test and Submit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run to read the next section\n",
    "ide.reader.view_section(17)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(ide.tester.test_notebook()) \n",
    "# print(ide.tester.test_notebook(verbose=True)) \n",
    "\n",
    "ide.tester.download_solution()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "AB",
   "collapsed_sections": [],
   "count": 19,
   "name": "TF‚Ä¢IDF (part 2)",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language": "python"
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
